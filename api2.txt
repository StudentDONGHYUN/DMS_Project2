TITLE: Example Text Generation - LiteRT LLM Pipeline - Python
DESCRIPTION: This example shows how to use the initialized LiteRT LLM pipeline to generate text. It defines a prompt and then calls the 'generate' method on the pipeline, specifying a maximum number of decode steps to control the length of the generated output.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_29

LANGUAGE: python
CODE:
```
prompt = "What is the primary function of mitochondria within a cell"
output = pipeline.generate(prompt, max_decode_steps = 100)
```

----------------------------------------

TITLE: Performing Pose Landmark Detection and Visualization with MediaPipe Tasks
DESCRIPTION: This code block orchestrates the entire pose landmark detection process. It initializes a `PoseLandmarker` object using the pre-trained model, loads the input image, performs the detection, and then visualizes the results by drawing the detected landmarks onto the image using the `draw_landmarks_on_image` utility function.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an PoseLandmarker object.
base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')
options = vision.PoseLandmarkerOptions(
    base_options=base_options,
    output_segmentation_masks=True)
detector = vision.PoseLandmarker.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file("image.jpg")

# STEP 4: Detect pose landmarks from the input image.
detection_result = detector.detect(image)

# STEP 5: Process the detection result. In this case, visualize it.
annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)
cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))
```

----------------------------------------

TITLE: Performing Face Detection and Visualization with MediaPipe Python
DESCRIPTION: This snippet demonstrates how to perform face detection using MediaPipe's Python API. It involves importing necessary modules, initializing a `FaceDetector` with a TFLite model (`detector.tflite`), loading an image from `IMAGE_FILE`, running the detection, and then visualizing the results using OpenCV functions like `cv2.cvtColor` and `cv2_imshow`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
import numpy as np
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an FaceDetector object.
base_options = python.BaseOptions(model_asset_path='detector.tflite')
options = vision.FaceDetectorOptions(base_options=base_options)
detector = vision.FaceDetector.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file(IMAGE_FILE)

# STEP 4: Detect faces in the input image.
detection_result = detector.detect(image)

# STEP 5: Process the detection result. In this case, visualize it.
image_copy = np.copy(image.numpy_view())
annotated_image = visualize(image_copy, detection_result)
rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
cv2_imshow(rgb_annotated_image)
```

----------------------------------------

TITLE: Performing Text Classification with MediaPipe Tasks in Python
DESCRIPTION: This code block demonstrates the complete workflow for classifying text using MediaPipe Tasks. It imports necessary modules, creates a `TextClassifier` instance from a TFLite model, performs classification on `INPUT_TEXT`, and then processes the result to print the top predicted category and its score.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/python/text_classifier.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
from mediapipe.tasks import python
from mediapipe.tasks.python import text

# STEP 2: Create an TextClassifier object.
base_options = python.BaseOptions(model_asset_path="classifier.tflite")
options = text.TextClassifierOptions(base_options=base_options)
classifier = text.TextClassifier.create_from_options(options)

# STEP 3: Classify the input text.
classification_result = classifier.classify(INPUT_TEXT)

# STEP 4: Process the classification result. In this case, print out the most likely category.
top_category = classification_result.classifications[0].categories[0]
print(f'{top_category.category_name} ({top_category.score:.2f})')
```

----------------------------------------

TITLE: Performing Audio Classification with MediaPipe Tasks - Python
DESCRIPTION: This comprehensive snippet demonstrates end-to-end audio classification using MediaPipe Tasks. It initializes the `AudioClassifier` with the downloaded YAMNet model, reads the WAV audio data, segments it into clips, performs inference, and then iterates through the classification results to print the top category and its score for specific timestamps. It requires `numpy`, `scipy.io.wavfile`, and MediaPipe Tasks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/python/audio_classification.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np

from mediapipe.tasks import python
from mediapipe.tasks.python.components import containers
from mediapipe.tasks.python import audio
from scipy.io import wavfile

# Customize and associate model for Classifier
base_options = python.BaseOptions(model_asset_path='classifier.tflite')
options = audio.AudioClassifierOptions(
    base_options=base_options, max_results=4)

# Create classifier, segment audio clips, and classify
with audio.AudioClassifier.create_from_options(options) as classifier:
  sample_rate, wav_data = wavfile.read(audio_file_name)
  audio_clip = containers.AudioData.create_from_array(
      wav_data.astype(float) / np.iinfo(np.int16).max, sample_rate)
  classification_result_list = classifier.classify(audio_clip)

  assert(len(classification_result_list) == 5)

# Iterate through clips to display classifications
  for idx, timestamp in enumerate([0, 975, 1950, 2925]):
    classification_result = classification_result_list[idx]
    top_category = classification_result.classifications[0].categories[0]
    print(f'Timestamp {timestamp}: {top_category.category_name} ({top_category.score:.2f})')
```

----------------------------------------

TITLE: Initiating Supervised Fine-Tuning (SFT) - TRL & Transformers - Python
DESCRIPTION: This snippet initializes and starts the supervised fine-tuning process using `SFTTrainer` from the TRL library. It configures training arguments such as batch size, learning rate, and optimization strategy, and integrates the previously defined LoRA configuration. The `trainer.train()` method then commences the fine-tuning on the prepared dataset.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
import transformers
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,
    train_dataset = ds['train'],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=2,
        max_steps=150,
        #num_train_epochs=1,
        # Copied from other hugging face tuning blog posts
        learning_rate=2e-4,
        #fp16=True,
        bf16=True,
        # It makes training faster
        logging_steps=1,
        output_dir="outputs",
        optim="paged_adamw_8bit",
        report_to = "none",
    ),
    peft_config=lora_config,
)
trainer.train()
```

----------------------------------------

TITLE: Loading Gemma-3-1B Model and Tokenizer (Python)
DESCRIPTION: This snippet loads the `google/gemma-3-1b-pt` model and its corresponding tokenizer from HuggingFace. It configures the model for `bfloat16` precision and sets up a custom chat template for the tokenizer, ensuring proper formatting for conversational inputs and outputs. The HuggingFace token is used for authentication during model and tokenizer download.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
import os

import torch
from transformers import AutoTokenizer, BitsAndBytesConfig, GemmaTokenizer
from transformers.models.gemma3 import Gemma3ForCausalLM

model_id = 'google/gemma-3-1b-pt'
tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])
model = Gemma3ForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto", token=os.environ['HF_TOKEN'], attn_implementation='eager')
# Set up the chat format
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.chat_template = "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
```

----------------------------------------

TITLE: Initializing MediaPipe ImageClassifier - Python
DESCRIPTION: This code initializes an `ImageClassifier` object. It sets up `BaseOptions` by specifying the path to the TFLite model (`classifier.tflite`) and then configures `ImageClassifierOptions` to limit the classification results to a maximum of 4. The `classifier` object is then created using these options, making it ready for image classification.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
base_options = python.BaseOptions(model_asset_path='classifier.tflite')
options = vision.ImageClassifierOptions(
    base_options=base_options, max_results=4)
classifier = vision.ImageClassifier.create_from_options(options)
```

----------------------------------------

TITLE: Initiating Object Detector Model Retraining (Python)
DESCRIPTION: This code initiates the retraining process for the object detection model using the `create()` method of `object_detector.ObjectDetector`. It takes the prepared training and validation datasets, along with the previously configured `options`, to start the resource-intensive training. The resulting `model` object represents the trained model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
model = object_detector.ObjectDetector.create(
    train_data=train_data,
    validation_data=validation_data,
    options=options)
```

----------------------------------------

TITLE: Loading and Tokenizing SFT Dataset - Hugging Face Datasets - Python
DESCRIPTION: This snippet loads a supervised fine-tuning (SFT) dataset from Hugging Face and prepares it for training. It defines a `tokenize_function` to format prompt-completion pairs into a chat template, then applies this function to the dataset in a batched manner, creating a 'text' column suitable for language model training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
from datasets import load_dataset

ds = load_dataset("argilla/synthetic-concise-reasoning-sft-filtered")
def tokenize_function(examples):
    # Process all examples in the batch
    prompts = examples["prompt"]
    completions = examples["completion"]
    texts = []
    for prompt, completion in zip(prompts, completions):
        text = tokenizer.apply_chat_template([{"role": "user", "content": prompt.strip()}, {"role": "assistant", "content": completion.strip()}], tokenize=False)
        texts.append(text)
    return { "text" : texts }  # Return a list of texts

ds = ds.map(tokenize_function, batched = True)
```

----------------------------------------

TITLE: Performing Text Embedding and Similarity Calculation with MediaPipe - Python
DESCRIPTION: This snippet demonstrates how to initialize the MediaPipe `TextEmbedder` with a downloaded model and then use it to generate embeddings for two text strings. It subsequently calculates and prints the cosine similarity between these embeddings, indicating how semantically similar the texts are. Key parameters include `l2_normalize` and `quantize` for embedding options.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_embedder/python/text_embedder.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from mediapipe.tasks import python
from mediapipe.tasks.python import text

# Create your base options with the model that was downloaded earlier
base_options = python.BaseOptions(model_asset_path='embedder.tflite')

# Set your values for using normalization and quantization
l2_normalize = True #@param {type:"boolean"}
quantize = False #@param {type:"boolean"}

# Create the final set of options for the Embedder
options = text.TextEmbedderOptions(
    base_options=base_options, l2_normalize=l2_normalize, quantize=quantize)

with text.TextEmbedder.create_from_options(options) as embedder:
  # Retrieve the first and second sets of text that will be compared
  first_text = "I'm feeling so good" #@param {type:"string"}
  second_text = "I'm okay I guess" #@param {type:"string"}

  # Convert both sets of text to embeddings
  first_embedding_result = embedder.embed(first_text)
  second_embedding_result = embedder.embed(second_text)

  # Calculate and print similarity
  similarity = text.TextEmbedder.cosine_similarity(
      first_embedding_result.embeddings[0],
      second_embedding_result.embeddings[0])
  print(similarity)
```

----------------------------------------

TITLE: Initializing LiteRT LLM Pipeline
DESCRIPTION: This snippet initializes a `LiteRTLlmPipeline` instance, which is essential for interacting with the loaded language model. It typically requires an `interpreter` for model execution and a `tokenizer` for text processing.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
# Disclaimer: Model performance demonstrated with the Python API in this notebook is not representative of performance on a local device.
pipeline = LiteRTLlmPipeline(interpreter, tokenizer)
```

----------------------------------------

TITLE: Initializing LiteRT LLM Pipeline - Python
DESCRIPTION: This line initializes an instance of the `LiteRTLlmPipeline` class, which is used to manage the LLM inference process. It requires an `interpreter` (likely a TFLite interpreter) and a `tokenizer` object to handle model execution and text tokenization, respectively. This pipeline object will then be used for text generation.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
pipeline = LiteRTLlmPipeline(interpreter, tokenizer)
```

----------------------------------------

TITLE: Converting Gemma Model to LiteRT Format (Python)
DESCRIPTION: This comprehensive script converts a PyTorch Gemma 1B model to the LiteRT format, including 8-bit quantization. It defines conversion flags, creates attention masks for prefill and decode stages, and uses `ai_edge_torch.generative.utilities.converter` to export the model as a TFLite file with specified quantization and KV cache settings.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
from absl import flags
from absl import app
import sys
import torch

from ai_edge_torch.generative.examples.gemma3 import gemma3
from ai_edge_torch.generative.layers import kv_cache
from ai_edge_torch.generative.utilities import converter
from ai_edge_torch.generative.utilities.export_config import ExportConfig


flags = converter.define_conversion_flags('gemma3-1b')
flags.FLAGS.mask_as_input = True
flags.FLAGS.prefill_seq_lens = [128]
flags.FLAGS.kv_cache_max_len = 1024

def _create_mask(mask_len, kv_cache_max_len):
  mask = torch.full(
      (mask_len, kv_cache_max_len), float('-inf'), dtype=torch.float32
  )
  mask = torch.triu(mask, diagonal=1).unsqueeze(0).unsqueeze(0)
  return mask


def _create_export_config(
    prefill_seq_lens: list[int], kv_cache_max_len: int
) -> ExportConfig:
  """Creates the export config for the model."""
  export_config = ExportConfig()
  if isinstance(prefill_seq_lens, list):
    prefill_mask = [_create_mask(i, kv_cache_max_len) for i in prefill_seq_lens]
  else:
    prefill_mask = _create_mask(prefill_seq_lens, kv_cache_max_len)

  export_config.prefill_mask = prefill_mask

  decode_mask = torch.full(
      (1, kv_cache_max_len), float('-inf'), dtype=torch.float32
  )
  decode_mask = torch.triu(decode_mask, diagonal=1).unsqueeze(0).unsqueeze(0)
  export_config.decode_mask = decode_mask
  export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED
  return export_config


def convert_to_litert(_):
  with torch.inference_mode(True):
    pytorch_model = gemma3.build_model_1b(
      "/content/merged_model", kv_cache_max_len=flags.FLAGS.kv_cache_max_len,
    )
    converter.convert_to_tflite(
        pytorch_model,
        output_path="/content/",
        output_name_prefix="gemma3_1b_finetune",
        prefill_seq_len=flags.FLAGS.prefill_seq_lens,
        quantize=converter.QuantizationName.DYNAMIC_INT4_BLOCK32,
        lora_ranks=None,
        export_config=_create_export_config(
            prefill_seq_lens=flags.FLAGS.prefill_seq_lens,
            kv_cache_max_len=flags.FLAGS.kv_cache_max_len,
        ),
    )

# Ignore flags passed from the colab runtime.
sys.argv = sys.argv[:1]
app.run(convert_to_litert)
```

----------------------------------------

TITLE: Generating Text with LiteRT LLM Pipeline
DESCRIPTION: This code demonstrates how to use the loaded LiteRT model to generate text. It takes a `prompt` as input and uses the `runner.generate` method to produce an `output`, with an option to specify maximum decoding steps.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
prompt = "What is the capital of France?"
output = runner.generate(prompt, max_decode_steps=None)
```

----------------------------------------

TITLE: Exporting Retrained MediaPipe Model to TensorFlow Lite (Python)
DESCRIPTION: This command exports the retrained MediaPipe model to the TensorFlow Lite format, which is necessary for deployment in applications. The export process also generates required model metadata and a classification label file.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
model.export_model()
```

----------------------------------------

TITLE: Exporting and Downloading TensorFlow Lite Model
DESCRIPTION: This snippet exports the trained object detection model to a TensorFlow Lite ('.tflite') format, lists the contents of the 'exported_model' directory to confirm the file's presence, and then initiates the download of the 'dogs.tflite' model file for on-device application use.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
model.export_model('dogs.tflite')
!ls exported_model
files.download('exported_model/dogs.tflite')
```

----------------------------------------

TITLE: Evaluating Object Detector Model Performance (Python)
DESCRIPTION: This snippet evaluates the performance of the retrained object detection model using the `evaluate()` method on the validation dataset. It calculates and prints the validation loss and COCO metrics, with 'AP' (Average Precision) being a key indicator of model performance. This step helps assess the model's accuracy and generalization capabilities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
loss, coco_metrics = model.evaluate(validation_data, batch_size=4)
print(f"Validation loss: {loss}")
print(f"Validation coco metrics: {coco_metrics}")
```

----------------------------------------

TITLE: Evaluating a Trained Image Classifier Model (Python)
DESCRIPTION: This code snippet shows how to evaluate the performance of a previously trained `ImageClassifier` model (`model_2`) using a `test_data` dataset. It returns the `loss` and `accuracy` metrics, providing insights into the model's generalization capabilities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
loss, accuracy = model_2.evaluate(test_data)
```

----------------------------------------

TITLE: Generating Text with Prefill and Decode - LiteRT LLM Pipeline - Python
DESCRIPTION: This is the main entry point for text generation, orchestrating the entire process. It tokenizes the input prompt, initializes the prefill runner, executes the prefill stage to get the initial KV cache, and then calls the decode method to generate the remaining text based on the prefilled context.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_27

LANGUAGE: python
CODE:
```
  def generate(self, prompt: str, max_decode_steps: int | None = None) -> str:
    messages=[{ 'role': 'user', 'content': prompt}]
    token_ids = self._tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
    # Initialize the prefill runner with the suitable input size.
    self._init_prefill_runner(len(token_ids))

    # Run prefill.
    # Prefill up to the seond to the last token of the prompt, because the last
    # token of the prompt will be used to bootstrap decode.
    prefill_token_length = len(token_ids) - 1

    print('Running prefill')
    kv_cache = self._run_prefill(token_ids[:prefill_token_length])
    # Run decode.
    print('Running decode')
    actual_max_decode_steps = self._max_kv_cache_seq_len - prefill_token_length - 1
    if max_decode_steps is not None:
      actual_max_decode_steps = min(actual_max_decode_steps, max_decode_steps)
    decode_text = self._run_decode(
        prefill_token_length,
        token_ids[prefill_token_length],
        kv_cache,
        actual_max_decode_steps,
    )
    return decode_text
```

----------------------------------------

TITLE: Training Image Classifier with Custom HParams and Model Options (Python)
DESCRIPTION: This snippet demonstrates how to train an `ImageClassifier` model with custom hyperparameters and model options. It sets the number of training epochs to 15 and a dropout rate of 0.07, overriding the default values to potentially improve model performance. It requires `train_data`, `validation_data`, and a `spec` for the supported model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
hparams=image_classifier.HParams(epochs=15, export_dir="exported_model_2")
options = image_classifier.ImageClassifierOptions(supported_model=spec, hparams=hparams)
options.model_options = image_classifier.ModelOptions(dropout_rate = 0.07)
model_2 = image_classifier.ImageClassifier.create(
    train_data = train_data,
    validation_data = validation_data,
    options=options,
)
```

----------------------------------------

TITLE: Invoking LLM Text Generation - Python
DESCRIPTION: This snippet demonstrates how to use the initialized `LiteRTLlmPipeline` to generate text. It sets a `prompt` string and then calls the `generate` method on the `pipeline` object, passing the prompt and setting `max_decode_steps` to `None` for potentially unlimited decoding. The generated text is stored in the `output` variable.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
prompt = "what is 8 mod 6"
output = pipeline.generate(prompt, max_decode_steps = None)
```

----------------------------------------

TITLE: Testing Fine-Tuned Model Inference - Hugging Face Transformers - Python
DESCRIPTION: This snippet demonstrates how to perform text generation inference using the fine-tuned model. It re-initializes a text generation pipeline with the updated model and tokenizer, then generates a response to a given prompt, showcasing the model's improved capabilities after fine-tuning.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
from transformers import pipeline
# Let's test the base model before training
prompt = "What is the primary function of mitochondria within a cell?"
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
pipe(prompt, max_new_tokens=100)
```

----------------------------------------

TITLE: Exporting Quantized MediaPipe Model (Python)
DESCRIPTION: Exports the trained model to a TensorFlow Lite (`.tflite`) file, applying the specified post-training quantization. The `model_name` parameter sets the output filename, and the `quantization_config` object dictates the quantization strategy, such as `int8` quantization.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_16

LANGUAGE: Python
CODE:
```
model.export_model(model_name="model_int8.tflite", quantization_config=quantization_config)
```

----------------------------------------

TITLE: Exporting Quantized BERT Text Classifier and Labels (Python)
DESCRIPTION: This snippet exports the BERT-based text classifier as a TFLite model with dynamic range quantization to reduce its size. It imports `quantization`, creates a `QuantizationConfig`, and then uses `bert_model.export_model()` with this configuration, also exporting the labels.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
from mediapipe_model_maker import quantization
quantization_config = quantization.QuantizationConfig.for_dynamic()
bert_model.export_model(quantization_config=quantization_config)
bert_model.export_labels(export_dir=options.hparams.export_dir)
```

----------------------------------------

TITLE: Exporting Retrained Face Stylizer Model (Python)
DESCRIPTION: This snippet exports the retrained face stylizer model to the TensorFlow Lite format, which is essential for integrating it into applications using MediaPipe. The export process automatically generates all required model metadata and a classification label file, preparing the model for deployment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
face_stylizer_model.export_model()
```

----------------------------------------

TITLE: Running Image Classifier Retraining with MediaPipe (Python)
DESCRIPTION: This code initiates the image classifier retraining process using the `create()` method of `ImageClassifier`. It requires prepared training and validation datasets, along with the previously defined retraining options. This process can be resource-intensive and its duration depends on compute resources.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
model = image_classifier.ImageClassifier.create(
    train_data = train_data,
    validation_data = validation_data,
    options=options,
)
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library using pip, Python's package installer. The '-q' flag ensures a quiet installation, suppressing detailed output, which is useful for cleaner notebook execution. This step is a prerequisite for utilizing any MediaPipe functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Model Maker Package
DESCRIPTION: This code block installs and upgrades the `pip` package manager, then installs the `mediapipe-model-maker` library, which is essential for customizing on-device ML models.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install --upgrade pip
!pip install mediapipe-model-maker
```

----------------------------------------

TITLE: Installing MediaPipe Model Maker Library
DESCRIPTION: This command installs the `mediapipe-model-maker` Python package, which is essential for customizing on-device machine learning models, including the face stylizer. It's a prerequisite for running the subsequent model customization steps.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install mediapipe-model-maker
```

----------------------------------------

TITLE: Running MediaPipe Gesture Recognition Inference (Python)
DESCRIPTION: This snippet demonstrates the core steps for performing gesture recognition using MediaPipe. It initializes a `GestureRecognizer` with a specified model, loads input images, performs recognition, and extracts the top gesture and hand landmarks from the results. It requires the `mediapipe` library and a `gesture_recognizer.task` model file. The final line `display_batch_of_images_with_gestures_and_hand_landmarks` implies a visualization function not included in the snippet.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an GestureRecognizer object.
base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')
options = vision.GestureRecognizerOptions(base_options=base_options)
recognizer = vision.GestureRecognizer.create_from_options(options)

images = []
results = []
for image_file_name in IMAGE_FILENAMES:
  # STEP 3: Load the input image.
  image = mp.Image.create_from_file(image_file_name)

  # STEP 4: Recognize gestures in the input image.
  recognition_result = recognizer.recognize(image)

  # STEP 5: Process the result. In this case, visualize it.
  images.append(image)
  top_gesture = recognition_result.gestures[0][0]
  hand_landmarks = recognition_result.hand_landmarks
  results.append((top_gesture, hand_landmarks))

display_batch_of_images_with_gestures_and_hand_landmarks(images, results)
```

----------------------------------------

TITLE: Applying Background Blur with MediaPipe ImageSegmenter in Python
DESCRIPTION: This snippet demonstrates how to initialize a MediaPipe ImageSegmenter, load images, perform segmentation to obtain category masks, convert image color spaces using OpenCV, apply a Gaussian blur to the background, and combine the original and blurred images based on the segmentation mask. It requires MediaPipe, OpenCV (cv2), and NumPy (np) libraries.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
# Create the segmenter
with python.vision.ImageSegmenter.create_from_options(options) as segmenter:

  # Loop through available image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe Image
    image = mp.Image.create_from_file(image_file_name)

    # Retrieve the category masks for the image
    segmentation_result = segmenter.segment(image)
    category_mask = segmentation_result.category_mask

    # Convert the BGR image to RGB
    image_data = cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB)

    # Apply effects
    blurred_image = cv2.GaussianBlur(image_data, (55,55), 0)
    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1
    output_image = np.where(condition, image_data, blurred_image)

    print(f'Blurred background of {image_file_name}:')
    resize_and_show(output_image)
```

----------------------------------------

TITLE: Loading Object Detection Datasets from Pascal VOC
DESCRIPTION: This snippet loads the training and validation datasets for object detection using 'object_detector.Dataset.from_pascal_voc_folder'. It specifies the folder paths for 'dogs copy/train' and 'dogs copy/validate' and sets cache directories for efficient data handling during model training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
train_data = object_detector.Dataset.from_pascal_voc_folder(
    'dogs copy/train',
    cache_dir="/tmp/od_data/train",
)

val_data = object_detector.Dataset.from_pascal_voc_folder(
    'dogs copy/validate',
    cache_dir="/tmp/od_data/validatation")
```

----------------------------------------

TITLE: Training MediaPipe Object Detector Model
DESCRIPTION: This snippet configures and trains an object detection model using MediaPipe Model Maker. It sets hyperparameters like 'batch_size=8', 'learning_rate=0.3', and 'epochs=50', specifies 'MOBILENET_V2' as the supported model, and then creates and trains the 'ObjectDetector' using the prepared training and validation data.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
hparams = object_detector.HParams(batch_size=8, learning_rate=0.3, epochs=50, export_dir='exported_model')
options = object_detector.ObjectDetectorOptions(
    supported_model=object_detector.SupportedModels.MOBILENET_V2,
    hparams=hparams
)
model = object_detector.ObjectDetector.create(
    train_data=train_data,
    validation_data=val_data,
    options=options)
```

----------------------------------------

TITLE: Training BERT Text Classifier (Python)
DESCRIPTION: This snippet initiates the training of the BERT-based text classifier using `TextClassifier.create()`. It utilizes the `train_data`, `validation_data`, and the `options` configured for the MobileBERT model. Note that this process can be computationally intensive.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
bert_model = text_classifier.TextClassifier.create(train_data, validation_data, options)
```

----------------------------------------

TITLE: Exporting Object Detector Model to TensorFlow Lite (Python)
DESCRIPTION: This code exports the trained object detection model to the TensorFlow Lite format, including essential metadata like the label map. It first calls `export_model()` to perform the conversion, then lists the contents of the `exported_model` directory, and finally downloads the generated `model.tflite` file. This prepares the model for on-device deployment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
model.export_model()
!ls exported_model
files.download('exported_model/model.tflite')
```

----------------------------------------

TITLE: Performing Language Detection Inference with MediaPipe Python API
DESCRIPTION: This comprehensive snippet demonstrates the full workflow for language detection using MediaPipe. It imports required modules, initializes the LanguageDetector with the downloaded model, performs detection on INPUT_TEXT, and then iterates through the results to print detected languages and their confidence scores. It outlines the core steps for using the MediaPipe Text API.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/language_detector/python/[MediaPipe_Python_Tasks]_Language_Detector.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
from mediapipe.tasks import python
from mediapipe.tasks.python import text

# STEP 2: Create a LanguageDetector object.
base_options = python.BaseOptions(model_asset_path="detector.tflite")
options = text.LanguageDetectorOptions(base_options=base_options)
detector = text.LanguageDetector.create_from_options(options)

# STEP 3: Get the language detcetion result for the input text.
detection_result = detector.detect(INPUT_TEXT)

# STEP 4: Process the detection result and print the languages detected and
# their scores.

for detection in detection_result.detections:
  print(f'{detection.language_code}: ({detection.probability:.2f})')
```

----------------------------------------

TITLE: Generating Text with LLM Pipeline - Python
DESCRIPTION: This method orchestrates the text generation process for an LLM. It first tokenizes the input prompt, then performs a prefill step to process the initial prompt and generate a KV cache. Finally, it calls the `_run_decode` method to generate the response tokens, returning the complete decoded text. It takes a `prompt` string and an optional `max_decode_steps` integer.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
def generate(self, prompt: str, max_decode_steps: int | None = None) -> str:
    messages=[{ 'role': 'user', 'content': prompt}]
    token_ids = self._tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
    # Initialize the prefill runner with the suitable input size.
    self._init_prefill_runner(len(token_ids))

    # Run prefill.
    # Prefill up to the seond to the last token of the prompt, because the last
    # token of the prompt will be used to bootstrap decode.
    prefill_token_length = len(token_ids) - 1

    print('Running prefill')
    kv_cache = self._run_prefill(token_ids[:prefill_token_length])
    # Run decode.
    print('Running decode')
    actual_max_decode_steps = self._max_kv_cache_seq_len - prefill_token_length - 1
    if max_decode_steps is not None:
      actual_max_decode_steps = min(actual_max_decode_steps, max_decode_steps)
    decode_text = self._run_decode(
        prefill_token_length,
        token_ids[prefill_token_length],
        kv_cache,
        actual_max_decode_steps,
    )
    return decode_text
```

----------------------------------------

TITLE: Creating a MediaPipe Task Bundle for a TFLite Model
DESCRIPTION: This Python snippet demonstrates how to create a task bundle for a converted TFLite model using the `mediapipe.tasks.python.genai.bundler`. It configures the `BundleConfig` with paths to the TFLite model and tokenizer, start/stop tokens, output filename, and an option for bytes-to-unicode mapping, then calls `create_bundle`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/bundling/llm_bundling.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
tflite_model="PATH/gemma.tflite" # @param {type:"string"}
tokenizer_model="PATH/tokenizer.model" # @param {type:"string"}
start_token="<bos>" # @param {type:"string"}
stop_token="<eos>" # @param {type:"string"}
output_filename="PATH/gemma.task" # @param {type:"string"}
enable_bytes_to_unicode_mapping=False # @param ["False", "True"] {type:"raw"}

config = bundler.BundleConfig(
    tflite_model=tflite_model,
    tokenizer_model=tokenizer_model,
    start_token=start_token,
    stop_tokens=[stop_token],
    output_filename=output_filename,
    enable_bytes_to_unicode_mapping=enable_bytes_to_unicode_mapping,
)
bundler.create_bundle(config)
```

----------------------------------------

TITLE: Performing Object Detection Inference with MediaPipe - Python
DESCRIPTION: This code demonstrates the complete process of running object detection using MediaPipe. It initializes an `ObjectDetector` with a pre-trained model, loads an image, performs the detection, and then visualizes the results by drawing bounding boxes and labels on a copy of the original image, finally displaying the annotated image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
import numpy as np
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an ObjectDetector object.
base_options = python.BaseOptions(model_asset_path='efficientdet.tflite')
options = vision.ObjectDetectorOptions(base_options=base_options,
                                       score_threshold=0.5)
detector = vision.ObjectDetector.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file(IMAGE_FILE)

# STEP 4: Detect objects in the input image.
detection_result = detector.detect(image)

# STEP 5: Process the detection result. In this case, visualize it.
image_copy = np.copy(image.numpy_view())
annotated_image = visualize(image_copy, detection_result)
rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
cv2_imshow(rgb_annotated_image)
```

----------------------------------------

TITLE: Performing Image Embedding and Cosine Similarity (MediaPipe Python)
DESCRIPTION: This snippet demonstrates how to initialize the MediaPipe `ImageEmbedder` with a downloaded model and custom options (L2 normalization, quantization). It then uses the embedder to generate embeddings for two images and calculates their cosine similarity, which indicates how alike the images are.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_embedder/python/image_embedder.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# Create options for Image Embedder
base_options = python.BaseOptions(model_asset_path='embedder.tflite')
l2_normalize = True #@param {type:"boolean"}
quantize = True #@param {type:"boolean"}
options = vision.ImageEmbedderOptions(
    base_options=base_options, l2_normalize=l2_normalize, quantize=quantize)


# Create Image Embedder
with vision.ImageEmbedder.create_from_options(options) as embedder:

  # Format images for MediaPipe
  first_image = mp.Image.create_from_file(IMAGE_FILENAMES[0])
  second_image = mp.Image.create_from_file(IMAGE_FILENAMES[1])
  first_embedding_result = embedder.embed(first_image)
  second_embedding_result = embedder.embed(second_image)

  # Calculate and print similarity
  similarity = vision.ImageEmbedder.cosine_similarity(
      first_embedding_result.embeddings[0],
      second_embedding_result.embeddings[0])
  print(similarity)
```

----------------------------------------

TITLE: Evaluating MediaPipe Gesture Recognizer Model Performance (Python)
DESCRIPTION: This snippet evaluates the performance of the trained gesture recognizer model using the `test_data` and a specified `batch_size`. It returns the loss and accuracy metrics, which are then printed to the console, providing an assessment of the model's generalization ability.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
loss, acc = model.evaluate(test_data, batch_size=1)
print(f"Test loss:{loss}, Test accuracy:{acc}")
```

----------------------------------------

TITLE: Running Face Stylizer Inference - Python
DESCRIPTION: This code initializes the MediaPipe `FaceStylizer` using the downloaded model and applies the pre-trained style to the input image. It demonstrates how to perform inference using the `stylize` method and then displays the resulting stylized image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_stylizer/python/face_stylizer.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
import numpy as np
import mediapipe as mp

from mediapipe.tasks import python
from mediapipe.tasks.python import vision


# Create the options that will be used for FaceStylizer
base_options = python.BaseOptions(model_asset_path='face_stylizer.task')
options = vision.FaceStylizerOptions(base_options=base_options)

# Create the face stylizer
with vision.FaceStylizer.create_from_options(options) as stylizer:

  # Loop through demo image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe image file that will be stylized
    image = mp.Image.create_from_file(image_file_name)
    # Retrieve the stylized image
    stylized_image = stylizer.stylize(image)

    # Show the stylized image
    rgb_stylized_image = cv2.cvtColor(stylized_image.numpy_view(), cv2.COLOR_BGR2RGB)
    resize_and_show(rgb_stylized_image)
```

----------------------------------------

TITLE: Merging LoRA Weights into Base Model - PEFT - Python
DESCRIPTION: This snippet loads the fine-tuned PEFT model and merges its LoRA adapter weights back into the base model. This process creates a single, consolidated model that can be deployed without requiring the separate PEFT adapter, making it suitable for on-device inference or other deployment scenarios.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
from peft import AutoPeftModelForCausalLM
import torch

# Load PEFT model on CPU
model = AutoPeftModelForCausalLM.from_pretrained("gemma3-1b-sft")
# Merge LoRA and base model and save
merged_model = model.merge_and_unload()
```

----------------------------------------

TITLE: Building MediaPipe LLM Task Bundle for Gemma (Python)
DESCRIPTION: This function `build_gemma3_1b_it_q8` creates a MediaPipe LLM task bundle (`.task` file) by configuring the TFLite model, tokenizer, and specific LLM parameters like start/stop tokens and prompt prefixes/suffixes. It uses `llm_bundler.create_bundle` to generate the final deployable task file, which is then saved to `/content/gemma3_1b_it_q8_ekv1280.task`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_31

LANGUAGE: python
CODE:
```
from mediapipe.tasks.python.genai.bundler import llm_bundler

def build_gemma3_1b_it_q8():
  output_file = "/content/gemma3_1b_it_q8_ekv1280.task"
  tflite_model = "/content/gemma3_1b_finetune_q8_ekv1024.tflite"
  tokenizer_model = (
      "/content/tokenizer.model"
  )
  config = llm_bundler.BundleConfig(
      tflite_model=tflite_model,
      tokenizer_model=tokenizer_model,
      start_token="<bos>",
      stop_tokens=["<eos>"],
      output_filename=output_file,
      enable_bytes_to_unicode_mapping=False,
      prompt_prefix="<start_of_turn>user\n",
      prompt_suffix="<end_of_turn>\n<start_of_turn>model\n"
  )
  llm_bundler.create_bundle(config)

# Build the MediaPipe task bundle.
build_gemma3_1b_it_q8()
```

----------------------------------------

TITLE: Performing Image Segmentation and Mask Visualization with MediaPipe
DESCRIPTION: This code initializes the MediaPipe `ImageSegmenter` using the downloaded DeepLab v3 model. It then processes the test image to generate a category mask, which is used to create a visual representation where the foreground and background are highlighted with distinct colors (white and gray, respectively). It depends on `numpy`, `mediapipe`, and the `resize_and_show` function.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np
import mediapipe as mp

from mediapipe.tasks import python
from mediapipe.tasks.python import vision


BG_COLOR = (192, 192, 192) # gray
MASK_COLOR = (255, 255, 255) # white


# Create the options that will be used for ImageSegmenter
base_options = python.BaseOptions(model_asset_path='deeplabv3.tflite')
options = vision.ImageSegmenterOptions(base_options=base_options,
                                       output_category_mask=True)

# Create the image segmenter
with vision.ImageSegmenter.create_from_options(options) as segmenter:

  # Loop through demo image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe image file that will be segmented
    image = mp.Image.create_from_file(image_file_name);

    # Retrieve the masks for the segmented image
    segmentation_result = segmenter.segment(image);
    category_mask = segmentation_result.category_mask;

    # Generate solid color images for showing the output segmentation mask.
    image_data = image.numpy_view();
    fg_image = np.zeros(image_data.shape, dtype=np.uint8);
    fg_image[:] = MASK_COLOR;
    bg_image = np.zeros(image_data.shape, dtype=np.uint8);
    bg_image[:] = BG_COLOR;

    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.2;
    output_image = np.where(condition, fg_image, bg_image);

    print(f'Segmentation mask of {name}:');
    resize_and_show(output_image)
```

----------------------------------------

TITLE: PASCAL VOC Dataset Directory and XML Structure
DESCRIPTION: This snippet describes the PASCAL VOC dataset format, featuring a `data` directory for images and an `Annotations` directory for per-image XML annotation files. It also provides the XML schema for these annotation files, detailing filename, object names, and bounding box coordinates.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_5

LANGUAGE: Text
CODE:
```
<dataset_dir>/
  data/
    <file0>.<jpg/jpeg>
    ...
  Annotations/
    <file0>.xml
    ...
```

LANGUAGE: XML
CODE:
```
<annotation>
  <filename>file0.jpg</filename>
  <object>
    <name>kangaroo</name>
    <bndbox>
      <xmin>233</xmin>
      <ymin>89</ymin>
      <xmax>386</xmax>
      <ymax>262</ymax>
    </bndbox>
  </object>
  <object>
    ...
  </object>
  ...
</annotation>
```

----------------------------------------

TITLE: Training MediaPipe Gesture Recognizer with Custom Hyperparameters (Python)
DESCRIPTION: This snippet demonstrates how to train a new gesture recognizer model with customized hyperparameters. It sets a specific `learning_rate` and `export_dir` via `HParams`, and a `dropout_rate` via `ModelOptions`, then combines them into `GestureRecognizerOptions` before creating and training the model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir="exported_model_2")
model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)
options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)
model_2 = gesture_recognizer.GestureRecognizer.create(
    train_data=train_data,
    validation_data=validation_data,
    options=options
)
```

----------------------------------------

TITLE: Drawing Hand Landmarks on Image - Python
DESCRIPTION: This Python function `draw_landmarks_on_image` visualizes detected hand landmarks and handedness on an input RGB image. It iterates through detected hands, draws landmarks and connections using MediaPipe's drawing utilities, and adds handedness text using OpenCV. It requires `mediapipe`, `numpy`, and `cv2` for image processing and drawing.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np

MARGIN = 10  # pixels
FONT_SIZE = 1
FONT_THICKNESS = 1
HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green

def draw_landmarks_on_image(rgb_image, detection_result):
  hand_landmarks_list = detection_result.hand_landmarks
  handedness_list = detection_result.handedness
  annotated_image = np.copy(rgb_image)

  # Loop through the detected hands to visualize.
  for idx in range(len(hand_landmarks_list)):
    hand_landmarks = hand_landmarks_list[idx]
    handedness = handedness_list[idx]

    # Draw the hand landmarks.
    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
    hand_landmarks_proto.landmark.extend([
      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks
    ])
    solutions.drawing_utils.draw_landmarks(
      annotated_image,
      hand_landmarks_proto,
      solutions.hands.HAND_CONNECTIONS,
      solutions.drawing_styles.get_default_hand_landmarks_style(),
      solutions.drawing_styles.get_default_hand_connections_style())

    # Get the top left corner of the detected hand's bounding box.
    height, width, _ = annotated_image.shape
    x_coordinates = [landmark.x for landmark in hand_landmarks]
    y_coordinates = [landmark.y for landmark in hand_landmarks]
    text_x = int(min(x_coordinates) * width)
    text_y = int(min(y_coordinates) * height) - MARGIN

    # Draw handedness (left or right hand) on the image.
    cv2.putText(annotated_image, f"{handedness[0].category_name}",
                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,
                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)

  return annotated_image
```

----------------------------------------

TITLE: Running Inference with Hugging Face Pipeline (Python)
DESCRIPTION: This code demonstrates how to perform text generation inference using the Hugging Face `transformers` pipeline. It initializes a text generation pipeline with the `merged_model` and `tokenizer`, then generates a response for a given `prompt` with a maximum of 100 new tokens.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
from transformers import pipeline

prompt = "What is the primary function of mitochondria within a cell?"
pipe = pipeline("text-generation", model=merged_model, tokenizer=tokenizer)
pipe(prompt, max_new_tokens=100)
```

----------------------------------------

TITLE: Performing Face Landmark Detection with MediaPipe in Python
DESCRIPTION: This snippet demonstrates the end-to-end process of setting up a MediaPipe FaceLandmarker, loading an image, detecting face landmarks, and visualizing the results. It includes importing necessary modules, configuring the detector with options like blendshapes and transformation matrices, and then processing the output for display.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an FaceLandmarker object.
base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')
options = vision.FaceLandmarkerOptions(base_options=base_options,
                                       output_face_blendshapes=True,
                                       output_facial_transformation_matrixes=True,
                                       num_faces=1)
detector = vision.FaceLandmarker.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file("image.png")

# STEP 4: Detect face landmarks from the input image.
detection_result = detector.detect(image)

# STEP 5: Process the detection result. In this case, visualize it.
annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)
cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))
```

----------------------------------------

TITLE: Creating and Splitting Image Classification Dataset (Python)
DESCRIPTION: This code demonstrates how to load image data into a `Dataset` object using `image_classifier.Dataset.from_folder` and then split it into training, testing, and validation sets. The data is initially split 80% for training, with the remaining 20% further split equally into 10% for testing and 10% for validation, preparing it for model retraining.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
data = image_classifier.Dataset.from_folder(image_path)
train_data, remaining_data = data.split(0.8)
test_data, validation_data = remaining_data.split(0.5)
```

----------------------------------------

TITLE: Running Greedy Decode for LLM - Python
DESCRIPTION: This function performs the token decoding process using a greedy sampler. It iteratively predicts the next token based on the current KV cache and input position, stopping when an end-of-sequence token is encountered or `max_decode_steps` is reached. It requires `start_pos`, `start_token_id`, `kv_cache` (from prefill), and `max_decode_steps` as inputs, returning the concatenated decoded text.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
next_pos = start_pos
next_token = start_token_id
decode_text = []
decode_inputs = kv_cache

for _ in range(max_decode_steps):
  decode_inputs.update({
      "tokens": np.array([[next_token]], dtype=np.int32),
      "input_pos": np.array([next_pos], dtype=np.int32),
  })
  decode_outputs = self._decode_runner(**decode_inputs)
  # Output logits has shape (batch=1, 1, vocab_size). We only take the first
  # element.
  logits = decode_outputs.pop("logits")[0][0]
  next_token = self._greedy_sampler(logits)
  if next_token == self._tokenizer.eos_token_id:
    break
  decode_text.append(self._tokenizer.decode(next_token, skip_special_tokens=False))
  print(decode_text[-1], end='', flush=True)
  # Decode outputs includes logits and kv cache. We already poped out
  # logits, so the rest is kv cache. We pass the updated kv cache as input
  # to the next decode step.
  decode_inputs = decode_outputs
  next_pos += 1

print() # print a new line at the end.
return ''.join(decode_text)
```

----------------------------------------

TITLE: Visualizing COCO Dataset Images and Bounding Boxes with Matplotlib
DESCRIPTION: This Python code defines functions to visualize images from a COCO dataset along with their bounding box annotations using Matplotlib. It includes utilities for drawing outlines, boxes, and text labels, then orchestrates loading image and annotation data from `labels.json` to display a specified number of example images with their corresponding object detections. Requires `matplotlib`, `collections`, and `math`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
#@title Visualize the training dataset
import matplotlib.pyplot as plt
from matplotlib import patches, text, patheffects
from collections import defaultdict
import math

def draw_outline(obj):
  obj.set_path_effects([patheffects.Stroke(linewidth=4,  foreground='black'), patheffects.Normal()])
def draw_box(ax, bb):
  patch = ax.add_patch(patches.Rectangle((bb[0],bb[1]), bb[2], bb[3], fill=False, edgecolor='red', lw=2))
  draw_outline(patch)
def draw_text(ax, bb, txt, disp):
  text = ax.text(bb[0],(bb[1]-disp),txt,verticalalignment='top'
  ,color='white',fontsize=10,weight='bold')
  draw_outline(text)
def draw_bbox(ax, annotations_list, id_to_label, image_shape):
  for annotation in annotations_list:
    cat_id = annotation["category_id"]
    bbox = annotation["bbox"]
    draw_box(ax, bbox)
    draw_text(ax, bbox, id_to_label[cat_id], image_shape[0] * 0.05)
def visualize(dataset_folder, max_examples=None):
  with open(os.path.join(dataset_folder, "labels.json"), "r") as f:
    labels_json = json.load(f)
  images = labels_json["images"]
  cat_id_to_label = {item["id"]:item["name"] for item in labels_json["categories"]}
  image_annots = defaultdict(list)
  for annotation_obj in labels_json["annotations"]:
    image_id = annotation_obj["image_id"]
    image_annots[image_id].append(annotation_obj)

  if max_examples is None:
    max_examples = len(image_annots.items())
  n_rows = math.ceil(max_examples / 3)
  fig, axs = plt.subplots(n_rows, 3, figsize=(24, n_rows*8)) # 3 columns(2nd index), 8x8 for each image
  for ind, (image_id, annotations_list) in enumerate(list(image_annots.items())[:max_examples]):
    ax = axs[ind//3, ind%3]
    img = plt.imread(os.path.join(dataset_folder, "images", images[image_id]["file_name"]))
    ax.imshow(img)
    draw_bbox(ax, annotations_list, cat_id_to_label, img.shape)
  plt.show()

visualize(train_dataset_path, 9)
```

----------------------------------------

TITLE: Blurring Image Background with MediaPipe Python
DESCRIPTION: This example demonstrates how to apply a background blur effect using MediaPipe's `InteractiveSegmenter`. It segments an image based on a keypoint ROI, then uses the resulting category mask to selectively apply a Gaussian blur to the background while keeping the foreground sharp. The keypoint is also drawn on the output image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
# Blur the image background based on the segmentation mask.

# Create the segmenter
with python.vision.InteractiveSegmenter.create_from_options(options) as segmenter:

  # Loop through available image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe Image
    image = mp.Image.create_from_file(image_file_name)

    # Retrieve the category masks for the image
    roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,
                           keypoint=NormalizedKeypoint(x, y))
    segmentation_result = segmenter.segment(image, roi)
    category_mask = segmentation_result.category_mask

    # Convert the BGR image to RGB
    image_data = cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB)

    # Apply effects
    blurred_image = cv2.GaussianBlur(image_data, (55,55), 0)
    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1
    output_image = np.where(condition, image_data, blurred_image)

    # Draw a white dot with black border to denote the point of interest
    thickness, radius = 6, -1
    keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)
    cv2.circle(output_image, keypoint_px, thickness + 5, (0, 0, 0), radius)
    cv2.circle(output_image, keypoint_px, thickness, (255, 255, 255), radius)

    print(f'Blurred background of {image_file_name}:')
    resize_and_show(output_image)
```

----------------------------------------

TITLE: Performing Hand Landmark Detection - MediaPipe Python
DESCRIPTION: This snippet demonstrates the core steps for performing hand landmark detection using MediaPipe Tasks. It imports necessary modules, creates an `HandLandmarker` object with specified options (like the model path and number of hands), loads an image, and then executes the detection process to obtain `detection_result`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an HandLandmarker object.
base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')
options = vision.HandLandmarkerOptions(base_options=base_options,
                                       num_hands=2)
detector = vision.HandLandmarker.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file("image.jpg")

# STEP 4: Detect hand landmarks from the input image.
detection_result = detector.detect(image)
```

----------------------------------------

TITLE: Exporting an FP16 Quantized Model with MediaPipe Model Maker (Python)
DESCRIPTION: This snippet demonstrates how to export a model with post-training float16 quantization. It first ensures the model is in its float state using `restore_float_ckpt()` if QAT was previously run, then applies the `quantization_config` during export. It also shows how to list and download the exported file.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_18

LANGUAGE: Python
CODE:
```
model.restore_float_ckpt()
model.export_model(model_name="model_fp16.tflite", quantization_config=quantization_config)
!ls -lh exported_model
files.download('exported_model/model_fp16.tflite')
```

----------------------------------------

TITLE: Exporting Average Word Embedding Text Classifier and Labels (Python)
DESCRIPTION: This snippet exports the trained average word embedding model as a TFLite file using `model.export_model()`. It also exports the training labels to the specified `export_dir` using `model.export_labels()` for on-device applications.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
model.export_model()
model.export_labels(export_dir=options.hparams.export_dir)
```

----------------------------------------

TITLE: Generating StableLM 3B Conversion Configuration
DESCRIPTION: Generates a `converter.ConversionConfig` object for the StableLM 3B model. This configuration defines the input checkpoint path, vocabulary file location, output directory, and the final TFLite binary path, tailored for the specified backend (CPU/GPU).
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
def stablelm_convert_config(backend):
  input_ckpt = '/content/stablelm-3b-4e1t/'
  vocab_model_file = '/content/stablelm-3b-4e1t/'
  output_dir = '/content/intermediate/stablelm-3b-4e1t/'
  output_tflite_file = f'/content/converted_models/stablelm_{backend}.bin'
```

----------------------------------------

TITLE: Generating Gemma 7B Conversion Configuration
DESCRIPTION: Generates a `converter.ConversionConfig` object for the Gemma 7B model. This configuration defines the input checkpoint path, vocabulary file location, output directory, and the final TFLite binary path, tailored for the specified backend (CPU/GPU).
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
def gemma7b_convert_config(backend):
  input_ckpt = '/content//gemma-1.1-7b-it/'
  vocab_model_file = '/content//gemma-1.1-7b-it/'
  output_dir = '/content/intermediate//gemma-1.1-7b-it/'
  output_tflite_file = f'/content/converted_models/gemma_{backend}.bin'
  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='GEMMA_7B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)
```

----------------------------------------

TITLE: Configuring Retraining Options for MediaPipe Object Detector (Python)
DESCRIPTION: This snippet configures the essential parameters for retraining an object detection model using MediaPipe Model Maker. It specifies the model architecture (MobileNet-MultiHW-AVG) and the output directory for the exported model, encapsulating these settings into an `ObjectDetectorOptions` object. This setup is a prerequisite for initiating the model retraining process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG
hparams = object_detector.HParams(export_dir='exported_model')
options = object_detector.ObjectDetectorOptions(
    supported_model=spec,
    hparams=hparams
)
```

----------------------------------------

TITLE: Highlighting Segmented Object with Color Overlay in MediaPipe Python
DESCRIPTION: This snippet shows how to highlight a segmented object with a custom overlay color. It uses the `InteractiveSegmenter` to obtain a category mask, then creates an overlay image of a specified color. The original image and the overlay are blended using the segmentation mask as an alpha channel, effectively coloring the segmented object. The keypoint is also drawn on the output image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
OVERLAY_COLOR = (100, 100, 0) # cyan

# Create the segmenter
with python.vision.InteractiveSegmenter.create_from_options(options) as segmenter:

  # Loop through available image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe Image
    image = mp.Image.create_from_file(image_file_name)

    # Retrieve the category masks for the image
    roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,
                           keypoint=NormalizedKeypoint(x, y))
    segmentation_result = segmenter.segment(image, roi)
    category_mask = segmentation_result.category_mask

    # Convert the BGR image to RGB
    image_data = cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB)

    # Create an overlay image with the desired color (e.g., (255, 0, 0) for red)
    overlay_image = np.zeros(image_data.shape, dtype=np.uint8)
    overlay_image[:] = OVERLAY_COLOR

    # Create the condition from the category_masks array
    alpha = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1

    # Create an alpha channel from the condition with the desired opacity (e.g., 0.7 for 70%)
    alpha = alpha.astype(float) * 0.7

    # Blend the original image and the overlay image based on the alpha channel
    output_image = image_data * (1 - alpha) + overlay_image * alpha
    output_image = output_image.astype(np.uint8)

    # Draw a white dot with black border to denote the point of interest
    thickness, radius = 6, -1
    keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)
    cv2.circle(output_image, keypoint_px, thickness + 5, (0, 0, 0), radius)
    cv2.circle(output_image, keypoint_px, thickness, (255, 255, 255), radius)

    print(f'{image_file_name}:')
    resize_and_show(output_image)
```

----------------------------------------

TITLE: Evaluating Object Detection Model Performance
DESCRIPTION: This snippet evaluates the trained object detection model using the validation dataset. It calculates the validation loss and COCO metrics, providing insights into the model's performance on unseen data, with a specified 'batch_size' of 4 for evaluation.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
loss, coco_metrics = model.evaluate(val_data, batch_size=4)
print(f"Validation loss: {loss}")
print(f"Validation coco metrics: {coco_metrics}")
```

----------------------------------------

TITLE: Configuring LoRA for Causal LM - PEFT - Python
DESCRIPTION: This snippet configures LoRA (Low-Rank Adaptation) for fine-tuning a causal language model. It disables Weights & Biases logging and defines a `LoraConfig` object, specifying the rank (`r`), target attention modules for adaptation, and the task type as `CAUSAL_LM`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
os.environ["WANDB_DISABLED"] = "true"

from peft import LoraConfig, PeftModel

lora_config = LoraConfig(
    r=16,
    target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"],
    task_type="CAUSAL_LM",
)
```

----------------------------------------

TITLE: Defining Pose Landmark Visualization Utility in Python
DESCRIPTION: This Python function, `draw_landmarks_on_image`, is designed to overlay detected pose landmarks and their connections onto an RGB image. It processes the `detection_result` by iterating through each detected pose, converting the landmarks into a `NormalizedLandmarkList` protocol buffer, and then using MediaPipe's drawing utilities to render them on a copy of the original image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.

from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np


def draw_landmarks_on_image(rgb_image, detection_result):
  pose_landmarks_list = detection_result.pose_landmarks
  annotated_image = np.copy(rgb_image)

  # Loop through the detected poses to visualize.
  for idx in range(len(pose_landmarks_list)):
    pose_landmarks = pose_landmarks_list[idx]

    # Draw the pose landmarks.
    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
    pose_landmarks_proto.landmark.extend([
      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks
    ])
    solutions.drawing_utils.draw_landmarks(
      annotated_image,
      pose_landmarks_proto,
      solutions.pose.POSE_CONNECTIONS,
      solutions.drawing_styles.get_default_pose_landmarks_style())
  return annotated_image
```

----------------------------------------

TITLE: MediaPipe Face Detection Visualization Utilities - Python
DESCRIPTION: This Python code defines utility functions for visualizing face detection results. The `_normalized_to_pixel_coordinates` function converts normalized coordinates to pixel coordinates, while the `visualize` function draws bounding boxes and keypoints on an input image based on the `detection_result` from MediaPipe.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from typing import Tuple, Union
import math
import cv2
import numpy as np

MARGIN = 10  # pixels
ROW_SIZE = 10  # pixels
FONT_SIZE = 1
FONT_THICKNESS = 1
TEXT_COLOR = (255, 0, 0)  # red


def _normalized_to_pixel_coordinates(
    normalized_x: float, normalized_y: float, image_width: int,
    image_height: int) -> Union[None, Tuple[int, int]]:
  """Converts normalized value pair to pixel coordinates."""

  # Checks if the float value is between 0 and 1.
  def is_valid_normalized_value(value: float) -> bool:
    return (value > 0 or math.isclose(0, value)) and (value < 1 or
                                                      math.isclose(1, value))

  if not (is_valid_normalized_value(normalized_x) and
          is_valid_normalized_value(normalized_y)):
    # TODO: Draw coordinates even if it's outside of the image bounds.
    return None
  x_px = min(math.floor(normalized_x * image_width), image_width - 1)
  y_px = min(math.floor(normalized_y * image_height), image_height - 1)
  return x_px, y_px


def visualize(
    image,
    detection_result
) -> np.ndarray:
  """Draws bounding boxes and keypoints on the input image and return it.
  Args:
    image: The input RGB image.
    detection_result: The list of all "Detection" entities to be visualize.
  Returns:
    Image with bounding boxes.
  """
  annotated_image = image.copy()
  height, width, _ = image.shape

  for detection in detection_result.detections:
    # Draw bounding_box
    bbox = detection.bounding_box
    start_point = bbox.origin_x, bbox.origin_y
    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height
    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)

    # Draw keypoints
    for keypoint in detection.keypoints:
      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,
                                                     width, height)
      color, thickness, radius = (0, 255, 0), 2, 2
      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)

    # Draw label and score
    category = detection.categories[0]
    category_name = category.category_name
    category_name = '' if category_name is None else category_name
    probability = round(category.score, 2)
    result_text = category_name + ' (' + str(probability) + ')'
    text_location = (MARGIN + bbox.origin_x,
                     MARGIN + ROW_SIZE + bbox.origin_y)
    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,
                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)

  return annotated_image
```

----------------------------------------

TITLE: Configuring Post-Training FP16 Quantization (Python)
DESCRIPTION: This snippet illustrates how to create a `QuantizationConfig` object specifically for float16 post-training quantization using `quantization.QuantizationConfig.for_float16()`. This configuration is then used to modify a trained model for reduced size and improved inference speed.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_17

LANGUAGE: Python
CODE:
```
quantization_config = quantization.QuantizationConfig.for_float16()
```

----------------------------------------

TITLE: Running MediaPipe Image Classifier with Custom Parameters (Python)
DESCRIPTION: This command demonstrates how to run the `classify.py` script with optional parameters to customize the image classification process. It specifies a custom TensorFlow Lite model (`--model`), limits the number of classification results (`--maxResults`), and sets a minimum score threshold (`--scoreThreshold`) for the output.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 classify.py \
  --model efficientnet_lite0.tflite \
  --maxResults 5 \
  --scoreThreshold 0.5
```

----------------------------------------

TITLE: Configuring BERT Text Classifier Options (Python)
DESCRIPTION: This snippet configures `TextClassifierOptions` for a MobileBERT-based text classifier. It sets `supported_model` to `MOBILEBERT_CLASSIFIER` and defines BERT-specific hyperparameters like `epochs`, `batch_size`, `learning_rate`, and `export_dir` using `BertHParams`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
supported_model = text_classifier.SupportedModels.MOBILEBERT_CLASSIFIER
hparams = text_classifier.BertHParams(epochs=2, batch_size=48, learning_rate=3e-5, export_dir="bert_exported_models")
options = text_classifier.TextClassifierOptions(supported_model=supported_model, hparams=hparams)
```

----------------------------------------

TITLE: Loading Gemma 3.1B IT Model with LiteRT Pipeline
DESCRIPTION: This Python code imports the `pipeline` module from `litert_tools` and loads the Gemma 3.1B IT model using its community identifier and a specific task file, preparing it for inference.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
from litert_tools.pipeline import pipeline
runner = pipeline.load("litert-community/Gemma3-1B-IT", "Gemma3-1B-IT_seq128_q8_ekv1280.task")
```

----------------------------------------

TITLE: Initializing Hugging Face AutoTokenizer for Gemma (Python)
DESCRIPTION: This code initializes a `transformers.AutoTokenizer` for the `google/gemma-3-1b-pt` model. It then sets a custom `chat_template` to enforce specific conversation formatting, ensuring roles alternate correctly and handling system messages by raising an exception.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_16

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer

model_id = 'google/gemma-3-1b-pt'
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.chat_template = "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
```

----------------------------------------

TITLE: Defining int8 Quantization Configuration (Python)
DESCRIPTION: Creates a `QuantizationConfig` object configured for 8-bit integer (`int8`) quantization using the `for_int8()` class method. This configuration specifies how the model's data types will be reduced, requiring a `train_data` representative dataset for the calibration process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
quantization_config = quantization.QuantizationConfig.for_int8(train_data)
```

----------------------------------------

TITLE: Loading, Classifying, and Processing Images with MediaPipe - Python
DESCRIPTION: This snippet iterates through a list of image filenames, loads each image using `mp.Image.create_from_file`, and then performs classification using the initialized `classifier` object. It processes the classification result by extracting the top category name and score, storing them for later visualization. The `display_batch_of_images` function is then called to show the results.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
images = []
predictions = []
for image_name in IMAGE_FILENAMES:
  # STEP 3: Load the input image.
  image = mp.Image.create_from_file(image_name)

  # STEP 4: Classify the input image.
  classification_result = classifier.classify(image)

  # STEP 5: Process the classification result. In this case, visualize it.
  images.append(image)
  top_category = classification_result.classifications[0].categories[0]
  predictions.append(f"{top_category.category_name} ({top_category.score:.2f})")

display_batch_of_images(images, predictions)
```

----------------------------------------

TITLE: Running MediaPipe Gesture Recognizer with Default Parameters
DESCRIPTION: This command initiates the MediaPipe gesture recognition application using Python. By default, it utilizes `gesture_recognizer.task` as the model, detects a single hand, and applies default confidence thresholds for detection, presence, and tracking.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/raspberry_pi/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
python3 recognize.py
```

----------------------------------------

TITLE: Running MediaPipe Object Detection with Default Model in Python
DESCRIPTION: This Python command executes the `detect.py` script to start real-time object detection using the specified TensorFlow Lite model, `efficientdet_lite0.tflite`. It displays the camera feed with detected objects, labels, and scores on a connected monitor.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 detect.py \
  --model efficientdet_lite0.tflite
```

----------------------------------------

TITLE: Running MediaPipe Face Detection with Default Model (Python)
DESCRIPTION: This Python command executes the `detect.py` script to start the real-time face detection application. It specifies `detector.tflite` as the TensorFlow Lite model to be used for inference, displaying results on a connected monitor.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 detect.py \
  --model detector.tflite
```

----------------------------------------

TITLE: Rerunning Quantization Aware Training after Restoring Checkpoint (Python)
DESCRIPTION: This snippet illustrates how to re-run Quantization Aware Training (QAT) without retraining the base float model. It uses `model.restore_float_ckpt()` to revert the model to its fully trained float state before applying new QAT hyperparameters and re-evaluating.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
new_qat_hparams = object_detector.QATHParams(learning_rate=0.9, batch_size=4, epochs=15, decay_steps=5, decay_rate=0.96)
model.restore_float_ckpt()
model.quantization_aware_training(train_data, validation_data, qat_hparams=new_qat_hparams)
qat_loss, qat_coco_metrics = model.evaluate(validation_data)
print(f"QAT validation loss: {qat_loss}")
print(f"QAT validation coco metrics: {qat_coco_metrics}")
```

----------------------------------------

TITLE: Configuring Face Stylizer Retraining Options in Python
DESCRIPTION: This snippet demonstrates how to configure the `FaceStylizerOptions` for retraining a face stylizer model. It specifies the model architecture (`BLAZE_FACE_STYLIZER_256`), defines `swap_layers` to control style application, and sets hyperparameters like `learning_rate`, `epochs`, `batch_size`, and an `export_dir` for the retrained model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
face_stylizer_options = face_stylizer.FaceStylizerOptions(
  model=face_stylizer.SupportedModels.BLAZE_FACE_STYLIZER_256,
  model_options=face_stylizer.ModelOptions(swap_layers=[10,11]),
  hparams=face_stylizer.HParams(
      learning_rate=8e-4, epochs=200, batch_size=2, export_dir="exported_model"
  )
)
```

----------------------------------------

TITLE: Utility Functions for Face Landmark Visualization - Python
DESCRIPTION: This snippet defines two Python functions: draw_landmarks_on_image and plot_face_blendshapes_bar_graph. draw_landmarks_on_image overlays detected face landmarks onto an image, while plot_face_blendshapes_bar_graph visualizes face blendshape scores as a bar graph. These utilities are crucial for interpreting MediaPipe's face landmark detection output.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
#@markdown We implemented some functions to visualize the face landmark detection results. <br/> Run the following cell to activate the functions.

from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np
import matplotlib.pyplot as plt


def draw_landmarks_on_image(rgb_image, detection_result):
  face_landmarks_list = detection_result.face_landmarks
  annotated_image = np.copy(rgb_image)

  # Loop through the detected faces to visualize.
  for idx in range(len(face_landmarks_list)):
    face_landmarks = face_landmarks_list[idx]

    # Draw the face landmarks.
    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
    face_landmarks_proto.landmark.extend([
      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks
    ])

    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,
        landmark_drawing_spec=None,
        connection_drawing_spec=mp.solutions.drawing_styles
        .get_default_face_mesh_tesselation_style())
    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,
        landmark_drawing_spec=None,
        connection_drawing_spec=mp.solutions.drawing_styles
        .get_default_face_mesh_contours_style())
    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_IRISES,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp.solutions.drawing_styles
          .get_default_face_mesh_iris_connections_style())

  return annotated_image

def plot_face_blendshapes_bar_graph(face_blendshapes):
  # Extract the face blendshapes category names and scores.
  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]
  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]
  # The blendshapes are ordered in decreasing score value.
  face_blendshapes_ranks = range(len(face_blendshapes_names))

  fig, ax = plt.subplots(figsize=(12, 12))
  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])
  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)
  ax.invert_yaxis()

  # Label each bar with values
  for score, patch in zip(face_blendshapes_scores, bar.patches):
    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f"{score:.4f}", va="top")

  ax.set_xlabel('Score')
  ax.set_title("Face Blendshapes")
  plt.tight_layout()
  plt.show()
```

----------------------------------------

TITLE: Defining Object Detection Visualization Function - Python
DESCRIPTION: This Python function, `visualize`, is designed to draw bounding boxes, category labels, and detection probabilities onto an input image. It leverages OpenCV (`cv2`) for image manipulation, making the object detection results visually interpretable. The function takes an image and a `detection_result` object as input and returns the annotated image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
#@markdown We implemented some functions to visualize the object detection results. <br/> Run the following cell to activate the functions.
import cv2
import numpy as np

MARGIN = 10  # pixels
ROW_SIZE = 10  # pixels
FONT_SIZE = 1
FONT_THICKNESS = 1
TEXT_COLOR = (255, 0, 0)  # red


def visualize(
    image,
    detection_result
) -> np.ndarray:
  """Draws bounding boxes on the input image and return it.
  Args:
    image: The input RGB image.
    detection_result: The list of all "Detection" entities to be visualize.
  Returns:
    Image with bounding boxes.
  """
  for detection in detection_result.detections:
    # Draw bounding_box
    bbox = detection.bounding_box
    start_point = bbox.origin_x, bbox.origin_y
    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height
    cv2.rectangle(image, start_point, end_point, TEXT_COLOR, 3)

    # Draw label and score
    category = detection.categories[0]
    category_name = category.category_name
    probability = round(category.score, 2)
    result_text = category_name + ' (' + str(probability) + ')'
    text_location = (MARGIN + bbox.origin_x,
                     MARGIN + ROW_SIZE + bbox.origin_y)
    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,
                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)

  return image
```

----------------------------------------

TITLE: Configuring Retraining Options for MediaPipe Image Classifier (Python)
DESCRIPTION: This snippet sets up the retraining options for a MediaPipe image classifier. It specifies the model architecture (MobileNetV2) and the output directory for the exported model using `HParams` and `ImageClassifierOptions`. These options are prerequisites for starting the retraining process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
spec = image_classifier.SupportedModels.MOBILENET_V2
hparams = image_classifier.HParams(export_dir="exported_model")
options = image_classifier.ImageClassifierOptions(supported_model=spec, hparams=hparams)
```

----------------------------------------

TITLE: Defining Phi-2 Model Conversion Configuration in Python
DESCRIPTION: This Python function `phi2_convert_config` generates a `converter.ConversionConfig` object for the Phi-2 model. It sets predefined paths for the input checkpoint, vocabulary model, and intermediate/final output directories, dynamically naming the TFLite output file based on the specified backend.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
def phi2_convert_config(backend):
  input_ckpt = '/content/phi-2'
  vocab_model_file = '/content/phi-2/'
  output_dir = '/content/intermediate/phi-2/'
  output_tflite_file = f'/content/converted_models/phi2_{backend}.bin'

  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='PHI_2', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)
```

----------------------------------------

TITLE: Visualizing MediaPipe Classification Results with OpenCV (Python)
DESCRIPTION: This snippet processes a MediaPipe classification result by drawing detected landmarks onto the original image. It then converts the annotated image from RGB to BGR format (required by OpenCV's `imshow`) and displays it using `cv2_imshow` (likely a wrapper for `cv2.imshow` in environments like Colab).
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)
cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))
```

----------------------------------------

TITLE: Visualizing Pose Segmentation Mask in Python
DESCRIPTION: This snippet extracts the segmentation mask generated by the pose landmarker from the detection result. It converts the single-channel mask into a 3-channel image by repeating the mask values across all color channels and then scales them to 255 for visualization, effectively highlighting the detected person's silhouette.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
segmentation_mask = detection_result.segmentation_masks[0].numpy_view()
visualized_mask = np.repeat(segmentation_mask[:, :, np.newaxis], 3, axis=2) * 255
cv2_imshow(visualized_mask)
```

----------------------------------------

TITLE: Executing Prefill Operation in LiteRTLlmPipeline (Python)
DESCRIPTION: This method executes the prefill step of the LLM inference, processing an initial sequence of tokens. It handles cases with zero input tokens, prepares the input token IDs and positions as NumPy arrays, and initializes or updates the KV cache before passing inputs to the prefill runner.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_23

LANGUAGE: python
CODE:
```
  def _run_prefill(
      self, prefill_token_ids: Sequence[int],
  ) -> dict[str, np.ndarray]:
    """Runs prefill and returns the kv cache.

    Args:
      prefill_token_ids: The token ids of the prefill input.

    Returns:
      The updated kv cache.
    """
    if not self._prefill_runner:
      raise ValueError("Prefill runner is not initialized.")
    prefill_token_length = len(prefill_token_ids)
    if prefill_token_length == 0:
      return self._init_kv_cache()

    # Prepare the input to be [1, max_seq_len].
    input_token_ids = [0] * self._max_seq_len
    input_token_ids[:prefill_token_length] = prefill_token_ids
    input_token_ids = np.asarray(input_token_ids, dtype=np.int32)
    input_token_ids = np.expand_dims(input_token_ids, axis=0);

    # Prepare the input position to be [max_seq_len].
    input_pos = [0] * self._max_seq_len
    input_pos[:prefill_token_length] = range(prefill_token_length)
    input_pos = np.asarray(input_pos, dtype=np.int32)

    # Initialize kv cache.
    prefill_inputs = self._init_kv_cache()
    # Prepare the tokens and input position inputs.
    prefill_inputs.update({
        "tokens": input_token_ids,
        "input_pos": input_pos,
    })
```

----------------------------------------

TITLE: Generating Falcon 1B Conversion Configuration
DESCRIPTION: Generates a `converter.ConversionConfig` object for the Falcon 1B model. It specifies the input PyTorch checkpoint, vocabulary file, intermediate and final output directories, and the target backend for converting the model to a TFLite binary.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
def falcon_convert_config(backend):
  input_ckpt = '/content/falcon-rw-1b/pytorch_model.bin'
  vocab_model_file = '/content/falcon-rw-1b/'
  output_dir = '/content/intermediate/falcon-rw-1b/'
  output_tflite_file = f'/content/converted_models/falcon_{backend}.bin'
  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='pytorch', model_type='FALCON_RW_1B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)
```

----------------------------------------

TITLE: Selecting Optimal Prefill Runner in LiteRTLlmPipeline (Python)
DESCRIPTION: This method selects the most suitable prefill runner from the interpreter's available signatures based on the number of input tokens. It iterates through signatures, identifies those related to 'prefill', and chooses the one with the smallest sequence size that can accommodate the given input tokens.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
  def _get_prefill_runner(self, num_input_tokens: int) :
    """Gets the prefill runner with the best suitable input size.

    Args:
      num_input_tokens: The number of input tokens.

    Returns:
      The prefill runner with the smallest input size.
    """
    best_signature = None
    delta = sys.maxsize
    max_prefill_len = -1
    for key in self._interpreter.get_signature_list().keys():
      if "prefill" not in key:
        continue
      input_pos = self._interpreter.get_signature_runner(key).get_input_details()[
          "input_pos"
      ]
      # input_pos["shape"] has shape (max_seq_len, )
      seq_size = input_pos["shape"][0]
      max_prefill_len = max(max_prefill_len, seq_size)
      if num_input_tokens <= seq_size and seq_size - num_input_tokens < delta:
        delta = seq_size - num_input_tokens
        best_signature = key
    if best_signature is None:
      raise ValueError(
          "The largest prefill length supported is %d, but we have %d number of input tokens"
          %(max_prefill_len, num_input_tokens)
      )
    return self._interpreter.get_signature_runner(best_signature)
```

----------------------------------------

TITLE: Generating Gemma 2B Conversion Configuration
DESCRIPTION: Generates a `converter.ConversionConfig` object for the Gemma 2B model. This configuration specifies input and output paths, checkpoint format (safetensors), model type, and the target backend (CPU/GPU) for the conversion process to a TFLite binary.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
def gemma2b_convert_config(backend):
  input_ckpt = '/content/gemma-2b-it/'
  vocab_model_file = '/content/gemma-2b-it/'
  output_dir = '/content/intermediate/gemma-2b-it/'
  output_tflite_file = f'/content/converted_models/gemma_{backend}.bin'
  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='GEMMA_2B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)
```

----------------------------------------

TITLE: Evaluating Custom-Trained MediaPipe Gesture Recognizer Model (Python)
DESCRIPTION: This snippet evaluates the performance of the `model_2`, which was trained with custom hyperparameters, using the `test_data`. It retrieves and prints the loss and accuracy, allowing for a comparison of performance against models trained with default or different configurations.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
loss, accuracy = model_2.evaluate(test_data)
print(f"Test loss:{loss}, Test accuracy:{accuracy}")
```

----------------------------------------

TITLE: Generating Segmentation Mask with MediaPipe Python
DESCRIPTION: This snippet initializes an `InteractiveSegmenter` to process images. It iterates through image files, creates a MediaPipe image, and retrieves a category mask based on a specified region of interest (ROI). The mask is then used to create a solid color foreground and background image, which are combined to visualize the segmentation. A keypoint is drawn to indicate the ROI.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
# Create the interactive segmenter
with vision.InteractiveSegmenter.create_from_options(options) as segmenter:

  # Loop through demo image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe image file that will be segmented
    image = mp.Image.create_from_file(image_file_name)

    # Retrieve the masks for the segmented image
    roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,
                           keypoint=NormalizedKeypoint(x, y))
    segmentation_result = segmenter.segment(image, roi)
    category_mask = segmentation_result.category_mask

    # Generate solid color images for showing the output segmentation mask.
    image_data = image.numpy_view()
    fg_image = np.zeros(image_data.shape, dtype=np.uint8)
    fg_image[:] = MASK_COLOR
    bg_image = np.zeros(image_data.shape, dtype=np.uint8)
    bg_image[:] = BG_COLOR

    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1
    output_image = np.where(condition, fg_image, bg_image)

    # Draw a white dot with black border to denote the point of interest
    thickness, radius = 6, -1
    keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)
    cv2.circle(output_image, keypoint_px, thickness + 5, (0, 0, 0), radius)
    cv2.circle(output_image, keypoint_px, thickness, (255, 255, 255), radius)

    print(f'Segmentation mask of {image_file_name}:')
    resize_and_show(output_image)
```

----------------------------------------

TITLE: Installing Core Fine-tuning Libraries (Python)
DESCRIPTION: This snippet installs several key Python libraries required for large language model fine-tuning, including `bitsandbytes`, `peft`, `trl`, `accelerate`, `datasets`, and `transformers`. These packages provide functionalities for efficient memory usage, parameter-efficient fine-tuning, training loops, distributed training, dataset handling, and transformer model operations, respectively.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip3 install --upgrade -q -U bitsandbytes
!pip3 install --upgrade -q -U peft
!pip3 install --upgrade -q -U trl
!pip3 install --upgrade -q -U accelerate
!pip3 install --upgrade -q -U datasets
!pip3 install --force-reinstall transformers
```

----------------------------------------

TITLE: Training MediaPipe Gesture Recognizer Model (Python)
DESCRIPTION: This snippet initializes hyperparameters and model options, then creates and trains a custom gesture recognizer model using the `create` method. It takes `train_data` and `validation_data` as input, along with `GestureRecognizerOptions` which encapsulate `HParams` for configuration like export directory.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
hparams = gesture_recognizer.HParams(export_dir="exported_model")
options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)
model = gesture_recognizer.GestureRecognizer.create(
    train_data=train_data,
    validation_data=validation_data,
    options=options
)
```

----------------------------------------

TITLE: Creating LiteRT Interpreter - Python
DESCRIPTION: This snippet initializes the LiteRT interpreter using `InterpreterWithCustomOps`. It registers custom GenAI operations, provides the path to the downloaded model, sets the number of threads for execution, and enables experimental delegate features for optimized performance.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
interpreter = interpreter_lib.InterpreterWithCustomOps(
    custom_op_registerers=["pywrap_genai_ops.GenAIOpsRegisterer"],
    model_path=model_path,
    num_threads=2,
    experimental_default_delegate_latest_features=True)
```

----------------------------------------

TITLE: Downloading and Preparing Gesture Recognition Dataset
DESCRIPTION: This snippet downloads a sample rock paper scissors dataset from Google Cloud Storage and unzips it. It sets the `dataset_path` variable, which is crucial for subsequent data loading and model training steps.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
!wget https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip
!unzip rps_data_sample.zip
dataset_path = "rps_data_sample"
```

----------------------------------------

TITLE: Downloading and Preparing Dog Dataset
DESCRIPTION: This snippet downloads a zipped dataset of dog images from Google Cloud Storage, unzips it, and then defines the paths for the training and validation subsets of the dataset, preparing them for model training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
!wget https://storage.googleapis.com/mediapipe-assets/dogs2.zip --no-check-certificate
!unzip dogs2.zip
train_dataset_path = "dogs/train"
validation_dataset_path = "dogs/validate"
```

----------------------------------------

TITLE: Loading and Splitting Dataset with MediaPipe Gesture Recognizer (Python)
DESCRIPTION: This snippet loads a hand gesture dataset from a specified folder, applying MediaPipe's hand detection to extract landmarks and filter out images without hands. It then splits the loaded data into 80% for training, 10% for validation, and 10% for testing, preparing it for model training. The `HandDataPreprocessingParams` can be used to configure shuffling and detection confidence.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
data = gesture_recognizer.Dataset.from_folder(
    dirname=dataset_path,
    hparams=gesture_recognizer.HandDataPreprocessingParams()
)
train_data, rest_data = data.split(0.8)
validation_data, test_data = rest_data.split(0.5)
```

----------------------------------------

TITLE: Applying Quantization Aware Training with MediaPipe Model Maker (Python)
DESCRIPTION: This snippet demonstrates how to configure and run Quantization Aware Training (QAT) using `object_detector.QATHParams` and `model.quantization_aware_training`. It shows the initial setup of hyperparameters for QAT and evaluates the model's performance after training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
qat_hparams = object_detector.QATHParams(learning_rate=0.3, batch_size=4, epochs=10, decay_steps=6, decay_rate=0.96)
model.quantization_aware_training(train_data, validation_data, qat_hparams=qat_hparams)
qat_loss, qat_coco_metrics = model.evaluate(validation_data)
print(f"QAT validation loss: {qat_loss}")
print(f"QAT validation coco metrics: {qat_coco_metrics}")
```

----------------------------------------

TITLE: COCO Dataset Directory and JSON Structure
DESCRIPTION: This snippet illustrates the standard directory layout for a COCO dataset, including a `data` folder for images and a `labels.json` file for annotations. It also details the JSON schema for `labels.json`, which contains categories, image metadata, and object annotations with bounding box coordinates.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_4

LANGUAGE: Text
CODE:
```
<dataset_dir>/
  data/
    <img0>.<jpg/jpeg>
    <img1>.<jpg/jpeg>
    ...
  labels.json
```

LANGUAGE: JSON
CODE:
```
{
  "categories":[
    {"id":1, "name":<cat1_name>},
    ...
  ],
  "images":[
    {"id":0, "file_name":"<img0>.<jpg/jpeg>"},
    ...
  ],
  "annotations":[
    {"id":0, "image_id":0, "category_id":1, "bbox":[x-top left, y-top left, width, height]},
    ...
  ]
}
```

----------------------------------------

TITLE: Defining Visualization Utilities for Gesture Recognition - Python
DESCRIPTION: This Python code block defines utility functions for visualizing gesture recognition results and hand landmarks using `matplotlib` and `mediapipe.solutions`. It includes `display_one_image` for showing a single image with a title and `display_batch_of_images_with_gestures_and_hand_landmarks` for displaying multiple images with detected gestures and annotated hand landmarks. It also configures `matplotlib` for cleaner plots and initializes MediaPipe drawing utilities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
#@markdown We implemented some functions to visualize the gesture recognition results. <br/> Run the following cell to activate the functions.
from matplotlib import pyplot as plt
import mediapipe as mp
from mediapipe.framework.formats import landmark_pb2

plt.rcParams.update({
    'axes.spines.top': False,
    'axes.spines.right': False,
    'axes.spines.left': False,
    'axes.spines.bottom': False,
    'xtick.labelbottom': False,
    'xtick.bottom': False,
    'ytick.labelleft': False,
    'ytick.left': False,
    'xtick.labeltop': False,
    'xtick.top': False,
    'ytick.labelright': False,
    'ytick.right': False
})

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles


def display_one_image(image, title, subplot, titlesize=16):
    """Displays one image along with the predicted category name and score."""
    plt.subplot(*subplot)
    plt.imshow(image)
    if len(title) > 0:
        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))
    return (subplot[0], subplot[1], subplot[2]+1)


def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):
    """Displays a batch of images with the gesture category and its score along with the hand landmarks."""
    # Images and labels.
    images = [image.numpy_view() for image in images]
    gestures = [top_gesture for (top_gesture, _) in results]
    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]

    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.
    rows = int(math.sqrt(len(images)))
    cols = len(images) // rows

    # Size and spacing.
    FIGSIZE = 13.0
    SPACING = 0.1
    subplot=(rows,cols, 1)
    if rows < cols:
        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))
    else:
        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))

    # Display gestures and hand landmarks.
    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):
        title = f"{gestures.category_name} ({gestures.score:.2f})"
        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3
        annotated_image = image.copy()

        for hand_landmarks in multi_hand_landmarks_list[i]:
          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
          hand_landmarks_proto.landmark.extend([
            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks
          ])

          mp_drawing.draw_landmarks(
            annotated_image,
            hand_landmarks_proto,
            mp_hands.HAND_CONNECTIONS,
            mp_drawing_styles.get_default_hand_landmarks_style(),
            mp_drawing_styles.get_default_hand_connections_style())

        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)

    # Layout.
    plt.tight_layout()
    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)
    plt.show()
```

----------------------------------------

TITLE: Resizing Token Embeddings and Saving Merged Model (Python)
DESCRIPTION: This snippet resizes the token embeddings of the `merged_model` to match a new vocabulary size (262144) and then saves the model to the 'merged_model' directory. The `safe_serialization` and `max_shard_size` parameters ensure efficient and safe storage of the potentially large model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
merged_model.resize_token_embeddings(262144)
merged_model.save_pretrained("merged_model", safe_serialization=True, max_shard_size="2GB")
```

----------------------------------------

TITLE: Exporting an Int8 Quantized Model with MediaPipe Model Maker (Python)
DESCRIPTION: This snippet demonstrates how to export a model that has undergone Quantization Aware Training (QAT) to an int8 quantized TensorFlow Lite format. The `export_model` function automatically determines the quantization type based on previous training steps. It also shows how to list and download the exported file.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
model.export_model('model_int8_qat.tflite')
!ls -lh exported_model
files.download('exported_model/model_int8_qat.tflite')
```

----------------------------------------

TITLE: Initializing Gemma2-2B-IT Tokenizer - Python
DESCRIPTION: This snippet initializes a tokenizer for the Gemma2-2B-IT model using `AutoTokenizer.from_pretrained` from the `transformers` library. It loads the pre-trained tokenizer from the specified HuggingFace model identifier, which is essential for processing text inputs for the model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b-it")
```

----------------------------------------

TITLE: Greedy Sampling for Token Selection - LiteRT LLM Pipeline - Python
DESCRIPTION: This utility method implements a simple greedy sampling strategy. It takes an array of logits (model outputs) and returns the integer ID of the token with the highest logit score, representing the most probable next token.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_25

LANGUAGE: python
CODE:
```
  def _greedy_sampler(self, logits: np.ndarray) -> int:
    return int(np.argmax(logits))
```

----------------------------------------

TITLE: Retrieving and Printing Image Classification Labels (Python)
DESCRIPTION: This code block retrieves and prints the class labels from the downloaded dataset. It iterates through the subdirectories within the `image_path`, assuming each subdirectory name represents a class label, and appends them to a list. This step is crucial for verifying the data's organization before training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
print(image_path)
labels = []
for i in os.listdir(image_path):
  if os.path.isdir(os.path.join(image_path, i)):
    labels.append(i)
print(labels)
```

----------------------------------------

TITLE: Downloading and Preparing Example Dataset for Object Detection (Python)
DESCRIPTION: This snippet downloads and extracts an example dataset for object detection model retraining. It uses `wget` to fetch a zip file containing android figurine images, then `unzip` to extract its contents, and finally sets variables for the training and validation dataset paths, which are organized in COCO Dataset format.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
!wget https://storage.googleapis.com/mediapipe-tasks/object_detector/android_figurine.zip
!unzip android_figurine.zip
train_dataset_path = "android_figurine/train"
validation_dataset_path = "android_figurine/validation"
```

----------------------------------------

TITLE: Downloading Example Image Dataset (Python)
DESCRIPTION: This snippet downloads and extracts an example dataset of flower photos from a Google Cloud Storage URL. It uses `tf.keras.utils.get_file` to handle the download and extraction, then constructs the local path to the extracted images, which are organized into subdirectories corresponding to class labels.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
image_path = tf.keras.utils.get_file(
    'flower_photos.tgz',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    extract=True)
image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')
```

----------------------------------------

TITLE: Loading COCO Datasets into TFRecord Format with MediaPipe Object Detector
DESCRIPTION: This Python snippet demonstrates how to load COCO-formatted datasets into `object_detector.Dataset` objects using the `from_coco_folder` method. It specifies the dataset path and a `cache_dir` to store the converted TFRecord format, preventing redundant conversions. The snippet then prints the size of the loaded training and validation datasets.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir="/tmp/od_data/train")
validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir="/tmp/od_data/validation")
print("train_data size: ", train_data.size)
print("validation_data size: ", validation_data.size)
```

----------------------------------------

TITLE: Greedy Sampling Logits in Python
DESCRIPTION: This helper method implements a simple greedy sampling strategy. It takes an array of logits and returns the index of the token with the highest logit value, effectively selecting the most probable next token.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
  def _greedy_sampler(self, logits: np.ndarray) -> int:
    return int(np.argmax(logits))
```

----------------------------------------

TITLE: Defining Image Visualization Utilities (Python)
DESCRIPTION: This Python code defines utility functions using matplotlib to visualize image classification results. `display_one_image` shows a single image with its predicted title, while `display_batch_of_images` arranges and displays multiple images with their classifications in a grid, adjusting layout for readability. It also configures matplotlib to remove axis spines and ticks for cleaner image display, and depends on the `math` module.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from matplotlib import pyplot as plt
plt.rcParams.update({
    'axes.spines.top': False,
    'axes.spines.right': False,
    'axes.spines.left': False,
    'axes.spines.bottom': False,
    'xtick.labelbottom': False,
    'xtick.bottom': False,
    'ytick.labelleft': False,
    'ytick.left': False,
    'xtick.labeltop': False,
    'xtick.top': False,
    'ytick.labelright': False,
    'ytick.right': False
})


def display_one_image(image, title, subplot, titlesize=16):
    """Displays one image along with the predicted category name and score."""
    plt.subplot(*subplot)
    plt.imshow(image)
    if len(title) > 0:
        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))
    return (subplot[0], subplot[1], subplot[2]+1)

def display_batch_of_images(images, predictions):
    """Displays a batch of images with the classifications."""
    # Images and predictions.
    images = [image.numpy_view() for image in images]

    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.
    rows = int(math.sqrt(len(images)))
    cols = len(images) // rows

    # Size and spacing.
    FIGSIZE = 13.0
    SPACING = 0.1
    subplot=(rows,cols, 1)
    if rows < cols:
        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))
    else:
        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))

    # Display.
    for i, (image, prediction) in enumerate(zip(images[:rows*cols], predictions[:rows*cols])):
        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3
        subplot = display_one_image(image, prediction, subplot, titlesize=dynamic_titlesize)

    # Layout.
    plt.tight_layout()
    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)
    plt.show()
```

----------------------------------------

TITLE: Configuring Average Word Embedding Text Classifier Options (Python)
DESCRIPTION: This snippet initializes `TextClassifierOptions` for an average word embedding model. It sets the `supported_model` to `AVERAGE_WORD_EMBEDDING_CLASSIFIER` and defines hyperparameters like `epochs`, `batch_size`, `learning_rate`, and `export_dir` using `AverageWordEmbeddingHParams`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
supported_model = text_classifier.SupportedModels.AVERAGE_WORD_EMBEDDING_CLASSIFIER
hparams = text_classifier.AverageWordEmbeddingHParams(epochs=10, batch_size=32, learning_rate=0, export_dir="awe_exported_models")
options = text_classifier.TextClassifierOptions(supported_model=supported_model, hparams=hparams)
```

----------------------------------------

TITLE: Creating Dataset for Face Stylizer Training
DESCRIPTION: This line of code creates a dataset object for the face stylizer model using the `Dataset.from_image` method from `mediapipe_model_maker.face_stylizer`. It takes the path to the single stylized image (`style_image_path`) as input, preparing the data in the format required for training the face stylizer model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
data = face_stylizer.Dataset.from_image(filename=style_image_path)
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library using pip, the Python package installer. The -q flag ensures a quiet installation, suppressing verbose output. This is a prerequisite for using MediaPipe Tasks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Exporting MediaPipe Model to TensorFlow Lite and Listing Files (Python)
DESCRIPTION: This snippet exports the trained `model` into a TensorFlow Lite format, including necessary metadata and label files, making it suitable for on-device applications. After export, it uses a shell command to list the contents of the `exported_model` directory, verifying the creation of the TFLite file.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
model.export_model()
!ls exported_model
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library using pip, the Python package installer. The `-q` flag ensures a quiet installation, suppressing verbose output. MediaPipe is a framework for building machine learning pipelines, essential for gesture recognition.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Library (Python)
DESCRIPTION: This snippet installs the MediaPipe library using pip. The `-q` flag ensures a quiet installation, suppressing verbose output. This is a prerequisite for using MediaPipe Tasks functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_embedder/python/image_embedder.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library using pip. The '-q' flag ensures a quiet installation, suppressing verbose output. MediaPipe is a prerequisite for utilizing its various machine learning solutions, including hand landmark detection.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Evaluating Retrained MediaPipe Image Classifier Performance (Python)
DESCRIPTION: This snippet evaluates the performance of the retrained model against a test dataset. It calculates and prints the test loss and accuracy, which are crucial metrics for assessing model quality. High accuracy is generally desired, but caution against overfitting is advised.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
loss, acc = model.evaluate(test_data)
print(f'Test loss:{loss}, Test accuracy:{acc}')
```

----------------------------------------

TITLE: Initializing LiteRTLlmPipeline for LLM Inference (Python)
DESCRIPTION: This constructor initializes the `LiteRTLlmPipeline` class, setting up the model interpreter and tokenizer. It also obtains the signature runner for the 'decode' operation immediately, while the 'prefill' runner is initialized dynamically later based on input size.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
class LiteRTLlmPipeline:

  def __init__(self, interpreter, tokenizer):
    """Initializes the pipeline."""
    self._interpreter = interpreter
    self._tokenizer = tokenizer

    self._prefill_runner = None
    self._decode_runner = self._interpreter.get_signature_runner("decode")
```

----------------------------------------

TITLE: Installing MediaPipe Library
DESCRIPTION: This command installs the MediaPipe library using pip, a Python package installer. The `-q` flag ensures a quiet installation, suppressing verbose output. This is a prerequisite for running MediaPipe samples.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Running Prefill Stage - LiteRT LLM Pipeline - Python
DESCRIPTION: This method handles the prefill stage of the LLM, processing initial tokens to populate the KV cache. It dynamically generates an attention mask based on input details and ensures that only the KV cache is returned, discarding intermediate logits.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_24

LANGUAGE: python
CODE:
```
    if "mask" in self._prefill_runner.get_input_details().keys():
      # For prefill, mask has shape [batch=1, 1, seq_len, kv_cache_size].
      # We want mask[0, 0, i, j] = 0 for j<=i and -inf otherwise.
      prefill_inputs["mask"] = _get_mask(
          shape=self._prefill_runner.get_input_details()["mask"]["shape"],
          k=1,
      )
    prefill_outputs = self._prefill_runner(**prefill_inputs)
    if "logits" in prefill_outputs:
      # Prefill outputs includes logits and kv cache. We only output kv cache.
      prefill_outputs.pop("logits")

    return prefill_outputs
```

----------------------------------------

TITLE: Installing MediaPipe Library (Python)
DESCRIPTION: This command installs the MediaPipe library using pip, the Python package installer. The -q flag ensures a quiet installation, suppressing verbose output. This is a prerequisite for using MediaPipe Tasks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing LiteRT Dependency - Python
DESCRIPTION: This snippet installs the `ai-edge-litert` library, which is a prerequisite for using the LiteRT interpreter for on-device AI models. It uses the `pip` package manager.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install ai-edge-litert
```

----------------------------------------

TITLE: Installing MediaPipe Python Library
DESCRIPTION: This command installs the MediaPipe Python library using pip, which is a fundamental dependency for running any MediaPipe tasks, including image segmentation. The `-q` flag ensures a quiet installation.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Evaluating Face Stylizer Model Performance (Python)
DESCRIPTION: This snippet evaluates the performance of a retrained face stylizer model by reconstructing a style image. It uses OpenCV (cv2) for image resizing and display, and the 'face_stylizer_model' to perform the stylization. The reconstructed image is then displayed to assess the model's convergence and quality, helping to determine if the model is suitable for new data.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
print('Input style image')
resized_style_cv_image = cv2.resize(style_cv_image, (256, 256))
cv2_imshow(resized_style_cv_image)

eval_output = face_stylizer_model.stylize(data)
eval_output_data = eval_output.gen_tf_dataset()
iterator = iter(eval_output_data)

reconstruct_style_image = (tf.squeeze(iterator.get_next()).numpy())
test_output_image = cv2.cvtColor(reconstruct_style_image, cv2.COLOR_RGB2BGR)
print('\nReconstructed style image')
cv2_imshow(test_output_image)
```

----------------------------------------

TITLE: Downloading Gemma 7B Model Files
DESCRIPTION: Downloads specific files for the Gemma 7B instruction-tuned model from Hugging Face. It requires a Hugging Face token for authentication and fetches tokenizer and model safetensors files into a local directory using `hf_hub_download`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
def gemma7b_download(token):
  REPO_ID = "google/gemma-1.1-7b-it"
  FILENAMES = ["tokenizer.json", "tokenizer_config.json", "model-00001-of-00004.safetensors", "model-00002-of-00004.safetensors", "model-00003-of-00004.safetensors", "model-00004-of-00004.safetensors"]
  os.environ['HF_TOKEN'] = token
  with out:
    for filename in FILENAMES:
      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir="./gemma-1.1-7b-it")
```

----------------------------------------

TITLE: Downloading Falcon 1B Model Files
DESCRIPTION: Downloads specific files for the Falcon 1B model from Hugging Face. It fetches tokenizer files and the PyTorch model binary into a local directory using `hf_hub_download`. This model does not require a Hugging Face token.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
def falcon_download():
  REPO_ID = "tiiuae/falcon-rw-1b"
  FILENAMES = ["tokenizer.json", "tokenizer_config.json", "pytorch_model.bin"]
  with out:
    for filename in FILENAMES:
      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir="./falcon-rw-1b")
```

----------------------------------------

TITLE: Running MediaPipe Object Detection with Custom Parameters in Python
DESCRIPTION: This Python command runs the `detect.py` script for real-time object detection, allowing customization of the model, maximum number of detection results (`maxResults`), and the confidence score threshold (`scoreThreshold`). It uses `efficientdet_lite0.tflite` as the model, limits results to 5, and sets the score threshold to 0.3.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 detect.py \
  --model efficientdet_lite0.tflite \
  --maxResults 5 \
  --scoreThreshold 0.3
```

----------------------------------------

TITLE: Running MediaPipe Audio Classifier with Custom Parameters (Python)
DESCRIPTION: This command demonstrates how to run the audio classification script with specific optional parameters. It sets the TensorFlow Lite model to `yamnet.tflite` and limits the classification results to 5, overriding the default values for more controlled inference.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 classify.py \
    --model yamnet.tflite \
    --maxResults 5
```

----------------------------------------

TITLE: Evaluating Average Word Embedding Text Classifier (Python)
DESCRIPTION: This snippet evaluates the trained average word embedding model on the `validation_data`. It uses `model.evaluate()` to get performance metrics (loss and accuracy) and prints them to the console.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
metrics = model.evaluate(validation_data)
print(f'Test loss:{metrics[0]}, Test accuracy:{metrics[1]}')
```

----------------------------------------

TITLE: Running MediaPipe Gesture Recognizer with Custom Parameters
DESCRIPTION: This command executes the MediaPipe gesture recognition application, allowing customization of key operational parameters. It demonstrates how to specify a custom model file, increase the maximum number of hands to detect, and adjust the minimum confidence score for hand detection.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/raspberry_pi/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
python3 recognize.py \
  --model gesture_recognizer.task \
  --numHands 2 \
  --minHandDetectionConfidence 0.5
```

----------------------------------------

TITLE: Setting HuggingFace Token in Colab Environment (Python)
DESCRIPTION: This snippet imports the `os` module and `userdata` from `google.colab` to securely retrieve and set the HuggingFace token as an environment variable. This token is essential for accessing models and tokenizers from HuggingFace, ensuring proper authentication for downloads.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import os
from google.colab import userdata
os.environ["HF_TOKEN"] = userdata.get('HF_TOKEN')
```

----------------------------------------

TITLE: Downloading Gemma 2B Model Files
DESCRIPTION: Downloads specific files for the Gemma 2B instruction-tuned model from Hugging Face. It sets the `HF_TOKEN` environment variable for authentication and uses `hf_hub_download` to fetch tokenizer and model safetensors files into a local directory.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
def gemma2b_download(token):
  REPO_ID = "google/gemma-2b-it"
  FILENAMES = ["tokenizer.json", "tokenizer_config.json", "model-00001-of-00002.safetensors", "model-00002-of-00002.safetensors"]
  os.environ['HF_TOKEN'] = token
  with out:
    for filename in FILENAMES:
      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir="./gemma-2b-it")
```

----------------------------------------

TITLE: Downloading Phi 2 Model Files
DESCRIPTION: Downloads specific files for the Phi 2 model from Hugging Face. It fetches tokenizer files and model safetensors into a local directory using `hf_hub_download`. This model does not require a Hugging Face token.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
def phi2_download():
  REPO_ID = "microsoft/phi-2"
  FILENAMES = ["tokenizer.json", "tokenizer_config.json", "model-00001-of-00002.safetensors", "model-00002-of-00002.safetensors"]
  with out:
    for filename in FILENAMES:
      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir="./phi-2")
```

----------------------------------------

TITLE: Converting Normalized Coordinates to Pixels in Python
DESCRIPTION: This Python function converts normalized (0-1) coordinates to pixel coordinates within an image. It takes normalized x and y values, image width, and image height as input, returning pixel (x, y) or None if coordinates are out of bounds. It's a utility for visualization.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
def _normalized_to_pixel_coordinates(
    normalized_x: float, normalized_y: float, image_width: int,
    image_height: int):
  """Converts normalized value pair to pixel coordinates."""

  # Checks if the float value is between 0 and 1.
  def is_valid_normalized_value(value: float) -> bool:
    return (value > 0 or math.isclose(0, value)) and (value < 1 or
                                                      math.isclose(1, value))

  if not (is_valid_normalized_value(normalized_x) and
          is_valid_normalized_value(normalized_y)):
    # TODO: Draw coordinates even if it's outside of the image bounds.
    return None
  x_px = min(math.floor(normalized_x * image_width), image_width - 1)
  y_px = min(math.floor(normalized_y * image_height), image_height - 1)
  return x_px, y_px
```

----------------------------------------

TITLE: Installing MediaPipe Python Package
DESCRIPTION: This command installs the MediaPipe library for Python using pip, the package installer. The '-q' flag ensures a quiet installation, suppressing detailed output. This step is a prerequisite for utilizing any MediaPipe functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library and its related dependencies using pip. It is a prerequisite for running the face stylizer demo, ensuring all necessary packages are available in the environment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_stylizer/python/face_stylizer.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library using pip, which is a prerequisite for running face detection tasks. It ensures all necessary MediaPipe components are available in the Python environment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library using pip, the Python package installer. The `-q` flag ensures a quiet installation, suppressing verbose output. This is a prerequisite for using MediaPipe Tasks functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/python/audio_classification.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Library in Python
DESCRIPTION: This command installs the MediaPipe library using pip, the Python package installer. The -q flag ensures a quiet installation, suppressing verbose output. This is a prerequisite for using MediaPipe's functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/language_detector/python/[MediaPipe_Python_Tasks]_Language_Detector.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Loading SST-2 Dataset from TSV Files
DESCRIPTION: This snippet defines `CSVParams` to specify the text and label columns and the tab delimiter for TSV files. It then loads the training and validation datasets from their respective TSV files (`train.tsv` and `dev.tsv`) using `text_classifier.Dataset.from_csv`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
csv_params = text_classifier.CSVParams(
    text_column='sentence', label_column='label', delimiter='\t')
train_data = text_classifier.Dataset.from_csv(
    filename=os.path.join(os.path.join(data_dir, 'train.tsv')),
    csv_params=csv_params)
validation_data = text_classifier.Dataset.from_csv(
    filename=os.path.join(os.path.join(data_dir, 'dev.tsv')),
    csv_params=csv_params)
```

----------------------------------------

TITLE: Training Average Word Embedding Text Classifier (Python)
DESCRIPTION: This snippet trains the text classifier using the `TextClassifier.create` function. It takes `train_data`, `validation_data`, and the previously defined `options` to create and train the model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
model = text_classifier.TextClassifier.create(train_data, validation_data, options)
```

----------------------------------------

TITLE: Running MediaPipe Text Classification with Default Model in Python
DESCRIPTION: This command executes the `classify.py` Python script to perform text classification. It takes an input text string via the `--inputText` parameter and uses the default `classifier.tflite` model to classify the text's sentiment (positive or negative) and provide a confidence score.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 classify.py --inputText "Your text goes here"
```

----------------------------------------

TITLE: Running MediaPipe Hand Landmarker Detection - Python
DESCRIPTION: This command executes the `detect.py` Python script, which initiates the real-time hand landmark detection using the MediaPipe Hand Landmarker. It uses default parameters for the model, number of hands, and confidence thresholds.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 detect.py
```

----------------------------------------

TITLE: Testing Base Model Inference - Hugging Face Transformers - Python
DESCRIPTION: This snippet demonstrates how to perform text generation inference using a pre-trained base model and the Hugging Face `pipeline` API. It initializes a text generation pipeline with a specified model and tokenizer, then generates a response to a given prompt with a maximum token limit.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
prompt = "What is the primary function of mitochondria within a cell?"
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
pipe(prompt, max_new_tokens=100)
```

----------------------------------------

TITLE: Initializing MediaPipe Interactive Segmenter Options
DESCRIPTION: This Python snippet initializes the `InteractiveSegmenter` options for MediaPipe. It sets the base model path to `model.tflite` and configures the segmenter to output a category mask, which is crucial for distinguishing foreground and background elements.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
import numpy as np
import mediapipe as mp

from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from mediapipe.tasks.python.components import containers


BG_COLOR = (192, 192, 192) # gray
MASK_COLOR = (255, 255, 255) # white

RegionOfInterest = vision.InteractiveSegmenterRegionOfInterest
NormalizedKeypoint = containers.keypoint.NormalizedKeypoint

# Create the options that will be used for InteractiveSegmenter
base_options = python.BaseOptions(model_asset_path='model.tflite')
options = vision.ImageSegmenterOptions(base_options=base_options,
                                       output_category_mask=True)
```

----------------------------------------

TITLE: Saving Fine-Tuned Model Weights - TRL Trainer - Python
DESCRIPTION: This snippet saves the weights of the fine-tuned model to a specified directory. The `save_model` method of the `SFTTrainer` instance persists the trained model's state, allowing it to be reloaded later for inference or further development.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
trainer.save_model("gemma3-1b-sft")
```

----------------------------------------

TITLE: Visualizing Face Blendshapes with MediaPipe in Python
DESCRIPTION: This snippet visualizes the detected face blendshapes from the MediaPipe FaceLandmarker result. It takes the first set of blendshapes from the `detection_result` and uses a helper function `plot_face_blendshapes_bar_graph` to display them, likely as a bar graph.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0])
```

----------------------------------------

TITLE: Installing Dependencies with CocoaPods (Shell)
DESCRIPTION: This command initiates the installation of all required project dependencies using CocoaPods. It must be executed from the root directory of the project where the Podfile is located.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/ios/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
pod install
```

----------------------------------------

TITLE: Downloading MediaPipe Face Detector Model - Python
DESCRIPTION: This command downloads a pre-trained `blaze_face_short_range.tflite` model from Google's storage, which is required by MediaPipe's Face Detector. The model is saved locally as `detector.tflite` for subsequent use in the application.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -q -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite
```

----------------------------------------

TITLE: Downloading EfficientDet Lite0 Model - Python
DESCRIPTION: This command downloads the 'efficientdet_lite0.tflite' model, a pre-trained TensorFlow Lite model optimized for object detection. It is fetched from a Google Cloud Storage bucket and saved locally as 'efficientdet.tflite', serving as the core model for MediaPipe's ObjectDetector.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -q -O efficientdet.tflite -q https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/int8/1/efficientdet_lite0.tflite
```

----------------------------------------

TITLE: Configuring StableLM 3B Model Conversion in Python
DESCRIPTION: This line returns a `converter.ConversionConfig` object specifically for the StableLM 3B model. It defines the input checkpoint path, 'safetensors' format, model type, target backend, output directory, vocabulary file, and the final TFLite output file path, preparing the model for conversion.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='STABLELM_4E1T_3B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)
```

----------------------------------------

TITLE: Downloading Exported MediaPipe TensorFlow Lite Model (Python)
DESCRIPTION: This snippet facilitates the download of the `gesture_recognizer.task` file, which is the exported TensorFlow Lite model, from the `exported_model` directory. This step is crucial for transferring the model to a local machine or an on-device application for deployment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
files.download('exported_model/gesture_recognizer.task')
```

----------------------------------------

TITLE: Executing Face Stylizer Model Retraining in Python
DESCRIPTION: This snippet shows how to initiate the retraining process for the Face Stylizer model using the `create()` method. It takes the prepared `train_data` and the previously configured `face_stylizer_options` to fine-tune the model, requiring GPU resources for execution and potentially taking several minutes to hours.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
face_stylizer_model = face_stylizer.FaceStylizer.create(
  train_data=data, options=face_stylizer_options
)
```

----------------------------------------

TITLE: Converting Model Checkpoints using Python Script
DESCRIPTION: This command executes the `convert.py` script to transform model checkpoints into a 'bins' folder. Users must specify the path to the input checkpoint file (`--ckpt_path`) and the desired output directory (`--output_path`).
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tools/image_generator_converter/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
python3 convert.py --ckpt_path <ckpt_path> --output_path <output_path>
```

----------------------------------------

TITLE: Downloading Face Landmarker Model Bundle - Python
DESCRIPTION: This command downloads the face_landmarker_v2_with_blendshapes.task model bundle from Google Cloud Storage. This pre-trained model is essential for performing face landmark detection with MediaPipe Tasks. The -q flag ensures quiet download.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task
```

----------------------------------------

TITLE: Running MediaPipe Face Landmarker with Custom Parameters - Python
DESCRIPTION: This command demonstrates how to run the `detect.py` script with optional parameters. It specifies the model file, sets the maximum number of faces to detect to 2, and adjusts the minimum confidence score for face detection to 0.5. This allows for customized behavior of the face landmarker.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 detect.py \
  --model face_landmarker.task \
  --numFaces 2 \
  --minFaceDetectionConfidence 0.5
```

----------------------------------------

TITLE: Running MediaPipe Hand Landmarker Detection with Custom Parameters - Python
DESCRIPTION: This command runs the `detect.py` Python script with specified optional parameters. It sets the `model` to `hand_landmarker.task`, limits `numHands` to 1, and adjusts the `minHandDetectionConfidence` to 0.5. These parameters allow for fine-tuning the detection process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 detect.py \
  --model hand_landmarker.task \
  --numHands 1 \
  --minHandDetectionConfidence 0.5
```

----------------------------------------

TITLE: Generating Mask for LiteRT Model Input (Python)
DESCRIPTION: This function generates a mask for the input to a LiteRT model. It initializes a NumPy array with negative infinity and then sets elements below the k-th diagonal to zero, which is useful for attention mechanisms in models. It requires the `numpy` library.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
def _get_mask(shape: Sequence[int], k: int):
  """Gets the mask for the input to the model.

  Args:
    shape: The shape of the mask input to the model.
    k: all elements below the k-th diagonal are set to 0.

  Returns:
    The mask for the input to the model. All the elements in the mask are set
    to -inf except that all the elements below the k-th diagonal are set to 0.
  """
  mask = np.ones(shape, dtype=np.float32) * float("-inf")
  mask = np.triu(mask, k=k)
  return mask
```

----------------------------------------

TITLE: Displaying Example Images per Class (Python)
DESCRIPTION: This snippet visualizes a specified number of example images from each identified class. It uses `matplotlib.pyplot` to display images, helping to visually confirm that the data is correctly classified and organized. The `NUM_EXAMPLES` constant controls how many images are shown per label.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
NUM_EXAMPLES = 5

for label in labels:
  label_dir = os.path.join(image_path, label)
  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]
  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))
  for i in range(NUM_EXAMPLES):
    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))
    axs[i].get_xaxis().set_visible(False)
    axs[i].get_yaxis().set_visible(False)
  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')

plt.show()
```

----------------------------------------

TITLE: Downloading MediaPipe Text Classifier Model
DESCRIPTION: This command downloads an off-the-shelf BERT text classifier model (`bert_classifier.tflite`) from Google Cloud Storage. The `-O` flag specifies the output filename, and `-q` ensures quiet download. This TFLite model is essential for performing text classification.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/python/text_classifier.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -O classifier.tflite -q https://storage.googleapis.com/mediapipe-models/text_classifier/bert_classifier/float32/1/bert_classifier.tflite
```

----------------------------------------

TITLE: Downloading YAMNet Audio Classifier Model - Python
DESCRIPTION: This command downloads the pre-trained YAMNet audio classification model (`.tflite` file) from Google Cloud Storage. The model is saved as `classifier.tflite` and is essential for performing audio classification with MediaPipe Tasks. The `-q` flag ensures quiet download.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/python/audio_classification.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -O classifier.tflite -q https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite
```

----------------------------------------

TITLE: Running MediaPipe Face Detection with Custom Parameters (Python)
DESCRIPTION: This Python command runs the `detect.py` script, allowing users to customize the face detection behavior. It sets the TensorFlow Lite model to `detector.tflite`, adjusts the minimum detection confidence to `0.3`, and the minimum non-maximum suppression threshold to `0.5`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 detect.py \
  --model detector.tflite \
  --minDetectionConfidence 0.3 \
  --minSuppressionThreshold 0.5
```

----------------------------------------

TITLE: Running MediaPipe Pose Landmarker with Custom Parameters
DESCRIPTION: This command demonstrates how to run the `detect.py` script with several optional parameters to customize the pose detection process. It specifies a custom model file, limits the number of detected poses, adjusts the minimum confidence score for pose detection, and enables the visualization of segmentation masks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 detect.py \
  --model pose_landmarker.task \
  --numPoses 1 \
  --minPoseDetectionConfidence 0.5\
  --outputSegmentationMasks
```

----------------------------------------

TITLE: Running MediaPipe Text Classification with Specified Model in Python
DESCRIPTION: This command runs the `classify.py` script, allowing the user to explicitly specify the TensorFlow Lite model to be used for classification via the `--model` parameter. It classifies the provided input text using the designated model, which must include metadata.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/raspberry_pi/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
python3 classify.py \
    --model classifier.tflite \
    --inputText "Your text goes here"
```

----------------------------------------

TITLE: Previewing Downloaded Images - Python
DESCRIPTION: This snippet defines a `resize_and_show` function to standardize image dimensions and displays the downloaded test image(s) using OpenCV. It helps verify that images are correctly loaded and prepared for subsequent processing by the MediaPipe model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_stylizer/python/face_stylizer.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
import cv2
from google.colab.patches import cv2_imshow
import math

# Height and width that will be used by the model
DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

# Performs resizing and showing the image
def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


# Preview the image(s)
images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
for name, image in images.items():
  print(name)
  resize_and_show(image)
```

----------------------------------------

TITLE: Setting Up MediaPipe for Task Bundler in Colab
DESCRIPTION: This Python snippet sets up the environment by installing the `mediapipe` library using pip and imports necessary modules like `ipywidgets`, `IPython.display`, `google.colab.files`, and `mediapipe.tasks.python.genai.bundler`. It uses an `ipywidgets.Output` widget to display installation progress.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/bundling/llm_bundling.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
#@title Setup { display-mode: "form" }
import ipywidgets as widgets
from IPython.display import display
from google.colab import files
install_out = widgets.Output()
display(install_out)
with install_out:
  !pip install mediapipe
  from mediapipe.tasks.python.genai import bundler

install_out.clear_output()
with install_out:
  print("Setup done.")
```

----------------------------------------

TITLE: Installing MediaPipe Dependencies and Models (Shell)
DESCRIPTION: This shell script navigates to the project directory and executes the `setup.sh` script to install necessary dependencies and download pre-trained TensorFlow Lite models for the MediaPipe image classification example. This is a prerequisite for running the main classification script.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/raspberry_pi/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
cd mediapipe/examples/image_classification/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Setting Up MediaPipe Text Classification on Raspberry Pi
DESCRIPTION: This snippet navigates into the specific project directory for text classification on Raspberry Pi and then executes the `setup.sh` script. This script is crucial for installing all necessary dependencies and downloading the required TensorFlow Lite models for the text classification demo.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/raspberry_pi/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
cd mediapipe/examples/text_classification/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Evaluating BERT Text Classifier (Python)
DESCRIPTION: This snippet evaluates the performance of the trained BERT model on the `validation_data`. It calls `bert_model.evaluate()` to obtain the test loss and accuracy, which are then printed to demonstrate the model's performance.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
metrics = bert_model.evaluate(validation_data)
print(f'Test loss:{metrics[0]}, Test accuracy:{metrics[1]}')
```

----------------------------------------

TITLE: Installing MediaPipe Model Maker Package
DESCRIPTION: This snippet installs the necessary MediaPipe Model Maker package and upgrades pip. It is a prerequisite for customizing on-device machine learning models and ensures all required dependencies are available.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install --upgrade pip
!pip install mediapipe-model-maker
```

----------------------------------------

TITLE: Installing Required Python Packages
DESCRIPTION: This snippet upgrades pip and installs the necessary Python packages for MediaPipe Model Maker, including 'keras<3.0.0' and 'mediapipe-model-maker', which are essential for training custom object detection models.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install --upgrade pip
!pip install 'keras<3.0.0' mediapipe-model-maker
```

----------------------------------------

TITLE: Importing Required Libraries - Python
DESCRIPTION: This snippet imports essential Python libraries for the project, including `interpreter_lib` from `ai_edge_litert` for model interpretation, `AutoTokenizer` from `transformers` for text processing, and standard libraries like `numpy`, `collections.abc.Sequence`, and `sys`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
from ai_edge_litert import interpreter as interpreter_lib
from transformers import AutoTokenizer
import numpy as np
from collections.abc import Sequence
import sys
```

----------------------------------------

TITLE: Importing MediaPipe Modules - Python
DESCRIPTION: This snippet imports the necessary MediaPipe modules for image classification, including `mediapipe` itself, `mediapipe.tasks.python` for task-specific functionalities, and `mediapipe.tasks.python.vision` for vision-related tasks. These modules are prerequisites for initializing the image classifier.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python.components import processors
from mediapipe.tasks.python import vision
```

----------------------------------------

TITLE: Installing MediaPipe Model Maker Dependencies in Python
DESCRIPTION: This snippet installs necessary Python libraries for customizing MediaPipe models. It first checks the Python version, then upgrades pip to its latest version, and finally installs the `mediapipe-model-maker` package, which is essential for model customization tasks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!python --version
!pip install --upgrade pip
!pip install mediapipe-model-maker
```

----------------------------------------

TITLE: Installing MediaPipe Model Maker Libraries (Python)
DESCRIPTION: This snippet installs the necessary Python libraries for customizing MediaPipe models within a Colab environment. It first checks the Python version, then upgrades pip to its latest version, and finally installs the `mediapipe-model-maker` package, which is essential for retraining object detection models.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!python --version
!pip install --upgrade pip
!pip install mediapipe-model-maker
```

----------------------------------------

TITLE: Running MediaPipe Audio Classifier (Python)
DESCRIPTION: This command executes the main Python script for the audio classification example. It runs the classifier with default parameters, processing audio streamed from the microphone to perform real-time classification.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 classify.py
```

----------------------------------------

TITLE: Running MediaPipe Image Classifier (Python)
DESCRIPTION: This command executes the `classify.py` Python script, which runs the MediaPipe image classification example using default settings. It processes images streamed from the camera and performs real-time classification.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 classify.py
```

----------------------------------------

TITLE: Running MediaPipe Pose Landmarker Detection Script
DESCRIPTION: This command executes the main Python script, `detect.py`, which performs real-time pose landmark detection using the camera stream. It initiates the MediaPipe pipeline with default settings to process video frames and identify human poses.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 detect.py
```

----------------------------------------

TITLE: Setting Up MediaPipe Pose Landmarker on Raspberry Pi
DESCRIPTION: This command sequence navigates into the specific project directory for the MediaPipe Pose Landmarker Raspberry Pi example and then executes the `setup.sh` script. The setup script is responsible for installing all required dependencies and downloading the necessary task file for the pose landmarker model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/raspberry_pi/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
cd mediapipe/examples/pose_landmarker/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Downloading Image Embedder Model (Python)
DESCRIPTION: This snippet downloads a pre-trained `mobilenet_v3_small.tflite` model, which is an off-the-shelf image embedder. The `wget` command saves the model as `embedder.tflite` for subsequent use with MediaPipe's Image Embedder.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_embedder/python/image_embedder.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
!wget -O embedder.tflite -q https://storage.googleapis.com/mediapipe-models/image_embedder/mobilenet_v3_small/float32/1/mobilenet_v3_small.tflite
```

----------------------------------------

TITLE: Downloading MediaPipe Text Embedder Model - Python
DESCRIPTION: This command downloads the `bert_embedder.tflite` model from Google Cloud Storage, which is essential for performing text embedding with MediaPipe. The model is saved as `embedder.tflite` in the current directory, and the `-q` flag suppresses output.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_embedder/python/text_embedder.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
#@title Start downloading here.
!wget -O embedder.tflite -q https://storage.googleapis.com/mediapipe-models/text_embedder/bert_embedder/float32/1/bert_embedder.tflite
```

----------------------------------------

TITLE: Downloading and Displaying Test Image - Python
DESCRIPTION: This snippet first downloads a test image named image.png from Google Cloud Storage using wget. It then uses OpenCV (cv2) to read the image and cv2_imshow (from google.colab.patches) to display it within the Colab environment. This image serves as input for subsequent face landmark detection.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
!wget -q -O image.png https://storage.googleapis.com/mediapipe-assets/business-person.png

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread("image.png")
cv2_imshow(img)
```

----------------------------------------

TITLE: Toggling Hugging Face Token Visibility
DESCRIPTION: A callback function `on_change_model` that controls the visibility of the Hugging Face token input field. If a 'gated' model (like Gemma 2B or Gemma 7B) is selected, the token input is displayed; otherwise, it is hidden, prompting the user for necessary authentication.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
def on_change_model(change):
  selected_values = ['Gemma 2B','Gemma 7B']

  if change['new'] in selected_values:
    token.layout.display = 'flex'
  else:
    token.layout.display = 'none'
```

----------------------------------------

TITLE: Downloading StableLM 3B Model Files
DESCRIPTION: Downloads specific files for the StableLM 3B model from Hugging Face. It fetches tokenizer files and the model safetensors into a local directory using `hf_hub_download`. This model does not require a Hugging Face token.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
def stablelm_download():
  REPO_ID = "stabilityai/stablelm-3b-4e1t"
  FILENAMES = ["tokenizer.json", "tokenizer_config.json", "model.safetensors"]
  with out:
    for filename in FILENAMES:
      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir="./stablelm-3b-4e1t")
```

----------------------------------------

TITLE: Running MediaPipe Face Landmarker Detection - Python
DESCRIPTION: This command executes the main Python script `detect.py` to start the face landmarker detection. By default, it uses `face_landmarker.task` and detects one face with default confidence thresholds. This is the basic command to run the application.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/raspberry_pi/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python3 detect.py
```

----------------------------------------

TITLE: Downloading and Displaying Test Image - Python
DESCRIPTION: This snippet downloads a sample image (`image.jpg`) from a URL and then uses OpenCV (`cv2`) and `google.colab.patches.cv2_imshow` to load and display the image within a Colab environment. This image serves as input for demonstrating the face detection capabilities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
!curl https://i.imgur.com/Vu2Nqwb.jpeg -s -o image.jpg

IMAGE_FILE = 'image.jpg'

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread(IMAGE_FILE)
cv2_imshow(img)
```

----------------------------------------

TITLE: Initializing Model, Backend, and Token Selection Widgets
DESCRIPTION: Initializes `ipywidgets.Dropdown` for model and backend selection, and `ipywidgets.Password` for Hugging Face token input. These widgets allow users to interactively choose an LLM, its target backend (CPU/GPU), and provide a necessary authentication token for gated models like Gemma.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
model = widgets.Dropdown(
    options=["Gemma 2B","Gemma 7B", "Falcon 1B", "StableLM 3B", "Phi 2"],
    value='Gemma 2B',
    description='model',
    disabled=False,
)

backend = widgets.Dropdown(
    options=["cpu", "gpu"],
    value='cpu',
    description='backend',
    disabled=False,
)

token = widgets.Password(
    value='',
    placeholder='huggingface token',
    description='HF token:',
    disabled=False
)
```

----------------------------------------

TITLE: Visualizing the Stylized Training Image
DESCRIPTION: This snippet loads the downloaded stylized image using `image_utils.load_image`, converts it from RGB to BGR format using OpenCV (`cv2`), and then displays it within the Google Colab environment using `cv2_imshow`. This step allows users to visually verify the input style image before proceeding with model training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
import cv2
from google.colab.patches import cv2_imshow

style_image_tensor = image_utils.load_image(style_image_path)
style_cv_image = cv2.cvtColor(style_image_tensor.numpy(), cv2.COLOR_RGB2BGR)
cv2_imshow(style_cv_image)
```

----------------------------------------

TITLE: Cloning MediaPipe Samples Repository - Bash
DESCRIPTION: This command downloads the MediaPipe samples demo code from the official GitHub repository. It is the initial step to obtain the source code for the LLM Inference Android demo app, enabling local setup and development.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/android/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
git clone https://github.com/google-ai-edge/mediapipe-samples
```

----------------------------------------

TITLE: Downloading Gemma2-2B-IT Model - Python
DESCRIPTION: This snippet downloads the Gemma2-2B-IT TFLite model file from HuggingFace Hub using `hf_hub_download`. It specifies the repository ID and the exact filename, storing the local path to the downloaded model in `model_path`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
from huggingface_hub import hf_hub_download

model_path = hf_hub_download(repo_id="litert-community/Gemma2-2B-IT", filename="gemma2_q8_seq128_ekv1280.tflite")
```

----------------------------------------

TITLE: Downloading Gemma Tokenizer Model for MediaPipe (Python)
DESCRIPTION: This snippet downloads the `tokenizer.model` file for the Gemma 3.1B model from Hugging Face Hub. It uses `hf_hub_download` to save the tokenizer locally to the `/content` directory, which is a prerequisite for building the MediaPipe LLM task bundle.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_30

LANGUAGE: python
CODE:
```
from huggingface_hub import hf_hub_download
import joblib

REPO_ID = "google/gemma-3-1b"
FILENAME = "tokenizer.model"
tokenizer_model = (
    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, local_dir="/content")
)
```

----------------------------------------

TITLE: Running Local HTTP Server with Python 3
DESCRIPTION: This command initiates a basic HTTP server using Python 3's `http.server` module. It hosts files from the current directory on port 8000, enabling local access to the MediaPipe LLM Inference web demo's HTML and JavaScript files. This is a prerequisite for running the demo locally.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/js/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
python3 -m http.server 8000
```

----------------------------------------

TITLE: Setting Up Environment for MediaPipe LLM Conversion - Python
DESCRIPTION: This snippet sets up the Python environment by installing `mediapipe`, `torch`, and `huggingface_hub` using pip. It then imports necessary modules like `ipywidgets`, `IPython.display`, `os`, `hf_hub_download` from `huggingface_hub`, and `converter` from `mediapipe.tasks.python.genai`. This prepares the environment for LLM conversion tasks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
# @title Setup { display-mode: "form" }
# @markdown import ipywidgets\
# @markdown import IPython.display\
# @markdown import os\
# @markdown import huggingface downloader\
# @markdown import mediapipe genai converter
import ipywidgets as widgets
from IPython.display import display
install_out = widgets.Output()
display(install_out)
with install_out:
  !pip install mediapipe
  !pip install torch
  !pip install huggingface_hub
  import os
  from huggingface_hub import hf_hub_download
  from mediapipe.tasks.python.genai import converter

install_out.clear_output()
with install_out:
  print("Setup done.")
```

----------------------------------------

TITLE: Initializing LiteRTLlmPipeline in Python
DESCRIPTION: This constructor initializes the `LiteRTLlmPipeline` class, setting up the interpreter and tokenizer. It also identifies and initializes the 'decode' signature runner, while the 'prefill' runner is initialized dynamically later.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
class LiteRTLlmPipeline:

  def __init__(self, interpreter, tokenizer):
    """Initializes the pipeline."""
    self._interpreter = interpreter
    self._tokenizer = tokenizer

    self._prefill_runner = None
    self._decode_runner = self._interpreter.get_signature_runner("decode")
```

----------------------------------------

TITLE: Downloading DeepLab v3 Image Segmenter Model
DESCRIPTION: This command downloads the pre-trained DeepLab v3 TFLite model from Google's storage. This model is specifically designed for image segmentation and is a prerequisite for initializing the MediaPipe ImageSegmenter.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
#@title Start downloading here.
!wget -O deeplabv3.tflite -q https://storage.googleapis.com/mediapipe-models/image_segmenter/deeplab_v3/float32/1/deeplab_v3.tflite
```

----------------------------------------

TITLE: Initializing LiteRT LLM Pipeline - Python
DESCRIPTION: This snippet demonstrates the instantiation of the LiteRTLlmPipeline class. It requires an 'interpreter' (likely for model execution) and a 'tokenizer' (for text processing) as arguments, setting up the necessary components for the LLM pipeline.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_28

LANGUAGE: python
CODE:
```
pipeline = LiteRTLlmPipeline(interpreter, tokenizer)
```

----------------------------------------

TITLE: Downloading Pre-trained Image Classification Model (Python)
DESCRIPTION: This command downloads a pre-trained efficientnet_lite0 image classification model in TFLite format from Google Cloud Storage. The model is saved as classifier.tflite and is essential for performing image classification inference.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -O classifier.tflite -q https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite
```

----------------------------------------

TITLE: Plotting Example Images from Gesture Dataset
DESCRIPTION: This snippet visualizes a specified number of example images for each gesture label in the dataset. It helps in understanding the dataset content and verifying the image quality and label association before model training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
NUM_EXAMPLES = 5

for label in labels:
  label_dir = os.path.join(dataset_path, label)
  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]
  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))
  for i in range(NUM_EXAMPLES):
    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))
    axs[i].get_xaxis().set_visible(False)
    axs[i].get_yaxis().set_visible(False)
  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')

plt.show()
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library, a prerequisite for using MediaPipe Tasks for text embedding. The `-q` flag ensures a quiet installation, suppressing verbose output during the process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_embedder/python/text_embedder.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing Python Dependencies with pip
DESCRIPTION: This command installs all necessary Python packages required for the MediaPipe samples. It includes libraries for deep learning (torch, pytorch_lightning), data manipulation (numpy, Pillow), and utilities (typing_extensions, requests, absl-py).
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tools/image_generator_converter/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
pip install torch typing_extensions numpy Pillow requests pytorch_lightning absl-py
```

----------------------------------------

TITLE: Installing MediaPipe Python Package
DESCRIPTION: This command installs the MediaPipe Python library using pip. The `-q` flag ensures a quiet installation, suppressing verbose output. MediaPipe is a prerequisite for using its text classification capabilities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/python/text_classifier.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Setting Up MediaPipe Object Detection Example on Raspberry Pi
DESCRIPTION: This shell script navigates to the project directory and executes `setup.sh` to install necessary dependencies and download TensorFlow Lite models for the MediaPipe object detection example on Raspberry Pi.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/raspberry_pi/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
cd mediapipe/examples/object_detection/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Installing MediaPipe Dependencies and Models (Shell)
DESCRIPTION: This shell script navigates to the project directory and executes the `setup.sh` script to install required dependencies and download TensorFlow Lite models for the MediaPipe face detection example on Raspberry Pi.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/raspberry_pi/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
cd mediapipe/examples/face_detection/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Installing LiteRT Pipeline Tools
DESCRIPTION: This command installs the LiteRT pipeline tools directly from the `ai-edge-apis` GitHub repository, specifically targeting the `litert_tools` subdirectory.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install git+https://github.com/google-ai-edge/ai-edge-apis.git#subdirectory=litert_tools
```

----------------------------------------

TITLE: Downloading MediaPipe Language Detector Model
DESCRIPTION: This command downloads the 'language_detector.tflite' model from Google Cloud Storage using 'wget'. The -O flag specifies the output filename as 'detector.tflite', and -q ensures quiet execution. This model is essential for performing language detection with MediaPipe.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/language_detector/python/[MediaPipe_Python_Tasks]_Language_Detector.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/language_detector/language_detector/float32/latest/language_detector.tflite
```

----------------------------------------

TITLE: Downloading Face Stylizer Model - Python
DESCRIPTION: This command downloads the pre-trained `face_stylizer.task` model from a Google Cloud Storage URL. This model contains the pre-determined style used for face stylization and is essential for the inference process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_stylizer/python/face_stylizer.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
#@title Start downloading here.
!wget -O face_stylizer.task -q https://storage.googleapis.com/mediapipe-models/face_stylizer/blaze_face_stylizer/float32/latest/face_stylizer_color_sketch.task
```

----------------------------------------

TITLE: Downloading MediaPipe Pose Landmarker Model
DESCRIPTION: This command downloads the 'pose_landmarker_heavy.task' model bundle from a Google Cloud Storage URL. This pre-trained model is essential for performing pose landmark detection with MediaPipe Tasks. The '-q' flag suppresses output, and '-O' specifies the output filename.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task
```

----------------------------------------

TITLE: Downloading MediaPipe Hand Landmarker Model - Python
DESCRIPTION: This command downloads the pre-trained 'hand_landmarker.task' model bundle from Google Cloud Storage. This model is essential for the MediaPipe HandLandmarker API to perform hand landmark detection. The '-q' flag ensures a quiet download.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task
```

----------------------------------------

TITLE: Downloading Gesture Recognizer Model - Python
DESCRIPTION: This command downloads an off-the-shelf MediaPipe Gesture Recognizer model from Google Cloud Storage using `wget`. The `-q` flag ensures quiet download. This `.task` file contains the pre-trained model weights and metadata required for gesture recognition.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task
```

----------------------------------------

TITLE: Downloading Interactive Segmenter Model
DESCRIPTION: This command downloads the `magic_touch.tflite` interactive segmentation model from Google Cloud Storage. The model is saved as `model.tflite` in the current directory, and the `-q` flag ensures quiet download progress.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
#@title Start downloading here.
!wget -O model.tflite -q https://storage.googleapis.com/mediapipe-models/interactive_segmenter/magic_touch/float32/1/magic_touch.tflite
```

----------------------------------------

TITLE: Updating Backend Options Based on Model Selection
DESCRIPTION: A callback function `on_use_gpu` that updates the `backend` dropdown options based on the currently selected model. It uses the `options_mapping` to filter available backends, ensuring only compatible options are presented to the user. If the model is not in the mapping, it clears token options.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
def on_use_gpu(change):
  selected_value = change['new']

  if selected_value in options_mapping:
    backend.options = options_mapping[selected_value]
    backend.value = options_mapping[selected_value][0]
  else:
    token.options = []
    token.value = None
```

----------------------------------------

TITLE: Setting up MediaPipe Gesture Recognizer on Raspberry Pi
DESCRIPTION: This command sequence navigates to the project directory and executes the setup script. The `setup.sh` script installs necessary dependencies and downloads the MediaPipe task file required for the gesture recognizer, preparing the environment for execution.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/raspberry_pi/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
cd mediapipe/examples/gesture_recognizer/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Installing MediaPipe Audio Classifier Dependencies (Shell)
DESCRIPTION: This command sequence navigates to the project directory and executes the `setup.sh` script. The script is responsible for installing necessary dependencies and downloading the TensorFlow Lite models required for the audio classification example.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/raspberry_pi/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
cd mediapipe/examples/audio_classifier/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Setting up MediaPipe Hand Landmarker Example on Raspberry Pi - Shell
DESCRIPTION: This shell script navigates to the MediaPipe hand landmarker example directory on the Raspberry Pi and then executes the `setup.sh` script. The `setup.sh` script is responsible for installing required dependencies and downloading the necessary task files for the hand landmarker.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/raspberry_pi/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
cd mediapipe/examples/hand_landmarker/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Handling Model Download and Conversion Button Click in Python
DESCRIPTION: This `on_button_clicked` function serves as a callback for an interactive button, orchestrating the download and conversion of selected AI models. It updates UI elements to reflect progress, calls specific download and conversion configuration functions based on the chosen model, and handles potential errors by resetting the UI and displaying messages.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
def on_button_clicked(b):
  try:
    out.clear_output()
    with out:
      print("Downloading model ...")
    button.description = "Downloading ..."
    button.disabled = True
    model.disabled = True
    backend.disabled = True

    if model.value == 'Gemma 2B':
      gemma2b_download(token.value)
    elif model.value == 'Gemma 7B':
      gemma7b_download(token.value)
    elif model.value == 'Falcon 1B':
      falcon_download()
    elif model.value == 'StableLM 3B':
      stablelm_download()
    elif model.value == 'Phi 2':
      phi2_download()
    else:
      raise Exception("Invalid model")

    with out:
      print("Done")
      print("Converting model ...")

    button.description = "Converting ..."

    if model.value == 'Gemma 2B':
      config = gemma2b_convert_config(backend.value)
    elif model.value == 'Gemma 7B':
      config = gemma7b_convert_config(backend.value)
    elif model.value == 'Falcon 1B':
      config = falcon_convert_config(backend.value)
    elif model.value == 'StableLM 3B':
      config = stablelm_convert_config(backend.value)
    elif model.value == 'Phi 2':
      config = phi2_convert_config(backend.value)
    else:
      with out:
        raise Exception("Invalid model")
      return

    with out:
      converter.convert_checkpoint(config)
      print("Done")

    button.description = "Start Conversion"
    button.disabled = False
    model.disabled = False
    backend.disabled = False

  except Exception as e:
    button.description = "Start Conversion"
    button.disabled = False
    model.disabled = False
    backend.disabled = False
    with out:
      print(e)
```

----------------------------------------

TITLE: Importing Libraries for Face Stylizer Customization
DESCRIPTION: This snippet imports necessary Python libraries for the face stylizer customization process. It includes `google.colab.files` for Colab-specific file operations, `os` for interacting with the operating system, `tensorflow` for ML operations (asserting version 2.x), and specific modules from `mediapipe_model_maker` (`face_stylizer`, `image_utils`) for model customization and image utilities, along with `matplotlib.pyplot` for plotting.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
from google.colab import files
import os
import tensorflow as tf
assert tf.__version__.startswith('2')

from mediapipe_model_maker import face_stylizer
from mediapipe_model_maker import image_utils

import matplotlib.pyplot as plt
```

----------------------------------------

TITLE: Downloading Test Image for Segmentation
DESCRIPTION: This Python script downloads a sample image from MediaPipe assets to be used as input for the segmentation demo. It uses `urllib.request.urlretrieve` to fetch the image and stores its filename in the `IMAGE_FILENAMES` array, which can be extended for multiple images.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
import urllib

IMAGE_FILENAMES = ['segmentation_input_rotation0.jpg']

for name in IMAGE_FILENAMES:
  url = f'https://storage.googleapis.com/mediapipe-assets/{name}'
  urllib.request.urlretrieve(url, name)
```

----------------------------------------

TITLE: Downloading Test Images for MediaPipe Gesture Recognition (Python)
DESCRIPTION: This snippet downloads a predefined set of test images from a Google Cloud Storage bucket. It uses the `urllib.request.urlretrieve` function to fetch each image and save it locally. The `IMAGE_FILENAMES` list specifies the names of the images to be downloaded.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
import urllib

IMAGE_FILENAMES = ['thumbs_down.jpg', 'victory.jpg', 'thumbs_up.jpg', 'pointing_up.jpg']

for name in IMAGE_FILENAMES:
  url = f'https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/{name}'
  urllib.request.urlretrieve(url, name)
```

----------------------------------------

TITLE: Defining Model-Backend Compatibility Mapping
DESCRIPTION: Defines a dictionary `options_mapping` that specifies which backends (CPU or GPU) are compatible with each large language model. This mapping is used to dynamically update the available backend options based on the selected model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
options_mapping = {
              'Gemma 2B': ['cpu', 'gpu'],
              'Gemma 7B': ['gpu'],
              'Falcon 1B': ['cpu', 'gpu'],
              'StableLM 3B': ['cpu', 'gpu'],
              'Phi 2': ['cpu', 'gpu']
}
```

----------------------------------------

TITLE: Optional Image Upload for Testing - Python
DESCRIPTION: This commented-out Python snippet provides functionality to upload a custom image from the local system using google.colab.files.upload(). It iterates through uploaded files, saves their content, and identifies the first uploaded file's name. This allows users to test the face landmark detection with their own images instead of the default one.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
# from google.colab import files
# uploaded = files.upload()

# for filename in uploaded:
#   content = uploaded[filename]
#   with open(filename, 'wb') as f:
#     f.write(content)

# if len(uploaded.keys()):
#   IMAGE_FILE = next(iter(uploaded))
#   print('Uploaded file:', IMAGE_FILE)
```

----------------------------------------

TITLE: Installing MediaPipe Dependencies and Downloading Task File - Shell
DESCRIPTION: This shell script navigates to the project directory and executes the `setup.sh` script. The `setup.sh` script is responsible for installing required dependencies and downloading the necessary task file for the MediaPipe face landmarker example on Raspberry Pi.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/raspberry_pi/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
cd mediapipe/examples/face_landmarker/raspberry_pi
sh setup.sh
```

----------------------------------------

TITLE: Installing AI Edge Torch and MediaPipe (Python)
DESCRIPTION: This snippet installs `ai-edge-torch` directly from its GitHub repository, along with `ai-edge-litert` and `mediapipe`. These libraries are crucial for converting the fine-tuned model to a LiteRT format and for on-device deployment and inference using MediaPipe.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
! pip install git+https://github.com/google-ai-edge/ai-edge-torch
! pip install ai-edge-litert
! pip install mediapipe
```

----------------------------------------

TITLE: Downloading and Displaying Test Image - Python
DESCRIPTION: This snippet downloads a sample image from Google Cloud Storage and then displays it using OpenCV and `cv2_imshow`, which is typically used in environments like Google Colab. This image serves as the input for the hand landmark detection process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
!wget -q -O image.jpg https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/woman_hands.jpg

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread("image.jpg")
cv2_imshow(img)
```

----------------------------------------

TITLE: Displaying and Resizing Images (Python)
DESCRIPTION: This snippet defines a utility function `resize_and_show` that resizes an image while maintaining its aspect ratio and then displays it using `cv2_imshow`. It then loads and displays the previously downloaded `burger.jpg` and `burger_crop.jpg` images to visually confirm their content.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_embedder/python/image_embedder.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
import cv2
from google.colab.patches import cv2_imshow
import math

DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


# Preview the images.
images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
for name, image in images.items():
  print(name)
  resize_and_show(image)
```

----------------------------------------

TITLE: Downloading SST-2 Sentiment Analysis Dataset
DESCRIPTION: This code downloads the SST-2 (Stanford Sentiment Treebank) dataset, a ZIP file containing movie reviews for sentiment analysis, and extracts it to a local directory. This dataset is used for training and testing text classification models.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
data_path = tf.keras.utils.get_file(
    fname='SST-2.zip',
    origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',
    extract=True)
data_dir = os.path.join(os.path.dirname(data_path), 'SST-2')  # folder name
```

----------------------------------------

TITLE: Importing MediaPipe Model Maker Quantization Module (Python)
DESCRIPTION: Imports the `quantization` module from `mediapipe_model_maker`, which provides functionalities for post-training model quantization. This is a prerequisite for applying quantization to a retrained model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
from mediapipe_model_maker import quantization
```

----------------------------------------

TITLE: Optional Image Upload for Object Detection - Python
DESCRIPTION: This commented-out code block provides an optional mechanism for users to upload their own image files directly into the Google Colab environment. It utilizes `google.colab.files` to handle the file upload, writes the content to a local file, and updates the `IMAGE_FILE` variable to point to the newly uploaded image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# from google.colab import files
# uploaded = files.upload()

# for filename in uploaded:
#   content = uploaded[filename]
#   with open(filename, 'wb') as f:
#     f.write(content)

# if len(uploaded.keys()):
#   IMAGE_FILE = next(iter(uploaded))
#   print('Uploaded file:', IMAGE_FILE)
```

----------------------------------------

TITLE: Downloading and Displaying Test Image in Python
DESCRIPTION: This snippet first downloads a sample image named 'image.jpg' from Pixabay, which will be used as input for the pose detection model. Subsequently, it uses the OpenCV library (`cv2`) to read the downloaded image and then displays it within the Google Colab environment using `cv2_imshow`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
!wget -q -O image.jpg https://cdn.pixabay.com/photo/2019/03/12/20/39/girl-4051811_960_720.jpg

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread("image.jpg")
cv2_imshow(img)
```

----------------------------------------

TITLE: Downloading and Displaying Test Image - Python
DESCRIPTION: This snippet downloads a sample image, 'cat_and_dog.jpg', from a Google Cloud Storage URL, saving it locally. It then uses OpenCV (`cv2`) to read the downloaded image and displays it within the notebook environment using `cv2_imshow`, preparing it for subsequent object detection tasks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
!wget -q -O image.jpg https://storage.googleapis.com/mediapipe-tasks/object_detector/cat_and_dog.jpg

IMAGE_FILE = 'image.jpg'

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread(IMAGE_FILE)
cv2_imshow(img)
```

----------------------------------------

TITLE: Importing Required Libraries for Gesture Recognition
DESCRIPTION: This snippet imports essential Python libraries for the gesture recognition task, including `files` and `os` for file operations, `tensorflow` for ML operations, `gesture_recognizer` from `mediapipe_model_maker`, and `matplotlib.pyplot` for plotting.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
from google.colab import files
import os
import tensorflow as tf
assert tf.__version__.startswith('2')

from mediapipe_model_maker import gesture_recognizer

import matplotlib.pyplot as plt
```

----------------------------------------

TITLE: Importing Libraries for MediaPipe Object Detector Customization (Python)
DESCRIPTION: This snippet imports the required Python modules for object detection model customization, typically used in a Google Colab environment. It includes `google.colab.files` for file operations, `os` and `json` for general utilities, `tensorflow` for ML operations (asserting version 2.x), and `mediapipe_model_maker.object_detector` for the core model customization functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
from google.colab import files
import os
import json
import tensorflow as tf
assert tf.__version__.startswith('2')

from mediapipe_model_maker import object_detector
```

----------------------------------------

TITLE: Importing MediaPipe Model Maker and Dependencies
DESCRIPTION: This snippet imports essential Python libraries: 'os' for operating system interaction, 'tensorflow' (asserting version 2.x), 'google.colab.files' for Colab-specific file operations, and 'object_detector' from 'mediapipe_model_maker' for object detection functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import os
import tensorflow as tf
assert tf.__version__.startswith('2')
from google.colab import files

from mediapipe_model_maker import object_detector
```

----------------------------------------

TITLE: Downloading Sample Images (Python)
DESCRIPTION: This snippet downloads two sample image files, `burger.jpg` and `burger_crop.jpg`, from Google Cloud Storage. It uses the `urllib.request` module to retrieve the images, which are then used as input for the image embedding process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_embedder/python/image_embedder.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
import urllib

IMAGE_FILENAMES = ['burger.jpg', 'burger_crop.jpg']

for name in IMAGE_FILENAMES:
  url = f'https://storage.googleapis.com/mediapipe-assets/{name}'
  urllib.request.urlretrieve(url, name)
```

----------------------------------------

TITLE: Importing Required Libraries for MediaPipe Image Classifier Customization in Python
DESCRIPTION: This snippet imports essential Python libraries for image classification model customization. It includes `google.colab.files` for Colab-specific file operations, `os` for operating system interactions, `tensorflow` for ML operations (asserting version 2.x), `mediapipe_model_maker.image_classifier` for the core model customization functionalities, and `matplotlib.pyplot` for plotting and visualization.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
from google.colab import files
import os
import tensorflow as tf
assert tf.__version__.startswith('2')

from mediapipe_model_maker import image_classifier

import matplotlib.pyplot as plt
```

----------------------------------------

TITLE: Downloading Stylized Face Image for Training
DESCRIPTION: This code downloads a sample stylized face image (`color_sketch.jpg`) from a Google Cloud Storage URL. This image serves as the target style for retraining the face stylizer model, demonstrating how to provide the single stylized face image required for the customization process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
style_image_path = 'color_sketch.jpg'
!wget -q -O {style_image_path} https://storage.googleapis.com/mediapipe-assets/face_stylizer_style_color_sketch.jpg
```

----------------------------------------

TITLE: Resizing and Displaying Test Images (Python)
DESCRIPTION: This Python code defines a `resize_and_show` function to scale images while maintaining aspect ratio, ensuring they fit within a `DESIRED_WIDTH` and `DESIRED_HEIGHT`. It then loads the previously downloaded test images using OpenCV (`cv2.imread`) and displays them using `cv2_imshow` (a Colab-specific patch for OpenCV display), providing a visual preview of the input data. It depends on `cv2`, `google.colab.patches`, and `math`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
import cv2
from google.colab.patches import cv2_imshow
import math

DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


# Preview the images.

images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
for name, image in images.items():
  print(name)
  resize_and_show(image)
```

----------------------------------------

TITLE: Previewing Downloaded Images with Resizing
DESCRIPTION: This snippet defines a `resize_and_show` function to scale images while maintaining aspect ratio and then displays the downloaded test image using OpenCV's `cv2_imshow`. It requires `cv2` and `google.colab.patches.cv2_imshow` for execution in a Colab environment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import cv2
from google.colab.patches import cv2_imshow
import math

# Height and width that will be used by the model
DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

# Performs resizing and showing the image
def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


# Preview the image(s)
images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
for name, image in images.items():
  print(name)
  resize_and_show(image)
```

----------------------------------------

TITLE: Resizing and Displaying Images with OpenCV in Colab (Python)
DESCRIPTION: This snippet defines a utility function `resize_and_show` to resize an image while maintaining its aspect ratio and then displays it using `cv2_imshow` (specific to Google Colab). It then loads all images specified in `IMAGE_FILENAMES` using OpenCV and previews them. Dependencies include `opencv-python` and `google.colab.patches`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
import cv2

from google.colab.patches import cv2_imshow
import math

DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


# Preview the images.
images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
for name, image in images.items():
  print(name)
  resize_and_show(image)
```

----------------------------------------

TITLE: Importing Transformers Pipeline (Python)
DESCRIPTION: This snippet imports the `pipeline` function from the `transformers` library, along with the `torch` library. The `pipeline` function provides a high-level API for various NLP tasks, simplifying the process of using pre-trained models for inference, such as text generation or question answering.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import torch

from transformers import pipeline
```

----------------------------------------

TITLE: Resizing and Displaying Images with OpenCV in Python
DESCRIPTION: This Python code defines a `resize_and_show` function to resize an image while maintaining its aspect ratio and displays it using `cv2_imshow`. It then iterates through downloaded images, printing their names and displaying them, preparing them for segmentation.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
import cv2
from google.colab.patches import cv2_imshow
import math

# Height and width that will be used by the model
DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

# Performs resizing and showing the image
def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


# Preview the image(s)
images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
for name, image in images.items():
  print(name)
  resize_and_show(image)
```

----------------------------------------

TITLE: Downloading Sample Image for Interactive Segmentation
DESCRIPTION: This Python snippet downloads a sample image, `cats_and_dogs.jpg`, from Google Cloud Storage using `urllib.request`. The image is used as input for demonstrating the interactive segmenter, and the code can be extended to download multiple images.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import urllib
IMAGE_FILENAMES = ['cats_and_dogs.jpg']

for name in IMAGE_FILENAMES:
  url = f'https://storage.googleapis.com/mediapipe-assets/{name}'
  urllib.request.urlretrieve(url, name)
```

----------------------------------------

TITLE: Printing COCO Dataset Categories in Python
DESCRIPTION: This Python snippet reads the `labels.json` file from a COCO dataset path, loads its content, and iterates through the 'categories' array to print each category's ID and name. It helps verify the dataset's category definitions, expecting a 'background' class at index 0 and two non-background categories.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
with open(os.path.join(train_dataset_path, "labels.json"), "r") as f:
  labels_json = json.load(f)
for category_item in labels_json["categories"]:
  print(f"{category_item['id']}: {category_item['name']}")
```

----------------------------------------

TITLE: Downloading Test Image - Python
DESCRIPTION: This Python snippet downloads a test image (`business-person.png`) from Google Cloud Storage using `urllib.request.urlretrieve`. The image is used as input for the face stylization process, demonstrating how to prepare input data for the model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_stylizer/python/face_stylizer.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
import urllib
IMAGE_FILENAMES = ['business-person.png']

for name in IMAGE_FILENAMES:
  url = f'https://storage.googleapis.com/mediapipe-assets/{name}'
  urllib.request.urlretrieve(url, name)
```

----------------------------------------

TITLE: Downloading Test Images (Python)
DESCRIPTION: This Python script downloads two sample images, 'burger.jpg' and 'cat.jpg', from a Google Cloud Storage URL. It uses the `urllib` module to retrieve the images and save them locally, which are then used for testing the image classification model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import urllib

IMAGE_FILENAMES = ['burger.jpg', 'cat.jpg']

for name in IMAGE_FILENAMES:
  url = f'https://storage.googleapis.com/mediapipe-tasks/image_classifier/{name}'
  urllib.request.urlretrieve(url, name)
```

----------------------------------------

TITLE: Importing Libraries for Text Classification
DESCRIPTION: This snippet imports necessary Python libraries: `os` for operating system interactions, `tensorflow` for machine learning operations (asserting version 2.x), and `text_classifier` from `mediapipe_model_maker` for text classification functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import os
import tensorflow as tf
assert tf.__version__.startswith('2')

from mediapipe_model_maker import text_classifier
```

----------------------------------------

TITLE: Downloading Sample Audio File - Python
DESCRIPTION: This Python snippet uses the `urllib` module to download a sample `.wav` audio file named `speech_16000_hz_mono.wav` from a Google Cloud Storage URL. This audio file will be used as input for the audio classification task.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/python/audio_classification.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
import urllib

audio_file_name = 'speech_16000_hz_mono.wav'
url = f'https://storage.googleapis.com/mediapipe-assets/{audio_file_name}'
urllib.request.urlretrieve(url, audio_file_name)
```

----------------------------------------

TITLE: Downloading the Generated MediaPipe Task Bundle
DESCRIPTION: This Python snippet facilitates downloading the `.task` file generated by the bundler. It uses `google.colab.files.download` to prompt the user to download the output bundle file, which is specified by the `output_filename` variable from the previous step.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/bundling/llm_bundling.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
#@title Download the Bundle { display-mode: "form" }
#@markdown Run this cell to download the generated `.task` file.
files.download(output_filename)
```

----------------------------------------

TITLE: Listing and Downloading Exported Model in Google Colab (Python)
DESCRIPTION: These commands are specific to a Google Colab environment. The first command lists the contents of the `exported_model` directory, and the second command downloads the `model.tflite` file to the local development environment. This facilitates access to the exported model for further use.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
!ls exported_model
files.download('exported_model/model.tflite')
```

----------------------------------------

TITLE: Verifying Dataset Labels for Gesture Recognition
DESCRIPTION: This snippet verifies the downloaded dataset by listing and printing the detected labels. It iterates through the dataset directory to identify subdirectories, which represent the different gesture labels, ensuring the 'none' label is present.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
print(dataset_path)
labels = []
for i in os.listdir(dataset_path):
  if os.path.isdir(os.path.join(dataset_path, i)):
    labels.append(i)
print(labels)
```

----------------------------------------

TITLE: Listing Exported Model Files (Shell)
DESCRIPTION: Executes a shell command to list the contents of the `exported_model` directory in a long format, showing file sizes in human-readable units. This command is used to verify the creation and compare the size of the newly exported quantized model (`model_int8.tflite`) against the original.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_17

LANGUAGE: Shell
CODE:
```
!ls -lh exported_model
```

----------------------------------------

TITLE: Instantiating AI Edge LiteRT Interpreter (Python)
DESCRIPTION: This snippet creates an instance of the `InterpreterWithCustomOps` from the `ai_edge_litert` library. It configures the interpreter to use custom GenAI operations, loads the quantized Gemma LiteRT model from the specified path, sets the number of inference threads, and enables experimental delegate features for optimal performance.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_17

LANGUAGE: Python
CODE:
```
interpreter = interpreter_lib.InterpreterWithCustomOps(
    custom_op_registerers=["pywrap_genai_ops.GenAIOpsRegisterer"],
    model_path="/content/gemma3_1b_finetune_q8_ekv1024.tflite",
    num_threads=2,
    experimental_default_delegate_latest_features=True)
```

----------------------------------------

TITLE: Importing AI Edge LiteRT Interpreter Dependencies (Python)
DESCRIPTION: This snippet imports essential Python libraries required for working with AI Edge LiteRT models. It includes `interpreter_lib` for the LiteRT interpreter, `AutoTokenizer` from `transformers` for tokenization, `numpy` for numerical operations, and `collections.abc.Sequence` and `sys` for general utility.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
from ai_edge_litert import interpreter as interpreter_lib
from transformers import AutoTokenizer
import numpy as np
from collections.abc import Sequence
import sys
```

----------------------------------------

TITLE: Uploading Custom Images in Google Colab (Python)
DESCRIPTION: This commented-out snippet provides an optional way to upload custom images when running in a Google Colab environment. It uses `google.colab.files.upload()` to prompt the user for file selection, then saves the uploaded content to local files and updates the `IMAGE_FILENAMES` list.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
# from google.colab import files
# uploaded = files.upload()

# for filename in uploaded:
#   content = uploaded[filename]
#   with open(filename, 'wb') as f:
#     f.write(content)
# IMAGE_FILENAMES = list(uploaded.keys())

# print('Uploaded files:', IMAGE_FILENAMES)
```

----------------------------------------

TITLE: Uploading Custom Image (Commented) - Python
DESCRIPTION: This commented-out Python snippet provides an alternative method for users to upload their own image files from their local machine to the Colab environment. It uses `google.colab.files.upload()` to handle the file upload and sets `IMAGE_FILE` to the name of the uploaded file.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# from google.colab import files
# uploaded = files.upload()

# for filename in uploaded:
#   content = uploaded[filename]
#   with open(filename, 'wb') as f:
#     f.write(content)

# if len(uploaded.keys()):
#   IMAGE_FILE = next(iter(uploaded))
#   print('Uploaded file:', IMAGE_FILE)
```

----------------------------------------

TITLE: Saving Converted Models to Google Drive in Python
DESCRIPTION: This Python snippet, designed for Google Colab, mounts Google Drive and copies the converted models from the local `/content/converted_models` directory to a specified folder within 'My Drive'. It includes shell commands for directory creation and file copying, providing persistent storage for the generated model files.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
# @title Save to google drive { display-mode: "form"}
google_drive_directory = "converted_models" #@param {type:"string"}
from google.colab import drive
drive.mount('/content/drive')
print("Copying models ...")
!mkdir -p /content/drive/MyDrive/$google_drive_directory
!cp -r -f /content/converted_models/* /content/drive/MyDrive/$google_drive_directory
print("Done")
```

----------------------------------------

TITLE: Installing Dependencies for LiteRT
DESCRIPTION: This snippet downloads and extracts the `protoc` compiler, a prerequisite for some LiteRT tools, ensuring it's available in the system's local directory.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
! wget -q https://github.com/protocolbuffers/protobuf/releases/download/v3.19.0/protoc-3.19.0-linux-x86_64.zip
! unzip -o protoc-3.19.0-linux-x86_64.zip -d /usr/local/
```

----------------------------------------

TITLE: Importing MediaPipe Model Maker Quantization Module (Python)
DESCRIPTION: This snippet shows the necessary import statement to access the quantization functionalities within the MediaPipe Model Maker library, which is a prerequisite for applying post-training quantization.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_16

LANGUAGE: Python
CODE:
```
from mediapipe_model_maker import quantization
```

----------------------------------------

TITLE: Printing Facial Transformation Matrix with MediaPipe in Python
DESCRIPTION: This snippet prints the facial transformation matrix obtained from the MediaPipe FaceLandmarker detection result. The `facial_transformation_matrixes` property contains the 3D transformation data for the detected face(s).
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
print(detection_result.facial_transformation_matrixes)
```

----------------------------------------

TITLE: Attaching Observers and Displaying UI Widgets
DESCRIPTION: Attaches the `on_change_model` and `on_use_gpu` functions as observers to the `model` dropdown, ensuring dynamic updates. It then displays all interactive widgets (`model`, `backend`, `token_description`, `token`) in the UI, along with a helpful message about the Hugging Face token.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
model.observe(on_change_model, names='value')
model.observe(on_use_gpu, names='value')


display(model)
display(backend)

token_description = widgets.Output()
with token_description:
  print("Huggingface token needed for gated model (e.g. Gemma)")
  print("You can get it from https://huggingface.co/settings/tokens")

display(token_description)
display(token)
```

----------------------------------------

TITLE: Optional: Uploading Custom Images (Python)
DESCRIPTION: This commented-out Python code provides an optional method for users to upload their own images in a Google Colab environment. It uses `google.colab.files` to handle file uploads and then saves the content locally, updating the `IMAGE_FILENAMES` list for subsequent processing.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# from google.colab import files
# uploaded = files.upload()

# for filename in uploaded:
#   content = uploaded[filename]
#   with open(filename, 'wb') as f:
#     f.write(content)
# IMAGE_FILENAMES = list(uploaded.keys())

# print('Uploaded files:', IMAGE_FILENAMES)
```

----------------------------------------

TITLE: Defining Input Text for Language Detection in Python
DESCRIPTION: This snippet defines a Python string variable 'INPUT_TEXT' containing a Chinese phrase. This variable serves as the input for the MediaPipe language detector, demonstrating how to prepare text for analysis. The #@param comment indicates it's a parameter in a notebook environment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/language_detector/python/[MediaPipe_Python_Tasks]_Language_Detector.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
INPUT_TEXT = "\u5206\u4E45\u5FC5\u5408\u5408\u5FC5\u5206" #@param {type:"string"}
```

----------------------------------------

TITLE: Defining Input Text for Classification in Python
DESCRIPTION: This snippet defines a string variable `INPUT_TEXT` which holds the text that will be classified by the MediaPipe model. This serves as the input for the text classification inference process.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/python/text_classifier.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
INPUT_TEXT = "I'm looking forward to what will come next."
```

----------------------------------------

TITLE: Running Decode Operation for LLM in Python
DESCRIPTION: This method is responsible for the iterative decoding phase of text generation. It takes the starting position, initial token ID, the current KV cache, and a maximum number of decode steps to generate subsequent tokens.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
  def _run_decode(
      self,
      start_pos: int,
      start_token_id: int,
      kv_cache: dict[str, np.ndarray],
      max_decode_steps: int,
  ) -> str:
```

----------------------------------------

TITLE: Running Prefill Operation for LLM in Python
DESCRIPTION: This method executes the prefill operation, preparing the input tokens and their positions for the model. It initializes the KV cache and then runs the prefill runner, returning the updated KV cache for subsequent decode steps.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
  def _run_prefill(
      self, prefill_token_ids: Sequence[int],
  ) -> dict[str, np.ndarray]:
    """Runs prefill and returns the kv cache.

    Args:
      prefill_token_ids: The token ids of the prefill input.

    Returns:
      The updated kv cache.
    """
    if not self._prefill_runner:
      raise ValueError("Prefill runner is not initialized.")
    prefill_token_length = len(prefill_token_ids)
    if prefill_token_length == 0:
      return self._init_kv_cache()

    # Prepare the input to be [1, max_seq_len].
    input_token_ids = [0] * self._max_seq_len
    input_token_ids[:prefill_token_length] = prefill_token_ids
    input_token_ids = np.asarray(input_token_ids, dtype=np.int32)
    input_token_ids = np.expand_dims(input_token_ids, axis=0)

    # Prepare the input position to be [max_seq_len].
    input_pos = [0] * self._max_seq_len
    input_pos[:prefill_token_length] = range(prefill_token_length)
    input_pos = np.asarray(input_pos, dtype=np.int32)

    # Initialize kv cache.
    prefill_inputs = self._init_kv_cache()
    prefill_inputs.update({
        "tokens": input_token_ids,
        "input_pos": input_pos,
    })
    prefill_outputs = self._prefill_runner(**prefill_inputs)
    if "logits" in prefill_outputs:
      # Prefill outputs includes logits and kv cache. We only output kv cache.
      prefill_outputs.pop("logits")

    return prefill_outputs
```

----------------------------------------

TITLE: Running Decode Stage - LiteRT LLM Pipeline - Python
DESCRIPTION: This method performs the iterative decoding process, generating tokens one by one. It updates input positions, manages the KV cache, dynamically creates attention masks for each step, and accumulates the generated text until an end-of-sequence token is encountered or the maximum decode steps are reached.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_26

LANGUAGE: python
CODE:
```
  def _run_decode(
      self,
      start_pos: int,
      start_token_id: int,
      kv_cache: dict[str, np.ndarray],
      max_decode_steps: int,
  ) -> str:
    """Runs decode and outputs the token ids from greedy sampler.

    Args:
      start_pos: The position of the first token of the decode input.
      start_token_id: The token id of the first token of the decode input.
      kv_cache: The kv cache from the prefill.
      max_decode_steps: The max decode steps.

    Returns:
      The token ids from the greedy sampler.
    """
    next_pos = start_pos
    next_token = start_token_id
    decode_text = []
    decode_inputs = kv_cache

    for _ in range(max_decode_steps):
      decode_inputs.update({
          "tokens": np.array([[next_token]], dtype=np.int32),
          "input_pos": np.array([next_pos], dtype=np.int32),
      })
      if "mask" in self._decode_runner.get_input_details().keys():
        # For decode, mask has shape [batch=1, 1, 1, kv_cache_size].
        # We want mask[0, 0, 0, j] = 0 for j<=next_pos and -inf otherwise.
        decode_inputs["mask"] = _get_mask(
            shape=self._decode_runner.get_input_details()["mask"]["shape"],
            k=next_pos + 1,
        )
      decode_outputs = self._decode_runner(**decode_inputs)
      # Output logits has shape (batch=1, 1, vocab_size). We only take the first
      # element.
      logits = decode_outputs.pop("logits")[0][0]
      next_token = self._greedy_sampler(logits)
      if next_token == self._tokenizer.eos_token_id:
        break
      decode_text.append(self._tokenizer.decode(next_token, skip_special_tokens=True))
      if len(decode_text[-1]) == 0:
        # Break out the loop if we hit the special token.
        break

      print(decode_text[-1], end='', flush=True)
      # Decode outputs includes logits and kv cache. We already poped out
      # logits, so the rest is kv cache. We pass the updated kv cache as input
      # to the next decode step.
      decode_inputs = decode_outputs
      next_pos += 1

    print() # print a new line at the end.
    return ''.join(decode_text)
```

----------------------------------------

TITLE: Initializing and Displaying UI Widgets in Python
DESCRIPTION: This snippet initializes a `Button` widget with 'Start Conversion' text and attaches the `on_button_clicked` function as its event handler. It also creates an `Output` widget for displaying messages and both widgets are displayed, providing an interactive interface for model operations and status updates.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
button = widgets.Button(description="Start Conversion")

button.on_click(on_button_clicked)
display(button)

out = widgets.Output(layout={'border': '1px solid black'})
display(out)

print("\nNotice: Converted models are saved under ./converted_models")
```

----------------------------------------

TITLE: Listing and Downloading Exported Model in Google Colab (Python)
DESCRIPTION: This snippet provides commands for use within Google Colab to list the contents of the 'exported_model' directory and download the 'face_stylizer.task' file. The '.task' file is a crucial bundle containing three TFLite models required to run the MediaPipe face stylizer task library in a development environment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
!ls exported_model
files.download('exported_model/face_stylizer.task')
```

----------------------------------------

TITLE: Initializing KV Cache in Python
DESCRIPTION: This method initializes the Key-Value (KV) cache, which is crucial for efficient sequence generation in LLMs. It creates a dictionary of zero-filled NumPy arrays for each 'kv_cache' input key identified by the prefill runner.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
  def _init_kv_cache(self) -> dict[str, np.ndarray]:
    if self._prefill_runner is None:
      raise ValueError("Prefill runner is not initialized.")
    kv_cache = {}
    for input_key in self._prefill_runner.get_input_details().keys():
      if "kv_cache" in input_key:
        kv_cache[input_key] = np.zeros(
            self._prefill_runner.get_input_details()[input_key]["shape"],
            dtype=np.float32,
        )
        kv_cache[input_key] = np.zeros(
            self._prefill_runner.get_input_details()[input_key]["shape"],
            dtype=np.float32,
        )
    return kv_cache
```

----------------------------------------

TITLE: Initializing KV Cache for LiteRTLlmPipeline (Python)
DESCRIPTION: This method initializes the Key-Value (KV) cache, which is crucial for efficient LLM inference. It creates a dictionary of NumPy arrays, where each array corresponds to a KV cache input of the prefill runner and is initialized with zeros based on the expected shape.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
  def _init_kv_cache(self) -> dict[str, np.ndarray]:
    if self._prefill_runner is None:
      raise ValueError("Prefill runner is not initialized.")
    kv_cache = {}
    for input_key in self._prefill_runner.get_input_details().keys():
      if "kv_cache" in input_key:
        kv_cache[input_key] = np.zeros(
            self._prefill_runner.get_input_details()[input_key]["shape"],
            dtype=np.float32,
        )
        kv_cache[input_key] = np.zeros(
            self._prefill_runner.get_input_details()[input_key]["shape"],
            dtype=np.float32,
        )
    return kv_cache
```

----------------------------------------

TITLE: Initializing Prefill Runner for LLM in Python
DESCRIPTION: This method initializes the prefill runner and determines the maximum sequence lengths for both input and KV cache based on the provided number of input tokens. It selects the most suitable prefill runner from available signatures.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
  def _init_prefill_runner(self, num_input_tokens: int):
    """Initializes all the variables related to the prefill runner.

    This method initializes the following variables:
      - self._prefill_runner: The prefill runner based on the input size.
      - self._max_seq_len: The maximum sequence length supported by the model.
      - self._max_kv_cache_seq_len: The maximum sequence length supported by the
        KV cache.

    Args:
      num_input_tokens: The number of input tokens.
    """
    if not self._interpreter:
      raise ValueError("Interpreter is not initialized.")

    # Prefill runner related variables will be initialized in `predict_text` and
    # `compute_log_likelihood`.
    self._prefill_runner = self._get_prefill_runner(num_input_tokens)
    # input_token_shape has shape (batch, max_seq_len)
    input_token_shape = self._prefill_runner.get_input_details()["tokens"][
        "shape"
    ]
    if len(input_token_shape) == 1:
      self._max_seq_len = input_token_shape[0]
    else:
      self._max_seq_len = input_token_shape[1]

    # kv cache input has shape [batch=1, seq_len, num_heads, dim].
    kv_cache_shape = self._prefill_runner.get_input_details()["kv_cache_k_0"][
        "shape"
    ]
    self._max_kv_cache_seq_len = kv_cache_shape[1]
```

----------------------------------------

TITLE: Initializing Prefill Runner in LiteRTLlmPipeline (Python)
DESCRIPTION: This method initializes the prefill runner and related variables, such as `_max_seq_len` and `_max_kv_cache_seq_len`, based on the number of input tokens. It dynamically selects the appropriate prefill runner from the interpreter's signatures and extracts sequence length information from its input details.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
  def _init_prefill_runner(self, num_input_tokens: int):
    """Initializes all the variables related to the prefill runner.

    This method initializes the following variables:
      - self._prefill_runner: The prefill runner based on the input size.
      - self._max_seq_len: The maximum sequence length supported by the model.

    Args:
      num_input_tokens: The number of input tokens.
    """
    if not self._interpreter:
      raise ValueError("Interpreter is not initialized.")

    # Prefill runner related variables will be initialized in `predict_text` and
    # `compute_log_likelihood`.
    self._prefill_runner = self._get_prefill_runner(num_input_tokens)
    # input_token_shape has shape (batch, max_seq_len)
    input_token_shape = self._prefill_runner.get_input_details()["tokens"][
        "shape"
    ]
    if len(input_token_shape) == 1:
      self._max_seq_len = input_token_shape[0]
    else:
      self._max_seq_len = input_token_shape[1]

    # kv cache input has shape [batch=1, num_kv_heads, cache_size, head_dim].
    kv_cache_shape = self._prefill_runner.get_input_details()["kv_cache_k_0"][
        "shape"
    ]
    self._max_kv_cache_seq_len = kv_cache_shape[2]
```

----------------------------------------

TITLE: Selecting Best Prefill Runner in Python
DESCRIPTION: This method dynamically selects the most suitable prefill runner from the interpreter's available signatures. It prioritizes runners that can accommodate the `num_input_tokens` with the smallest possible sequence length, optimizing for efficiency.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
  def _get_prefill_runner(self, num_input_tokens: int) :
    """Gets the prefill runner with the best suitable input size.

    Args:
      num_input_tokens: The number of input tokens.

    Returns:
      The prefill runner with the smallest input size.
    """
    best_signature = None
    delta = sys.maxsize
    max_prefill_len = -1
    for key in self._interpreter.get_signature_list().keys():
      if "prefill" not in key:
        continue
      input_pos = self._interpreter.get_signature_runner(key).get_input_details()[
          "input_pos"
      ]
      # input_pos["shape"] has shape (max_seq_len, )
      seq_size = input_pos["shape"][0]
      max_prefill_len = max(max_prefill_len, seq_size)
      if num_input_tokens <= seq_size and seq_size - num_input_tokens < delta:
        delta = seq_size - num_input_tokens
        best_signature = key
    if best_signature is None:
      raise ValueError(
          "The largest prefill length supported is %d, but we have %d number of input tokens"
          %(max_prefill_len, num_input_tokens)
      )
    return self._interpreter.get_signature_runner(best_signature)
```

----------------------------------------

TITLE: Running Local HTTP Server with Python 2
DESCRIPTION: This command starts a simple HTTP server using Python 2's `SimpleHTTPServer` module. It serves files from the current directory on port 8000, providing local access to the MediaPipe LLM Inference web demo. This option is intended for environments with older Python versions.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/js/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
python -m SimpleHTTPServer 8000
```

----------------------------------------

TITLE: Defining Interactive Slider Parameters for ROI
DESCRIPTION: This Python snippet defines `x` and `y` variables, marked as interactive sliders for a Colab environment. These parameters represent normalized coordinates (0 to 1) and are intended to specify the Region of Interest (ROI) for the interactive segmenter.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
x = 0.68 #@param {type:"slider", min:0, max:1, step:0.01}
y = 0.68 #@param {type:"slider", min:0, max:1, step:0.01}
```

----------------------------------------

TITLE: Playing Back Downloaded Audio in IPython - Python
DESCRIPTION: This snippet uses `IPython.display.Audio` to create an audio playback widget within an IPython or Jupyter environment. It allows users to verify the downloaded audio file (`speech_16000_hz_mono.wav`) by playing it back directly in the notebook, with `autoplay` set to `False`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/python/audio_classification.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
from IPython.display import Audio, display

file_name = 'speech_16000_hz_mono.wav'
display(Audio(file_name, autoplay=False))
```

----------------------------------------

TITLE: Uploading Custom Image (Optional) - Python
DESCRIPTION: This commented-out snippet provides an optional way for users to upload their own image files from their local machine. Once uploaded, the `IMAGE_FILE` variable would store the name of the first uploaded file, allowing it to be used as input for the hand landmark detection.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
# from google.colab import files
# uploaded = files.upload()

# for filename in uploaded:
#   content = uploaded[filename]
#   with open(filename, 'wb') as f:
#     f.write(content)

# if len(uploaded.keys()):
#   IMAGE_FILE = next(iter(uploaded))
#   print('Uploaded file:', IMAGE_FILE)
```

----------------------------------------

TITLE: License Information for MediaPipe Model Maker Python
DESCRIPTION: This snippet provides the Apache License, Version 2.0, under which the MediaPipe Authors distribute their code. It specifies the terms and conditions for use, reproduction, and distribution of the software, ensuring compliance with open-source guidelines.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title License information
# Copyright 2023 The MediaPipe Authors.
# Licensed under the Apache License, Version 2.0 (the "License");
#
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache 2.0 License Header - Python
DESCRIPTION: This snippet contains the Apache License, Version 2.0 header, indicating the terms under which the code is distributed and used. It specifies permissions, limitations, and disclaimers for the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_stylizer/python/face_stylizer.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache 2.0 License Header - Python
DESCRIPTION: This snippet contains the standard Apache License, Version 2.0 header, outlining the terms under which the code is provided and may be used. It specifies the conditions for distribution, modification, and limitations of liability.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License 2.0 for MediaPipe Samples - Python
DESCRIPTION: This snippet provides the Apache License, Version 2.0, under which the MediaPipe samples are distributed. It outlines the terms and conditions for use, reproduction, and distribution of the software, emphasizing the disclaimer of warranties and limitations of liability.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/python/audio_classification.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header for MediaPipe Samples
DESCRIPTION: This snippet contains the standard Apache License 2.0 header, outlining the terms under which the MediaPipe samples are distributed and used, ensuring compliance with open-source licensing requirements.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/interactive_segmentation/python/interactive_segmenter.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache 2.0 License Header
DESCRIPTION: This snippet contains the Apache License, Version 2.0 header, indicating the licensing terms under which the MediaPipe code is distributed. It specifies the conditions for use, reproduction, and distribution.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/python/text_classifier.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header - Python
DESCRIPTION: This snippet contains the standard Apache License 2.0 header, indicating the licensing terms under which the code is distributed. It specifies the conditions for use, reproduction, and distribution.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Header for MediaPipe Samples - Python
DESCRIPTION: This code block contains the standard Apache License, Version 2.0 header. It specifies the terms and conditions under which the MediaPipe code is licensed, including permissions for use, reproduction, and distribution, along with disclaimers of warranty.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Information for MediaPipe LLM Conversion - Python
DESCRIPTION: This snippet displays the Apache License, Version 2.0, under which the MediaPipe LLM conversion code is distributed. It outlines the terms and conditions for use, reproduction, and distribution of the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/conversion/llm_conversion.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title License information { display-mode: "form" }
#@markdown Copyright 2024 The MediaPipe Authors.
#@markdown Licensed under the Apache License, Version 2.0 (the "License");
#@markdown
#@markdown you may not use this file except in compliance with the License.
#@markdown You may obtain a copy of the License at
#@markdown
#@markdown https://www.apache.org/licenses/LICENSE-2.0
#@markdown
#@markdown Unless required by applicable law or agreed to in writing, software
#@markdown distributed under the License is distributed on an "AS IS" BASIS,
#@markdown WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#@markdown See the License for the specific language governing permissions and
#@markdown limitations under the License.
```

----------------------------------------

TITLE: License Information for MediaPipe Samples
DESCRIPTION: This snippet displays the Apache License, Version 2.0, under which the MediaPipe samples are distributed. It outlines the terms and conditions for use, reproduction, and distribution of the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/bundling/llm_bundling.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title License information { display-mode: "form" }
#@markdown Copyright 2024 The MediaPipe Authors.
#@markdown Licensed under the Apache License, Version 2.0 (the "License");
#@markdown
#@markdown you may not use this file except in compliance with the License.
#@markdown You may obtain a copy of the License at
#@markdown
#@markdown https://www.apache.org/licenses/LICENSE-2.0
#@markdown
#@markdown Unless required by applicable law or agreed to in writing, software
#@markdown distributed under the License is distributed on an "AS IS" BASIS,
#@markdown WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#@markdown See the License for the specific language governing permissions and
#@markdown limitations under the License.
```

----------------------------------------

TITLE: Apache 2.0 License Header (Python)
DESCRIPTION: This snippet provides the standard Apache License, Version 2.0 header, outlining the terms under which the code is distributed and used. It specifies the permissions, limitations, and conditions for using the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Header for MediaPipe Samples - Python
DESCRIPTION: This snippet contains the Apache License, Version 2.0 header, indicating the licensing terms under which the MediaPipe samples are distributed. It specifies the conditions for use, reproduction, and distribution of the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_landmarker/python/[MediaPipe_Python_Tasks]_Face_Landmarker.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Header for MediaPipe Samples - Python
DESCRIPTION: This snippet contains the Apache License, Version 2.0, which governs the use and distribution of the MediaPipe samples. It specifies the terms under which the code can be used, modified, and shared, ensuring compliance with open-source principles.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_embedder/python/text_embedder.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Information for MediaPipe Object Detector (Python)
DESCRIPTION: This snippet provides the Apache 2.0 license information for the MediaPipe Object Detector, outlining the terms under which the software can be used, modified, and distributed. It ensures compliance with open-source guidelines for the project.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title License information
# Copyright 2023 The MediaPipe Authors.
# Licensed under the Apache License, Version 2.0 (the "License");
#
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache 2.0 License Header
DESCRIPTION: This snippet contains the standard Apache License, Version 2.0 boilerplate, outlining the terms under which the MediaPipe code samples are distributed and used.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Information for MediaPipe Model Maker
DESCRIPTION: This snippet provides the Apache License, Version 2.0, for the MediaPipe Model Maker project, outlining the terms under which the software is distributed and used.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title License information
# Copyright 2023 The MediaPipe Authors.
# Licensed under the Apache License, Version 2.0 (the "License");
#
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Information for MediaPipe Model Maker
DESCRIPTION: This snippet provides the Apache License, Version 2.0, under which the MediaPipe Authors distribute their work. It outlines the terms and conditions for using, reproducing, and distributing the software components.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title License information
# Copyright 2023 The MediaPipe Authors.
# Licensed under the Apache License, Version 2.0 (the "License");
#
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header for MediaPipe Samples
DESCRIPTION: This snippet contains the standard Apache License, Version 2.0 header, outlining the terms under which the MediaPipe code is licensed. It specifies permissions, limitations, and disclaimers for use, reproduction, and distribution.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/language_detector/python/[MediaPipe_Python_Tasks]_Language_Detector.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header
DESCRIPTION: This snippet contains the standard Apache License 2.0 header, indicating the licensing terms for the code. It specifies that the code is licensed under Apache License, Version 2.0, and outlines the conditions for use, reproduction, and distribution.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_embedder/python/image_embedder.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache 2.0 License Header - Python
DESCRIPTION: This snippet contains the standard Apache License, Version 2.0 header, indicating the licensing terms under which the MediaPipe code is distributed. It specifies the conditions for use, reproduction, and distribution of the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header for MediaPipe
DESCRIPTION: This snippet provides the standard Apache License, Version 2.0 header, outlining the terms under which the code is licensed. It specifies the permissions and limitations for use, reproduction, and distribution of the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Information for MediaPipe Face Stylizer
DESCRIPTION: This snippet provides the Apache License, Version 2.0, for the MediaPipe project, outlining the terms under which the code can be used, modified, and distributed. It's a standard boilerplate for open-source projects.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title License information
# Copyright 2023 The MediaPipe Authors.
# Licensed under the Apache License, Version 2.0 (the "License");
#
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header for MediaPipe
DESCRIPTION: This code block contains the standard Apache 2.0 license header, which specifies the terms and conditions under which the MediaPipe code is distributed. It outlines the permissions, limitations, and disclaimers associated with using and modifying the software.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```