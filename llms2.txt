TITLE: Example Text Generation - LiteRT LLM Pipeline - Python
DESCRIPTION: This example shows how to use the initialized LiteRT LLM pipeline to generate text. It defines a prompt and then calls the 'generate' method on the pipeline, specifying a maximum number of decode steps to control the length of the generated output.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_29

LANGUAGE: python
CODE:
```
prompt = "What is the primary function of mitochondria within a cell"
output = pipeline.generate(prompt, max_decode_steps = 100)
```

----------------------------------------

TITLE: Performing Pose Landmark Detection and Visualization with MediaPipe Tasks
DESCRIPTION: This code block orchestrates the entire pose landmark detection process. It initializes a `PoseLandmarker` object using the pre-trained model, loads the input image, performs the detection, and then visualizes the results by drawing the detected landmarks onto the image using the `draw_landmarks_on_image` utility function.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/[MediaPipe_Python_Tasks]_Pose_Landmarker.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an PoseLandmarker object.
base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')
options = vision.PoseLandmarkerOptions(
    base_options=base_options,
    output_segmentation_masks=True)
detector = vision.PoseLandmarker.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file("image.jpg")

# STEP 4: Detect pose landmarks from the input image.
detection_result = detector.detect(image)

# STEP 5: Process the detection result. In this case, visualize it.
annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)
cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))
```

----------------------------------------

TITLE: Performing Face Detection and Visualization with MediaPipe Python
DESCRIPTION: This snippet demonstrates how to perform face detection using MediaPipe's Python API. It involves importing necessary modules, initializing a `FaceDetector` with a TFLite model (`detector.tflite`), loading an image from `IMAGE_FILE`, running the detection, and then visualizing the results using OpenCV functions like `cv2.cvtColor` and `cv2_imshow`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_detector/python/face_detector.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
import numpy as np
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an FaceDetector object.
base_options = python.BaseOptions(model_asset_path='detector.tflite')
options = vision.FaceDetectorOptions(base_options=base_options)
detector = vision.FaceDetector.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file(IMAGE_FILE)

# STEP 4: Detect faces in the input image.
detection_result = detector.detect(image)

# STEP 5: Process the detection result. In this case, visualize it.
image_copy = np.copy(image.numpy_view())
annotated_image = visualize(image_copy, detection_result)
rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
cv2_imshow(rgb_annotated_image)
```

----------------------------------------

TITLE: Performing Text Classification with MediaPipe Tasks in Python
DESCRIPTION: This code block demonstrates the complete workflow for classifying text using MediaPipe Tasks. It imports necessary modules, creates a `TextClassifier` instance from a TFLite model, performs classification on `INPUT_TEXT`, and then processes the result to print the top predicted category and its score.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_classification/python/text_classifier.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
from mediapipe.tasks import python
from mediapipe.tasks.python import text

# STEP 2: Create an TextClassifier object.
base_options = python.BaseOptions(model_asset_path="classifier.tflite")
options = text.TextClassifierOptions(base_options=base_options)
classifier = text.TextClassifier.create_from_options(options)

# STEP 3: Classify the input text.
classification_result = classifier.classify(INPUT_TEXT)

# STEP 4: Process the classification result. In this case, print out the most likely category.
top_category = classification_result.classifications[0].categories[0]
print(f'{top_category.category_name} ({top_category.score:.2f})')
```

----------------------------------------

TITLE: Performing Audio Classification with MediaPipe Tasks - Python
DESCRIPTION: This comprehensive snippet demonstrates end-to-end audio classification using MediaPipe Tasks. It initializes the `AudioClassifier` with the downloaded YAMNet model, reads the WAV audio data, segments it into clips, performs inference, and then iterates through the classification results to print the top category and its score for specific timestamps. It requires `numpy`, `scipy.io.wavfile`, and MediaPipe Tasks.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/audio_classifier/python/audio_classification.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np

from mediapipe.tasks import python
from mediapipe.tasks.python.components import containers
from mediapipe.tasks.python import audio
from scipy.io import wavfile

# Customize and associate model for Classifier
base_options = python.BaseOptions(model_asset_path='classifier.tflite')
options = audio.AudioClassifierOptions(
    base_options=base_options, max_results=4)

# Create classifier, segment audio clips, and classify
with audio.AudioClassifier.create_from_options(options) as classifier:
  sample_rate, wav_data = wavfile.read(audio_file_name)
  audio_clip = containers.AudioData.create_from_array(
      wav_data.astype(float) / np.iinfo(np.int16).max, sample_rate)
  classification_result_list = classifier.classify(audio_clip)

  assert(len(classification_result_list) == 5)

# Iterate through clips to display classifications
  for idx, timestamp in enumerate([0, 975, 1950, 2925]):
    classification_result = classification_result_list[idx]
    top_category = classification_result.classifications[0].categories[0]
    print(f'Timestamp {timestamp}: {top_category.category_name} ({top_category.score:.2f})')
```

----------------------------------------

TITLE: Initiating Supervised Fine-Tuning (SFT) - TRL & Transformers - Python
DESCRIPTION: This snippet initializes and starts the supervised fine-tuning process using `SFTTrainer` from the TRL library. It configures training arguments such as batch size, learning rate, and optimization strategy, and integrates the previously defined LoRA configuration. The `trainer.train()` method then commences the fine-tuning on the prepared dataset.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
import transformers
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,
    train_dataset = ds['train'],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=2,
        max_steps=150,
        #num_train_epochs=1,
        # Copied from other hugging face tuning blog posts
        learning_rate=2e-4,
        #fp16=True,
        bf16=True,
        # It makes training faster
        logging_steps=1,
        output_dir="outputs",
        optim="paged_adamw_8bit",
        report_to = "none",
    ),
    peft_config=lora_config,
)
trainer.train()
```

----------------------------------------

TITLE: Loading Gemma-3-1B Model and Tokenizer (Python)
DESCRIPTION: This snippet loads the `google/gemma-3-1b-pt` model and its corresponding tokenizer from HuggingFace. It configures the model for `bfloat16` precision and sets up a custom chat template for the tokenizer, ensuring proper formatting for conversational inputs and outputs. The HuggingFace token is used for authentication during model and tokenizer download.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
import os

import torch
from transformers import AutoTokenizer, BitsAndBytesConfig, GemmaTokenizer
from transformers.models.gemma3 import Gemma3ForCausalLM

model_id = 'google/gemma-3-1b-pt'
tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])
model = Gemma3ForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto", token=os.environ['HF_TOKEN'], attn_implementation='eager')
# Set up the chat format
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.chat_template = "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
```

----------------------------------------

TITLE: Initializing MediaPipe ImageClassifier - Python
DESCRIPTION: This code initializes an `ImageClassifier` object. It sets up `BaseOptions` by specifying the path to the TFLite model (`classifier.tflite`) and then configures `ImageClassifierOptions` to limit the classification results to a maximum of 4. The `classifier` object is then created using these options, making it ready for image classification.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_classification/python/image_classifier.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
base_options = python.BaseOptions(model_asset_path='classifier.tflite')
options = vision.ImageClassifierOptions(
    base_options=base_options, max_results=4)
classifier = vision.ImageClassifier.create_from_options(options)
```

----------------------------------------

TITLE: Initiating Object Detector Model Retraining (Python)
DESCRIPTION: This code initiates the retraining process for the object detection model using the `create()` method of `object_detector.ObjectDetector`. It takes the prepared training and validation datasets, along with the previously configured `options`, to start the resource-intensive training. The resulting `model` object represents the trained model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
model = object_detector.ObjectDetector.create(
    train_data=train_data,
    validation_data=validation_data,
    options=options)
```

----------------------------------------

TITLE: Loading and Tokenizing SFT Dataset - Hugging Face Datasets - Python
DESCRIPTION: This snippet loads a supervised fine-tuning (SFT) dataset from Hugging Face and prepares it for training. It defines a `tokenize_function` to format prompt-completion pairs into a chat template, then applies this function to the dataset in a batched manner, creating a 'text' column suitable for language model training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
from datasets import load_dataset

ds = load_dataset("argilla/synthetic-concise-reasoning-sft-filtered")
def tokenize_function(examples):
    # Process all examples in the batch
    prompts = examples["prompt"]
    completions = examples["completion"]
    texts = []
    for prompt, completion in zip(prompts, completions):
        text = tokenizer.apply_chat_template([{"role": "user", "content": prompt.strip()}, {"role": "assistant", "content": completion.strip()}], tokenize=False)
        texts.append(text)
    return { "text" : texts }  # Return a list of texts

ds = ds.map(tokenize_function, batched = True)
```

----------------------------------------

TITLE: Performing Text Embedding and Similarity Calculation with MediaPipe - Python
DESCRIPTION: This snippet demonstrates how to initialize the MediaPipe `TextEmbedder` with a downloaded model and then use it to generate embeddings for two text strings. It subsequently calculates and prints the cosine similarity between these embeddings, indicating how semantically similar the texts are. Key parameters include `l2_normalize` and `quantize` for embedding options.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/text_embedder/python/text_embedder.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from mediapipe.tasks import python
from mediapipe.tasks.python import text

# Create your base options with the model that was downloaded earlier
base_options = python.BaseOptions(model_asset_path='embedder.tflite')

# Set your values for using normalization and quantization
l2_normalize = True #@param {type:"boolean"}
quantize = False #@param {type:"boolean"}

# Create the final set of options for the Embedder
options = text.TextEmbedderOptions(
    base_options=base_options, l2_normalize=l2_normalize, quantize=quantize)

with text.TextEmbedder.create_from_options(options) as embedder:
  # Retrieve the first and second sets of text that will be compared
  first_text = "I'm feeling so good" #@param {type:"string"}
  second_text = "I'm okay I guess" #@param {type:"string"}

  # Convert both sets of text to embeddings
  first_embedding_result = embedder.embed(first_text)
  second_embedding_result = embedder.embed(second_text)

  # Calculate and print similarity
  similarity = text.TextEmbedder.cosine_similarity(
      first_embedding_result.embeddings[0],
      second_embedding_result.embeddings[0])
  print(similarity)
```

----------------------------------------

TITLE: Initializing LiteRT LLM Pipeline
DESCRIPTION: This snippet initializes a `LiteRTLlmPipeline` instance, which is essential for interacting with the loaded language model. It typically requires an `interpreter` for model execution and a `tokenizer` for text processing.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
# Disclaimer: Model performance demonstrated with the Python API in this notebook is not representative of performance on a local device.
pipeline = LiteRTLlmPipeline(interpreter, tokenizer)
```

----------------------------------------

TITLE: Initializing LiteRT LLM Pipeline - Python
DESCRIPTION: This line initializes an instance of the `LiteRTLlmPipeline` class, which is used to manage the LLM inference process. It requires an `interpreter` (likely a TFLite interpreter) and a `tokenizer` object to handle model execution and text tokenization, respectively. This pipeline object will then be used for text generation.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
pipeline = LiteRTLlmPipeline(interpreter, tokenizer)
```

----------------------------------------

TITLE: Converting Gemma Model to LiteRT Format (Python)
DESCRIPTION: This comprehensive script converts a PyTorch Gemma 1B model to the LiteRT format, including 8-bit quantization. It defines conversion flags, creates attention masks for prefill and decode stages, and uses `ai_edge_torch.generative.utilities.converter` to export the model as a TFLite file with specified quantization and KV cache settings.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
from absl import flags
from absl import app
import sys
import torch

from ai_edge_torch.generative.examples.gemma3 import gemma3
from ai_edge_torch.generative.layers import kv_cache
from ai_edge_torch.generative.utilities import converter
from ai_edge_torch.generative.utilities.export_config import ExportConfig


flags = converter.define_conversion_flags('gemma3-1b')
flags.FLAGS.mask_as_input = True
flags.FLAGS.prefill_seq_lens = [128]
flags.FLAGS.kv_cache_max_len = 1024

def _create_mask(mask_len, kv_cache_max_len):
  mask = torch.full(
      (mask_len, kv_cache_max_len), float('-inf'), dtype=torch.float32
  )
  mask = torch.triu(mask, diagonal=1).unsqueeze(0).unsqueeze(0)
  return mask


def _create_export_config(
    prefill_seq_lens: list[int], kv_cache_max_len: int
) -> ExportConfig:
  """Creates the export config for the model."""
  export_config = ExportConfig()
  if isinstance(prefill_seq_lens, list):
    prefill_mask = [_create_mask(i, kv_cache_max_len) for i in prefill_seq_lens]
  else:
    prefill_mask = _create_mask(prefill_seq_lens, kv_cache_max_len)

  export_config.prefill_mask = prefill_mask

  decode_mask = torch.full(
      (1, kv_cache_max_len), float('-inf'), dtype=torch.float32
  )
  decode_mask = torch.triu(decode_mask, diagonal=1).unsqueeze(0).unsqueeze(0)
  export_config.decode_mask = decode_mask
  export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED
  return export_config


def convert_to_litert(_):
  with torch.inference_mode(True):
    pytorch_model = gemma3.build_model_1b(
      "/content/merged_model", kv_cache_max_len=flags.FLAGS.kv_cache_max_len,
    )
    converter.convert_to_tflite(
        pytorch_model,
        output_path="/content/",
        output_name_prefix="gemma3_1b_finetune",
        prefill_seq_len=flags.FLAGS.prefill_seq_lens,
        quantize=converter.QuantizationName.DYNAMIC_INT4_BLOCK32,
        lora_ranks=None,
        export_config=_create_export_config(
            prefill_seq_lens=flags.FLAGS.prefill_seq_lens,
            kv_cache_max_len=flags.FLAGS.kv_cache_max_len,
        ),
    )

# Ignore flags passed from the colab runtime.
sys.argv = sys.argv[:1]
app.run(convert_to_litert)
```

----------------------------------------

TITLE: Generating Text with LiteRT LLM Pipeline
DESCRIPTION: This code demonstrates how to use the loaded LiteRT model to generate text. It takes a `prompt` as input and uses the `runner.generate` method to produce an `output`, with an option to specify maximum decoding steps.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
prompt = "What is the capital of France?"
output = runner.generate(prompt, max_decode_steps=None)
```

----------------------------------------

TITLE: Exporting Retrained MediaPipe Model to TensorFlow Lite (Python)
DESCRIPTION: This command exports the retrained MediaPipe model to the TensorFlow Lite format, which is necessary for deployment in applications. The export process also generates required model metadata and a classification label file.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
model.export_model()
```

----------------------------------------

TITLE: Exporting and Downloading TensorFlow Lite Model
DESCRIPTION: This snippet exports the trained object detection model to a TensorFlow Lite ('.tflite') format, lists the contents of the 'exported_model' directory to confirm the file's presence, and then initiates the download of the 'dogs.tflite' model file for on-device application use.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
model.export_model('dogs.tflite')
!ls exported_model
files.download('exported_model/dogs.tflite')
```

----------------------------------------

TITLE: Evaluating Object Detector Model Performance (Python)
DESCRIPTION: This snippet evaluates the performance of the retrained object detection model using the `evaluate()` method on the validation dataset. It calculates and prints the validation loss and COCO metrics, with 'AP' (Average Precision) being a key indicator of model performance. This step helps assess the model's accuracy and generalization capabilities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
loss, coco_metrics = model.evaluate(validation_data, batch_size=4)
print(f"Validation loss: {loss}")
print(f"Validation coco metrics: {coco_metrics}")
```

----------------------------------------

TITLE: Evaluating a Trained Image Classifier Model (Python)
DESCRIPTION: This code snippet shows how to evaluate the performance of a previously trained `ImageClassifier` model (`model_2`) using a `test_data` dataset. It returns the `loss` and `accuracy` metrics, providing insights into the model's generalization capabilities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
loss, accuracy = model_2.evaluate(test_data)
```

----------------------------------------

TITLE: Generating Text with Prefill and Decode - LiteRT LLM Pipeline - Python
DESCRIPTION: This is the main entry point for text generation, orchestrating the entire process. It tokenizes the input prompt, initializes the prefill runner, executes the prefill stage to get the initial KV cache, and then calls the decode method to generate the remaining text based on the prefilled context.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_27

LANGUAGE: python
CODE:
```
  def generate(self, prompt: str, max_decode_steps: int | None = None) -> str:
    messages=[{ 'role': 'user', 'content': prompt}]
    token_ids = self._tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
    # Initialize the prefill runner with the suitable input size.
    self._init_prefill_runner(len(token_ids))

    # Run prefill.
    # Prefill up to the seond to the last token of the prompt, because the last
    # token of the prompt will be used to bootstrap decode.
    prefill_token_length = len(token_ids) - 1

    print('Running prefill')
    kv_cache = self._run_prefill(token_ids[:prefill_token_length])
    # Run decode.
    print('Running decode')
    actual_max_decode_steps = self._max_kv_cache_seq_len - prefill_token_length - 1
    if max_decode_steps is not None:
      actual_max_decode_steps = min(actual_max_decode_steps, max_decode_steps)
    decode_text = self._run_decode(
        prefill_token_length,
        token_ids[prefill_token_length],
        kv_cache,
        actual_max_decode_steps,
    )
    return decode_text
```

----------------------------------------

TITLE: Training Image Classifier with Custom HParams and Model Options (Python)
DESCRIPTION: This snippet demonstrates how to train an `ImageClassifier` model with custom hyperparameters and model options. It sets the number of training epochs to 15 and a dropout rate of 0.07, overriding the default values to potentially improve model performance. It requires `train_data`, `validation_data`, and a `spec` for the supported model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
hparams=image_classifier.HParams(epochs=15, export_dir="exported_model_2")
options = image_classifier.ImageClassifierOptions(supported_model=spec, hparams=hparams)
options.model_options = image_classifier.ModelOptions(dropout_rate = 0.07)
model_2 = image_classifier.ImageClassifier.create(
    train_data = train_data,
    validation_data = validation_data,
    options=options,
)
```

----------------------------------------

TITLE: Invoking LLM Text Generation - Python
DESCRIPTION: This snippet demonstrates how to use the initialized `LiteRTLlmPipeline` to generate text. It sets a `prompt` string and then calls the `generate` method on the `pipeline` object, passing the prompt and setting `max_decode_steps` to `None` for potentially unlimited decoding. The generated text is stored in the `output` variable.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
prompt = "what is 8 mod 6"
output = pipeline.generate(prompt, max_decode_steps = None)
```

----------------------------------------

TITLE: Testing Fine-Tuned Model Inference - Hugging Face Transformers - Python
DESCRIPTION: This snippet demonstrates how to perform text generation inference using the fine-tuned model. It re-initializes a text generation pipeline with the updated model and tokenizer, then generates a response to a given prompt, showcasing the model's improved capabilities after fine-tuning.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
from transformers import pipeline
# Let's test the base model before training
prompt = "What is the primary function of mitochondria within a cell?"
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
pipe(prompt, max_new_tokens=100)
```

----------------------------------------

TITLE: Exporting Quantized MediaPipe Model (Python)
DESCRIPTION: Exports the trained model to a TensorFlow Lite (`.tflite`) file, applying the specified post-training quantization. The `model_name` parameter sets the output filename, and the `quantization_config` object dictates the quantization strategy, such as `int8` quantization.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_16

LANGUAGE: Python
CODE:
```
model.export_model(model_name="model_int8.tflite", quantization_config=quantization_config)
```

----------------------------------------

TITLE: Exporting Quantized BERT Text Classifier and Labels (Python)
DESCRIPTION: This snippet exports the BERT-based text classifier as a TFLite model with dynamic range quantization to reduce its size. It imports `quantization`, creates a `QuantizationConfig`, and then uses `bert_model.export_model()` with this configuration, also exporting the labels.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
from mediapipe_model_maker import quantization
quantization_config = quantization.QuantizationConfig.for_dynamic()
bert_model.export_model(quantization_config=quantization_config)
bert_model.export_labels(export_dir=options.hparams.export_dir)
```

----------------------------------------

TITLE: Exporting Retrained Face Stylizer Model (Python)
DESCRIPTION: This snippet exports the retrained face stylizer model to the TensorFlow Lite format, which is essential for integrating it into applications using MediaPipe. The export process automatically generates all required model metadata and a classification label file, preparing the model for deployment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
face_stylizer_model.export_model()
```

----------------------------------------

TITLE: Running Image Classifier Retraining with MediaPipe (Python)
DESCRIPTION: This code initiates the image classifier retraining process using the `create()` method of `ImageClassifier`. It requires prepared training and validation datasets, along with the previously defined retraining options. This process can be resource-intensive and its duration depends on compute resources.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/image_classifier.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
model = image_classifier.ImageClassifier.create(
    train_data = train_data,
    validation_data = validation_data,
    options=options,
)
```

----------------------------------------

TITLE: Installing MediaPipe Library - Python
DESCRIPTION: This command installs the MediaPipe library using pip, Python's package installer. The '-q' flag ensures a quiet installation, suppressing detailed output, which is useful for cleaner notebook execution. This step is a prerequisite for utilizing any MediaPipe functionalities.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install -q mediapipe
```

----------------------------------------

TITLE: Installing MediaPipe Model Maker Package
DESCRIPTION: This code block installs and upgrades the `pip` package manager, then installs the `mediapipe-model-maker` library, which is essential for customizing on-device ML models.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip install --upgrade pip
!pip install mediapipe-model-maker
```

----------------------------------------

TITLE: Installing MediaPipe Model Maker Library
DESCRIPTION: This command installs the `mediapipe-model-maker` Python package, which is essential for customizing on-device machine learning models, including the face stylizer. It's a prerequisite for running the subsequent model customization steps.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/face_stylizer.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install mediapipe-model-maker
```

----------------------------------------

TITLE: Running MediaPipe Gesture Recognition Inference (Python)
DESCRIPTION: This snippet demonstrates the core steps for performing gesture recognition using MediaPipe. It initializes a `GestureRecognizer` with a specified model, loads input images, performs recognition, and extracts the top gesture and hand landmarks from the results. It requires the `mediapipe` library and a `gesture_recognizer.task` model file. The final line `display_batch_of_images_with_gestures_and_hand_landmarks` implies a visualization function not included in the snippet.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an GestureRecognizer object.
base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')
options = vision.GestureRecognizerOptions(base_options=base_options)
recognizer = vision.GestureRecognizer.create_from_options(options)

images = []
results = []
for image_file_name in IMAGE_FILENAMES:
  # STEP 3: Load the input image.
  image = mp.Image.create_from_file(image_file_name)

  # STEP 4: Recognize gestures in the input image.
  recognition_result = recognizer.recognize(image)

  # STEP 5: Process the result. In this case, visualize it.
  images.append(image)
  top_gesture = recognition_result.gestures[0][0]
  hand_landmarks = recognition_result.hand_landmarks
  results.append((top_gesture, hand_landmarks))

display_batch_of_images_with_gestures_and_hand_landmarks(images, results)
```

----------------------------------------

TITLE: Applying Background Blur with MediaPipe ImageSegmenter in Python
DESCRIPTION: This snippet demonstrates how to initialize a MediaPipe ImageSegmenter, load images, perform segmentation to obtain category masks, convert image color spaces using OpenCV, apply a Gaussian blur to the background, and combine the original and blurred images based on the segmentation mask. It requires MediaPipe, OpenCV (cv2), and NumPy (np) libraries.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
# Create the segmenter
with python.vision.ImageSegmenter.create_from_options(options) as segmenter:

  # Loop through available image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe Image
    image = mp.Image.create_from_file(image_file_name)

    # Retrieve the category masks for the image
    segmentation_result = segmenter.segment(image)
    category_mask = segmentation_result.category_mask

    # Convert the BGR image to RGB
    image_data = cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB)

    # Apply effects
    blurred_image = cv2.GaussianBlur(image_data, (55,55), 0)
    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1
    output_image = np.where(condition, image_data, blurred_image)

    print(f'Blurred background of {image_file_name}:')
    resize_and_show(output_image)
```

----------------------------------------

TITLE: Loading Object Detection Datasets from Pascal VOC
DESCRIPTION: This snippet loads the training and validation datasets for object detection using 'object_detector.Dataset.from_pascal_voc_folder'. It specifies the folder paths for 'dogs copy/train' and 'dogs copy/validate' and sets cache directories for efficient data handling during model training.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
train_data = object_detector.Dataset.from_pascal_voc_folder(
    'dogs copy/train',
    cache_dir="/tmp/od_data/train",
)

val_data = object_detector.Dataset.from_pascal_voc_folder(
    'dogs copy/validate',
    cache_dir="/tmp/od_data/validatation")
```

----------------------------------------

TITLE: Training MediaPipe Object Detector Model
DESCRIPTION: This snippet configures and trains an object detection model using MediaPipe Model Maker. It sets hyperparameters like 'batch_size=8', 'learning_rate=0.3', and 'epochs=50', specifies 'MOBILENET_V2' as the supported model, and then creates and trains the 'ObjectDetector' using the prepared training and validation data.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/object_detection/Object_Detection_for_3_dogs.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
hparams = object_detector.HParams(batch_size=8, learning_rate=0.3, epochs=50, export_dir='exported_model')
options = object_detector.ObjectDetectorOptions(
    supported_model=object_detector.SupportedModels.MOBILENET_V2,
    hparams=hparams
)
model = object_detector.ObjectDetector.create(
    train_data=train_data,
    validation_data=val_data,
    options=options)
```

----------------------------------------

TITLE: Training BERT Text Classifier (Python)
DESCRIPTION: This snippet initiates the training of the BERT-based text classifier using `TextClassifier.create()`. It utilizes the `train_data`, `validation_data`, and the `options` configured for the MobileBERT model. Note that this process can be computationally intensive.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/text_classifier.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
bert_model = text_classifier.TextClassifier.create(train_data, validation_data, options)
```

----------------------------------------

TITLE: Exporting Object Detector Model to TensorFlow Lite (Python)
DESCRIPTION: This code exports the trained object detection model to the TensorFlow Lite format, including essential metadata like the label map. It first calls `export_model()` to perform the conversion, then lists the contents of the `exported_model` directory, and finally downloads the generated `model.tflite` file. This prepares the model for on-device deployment.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
model.export_model()
!ls exported_model
files.download('exported_model/model.tflite')
```

----------------------------------------

TITLE: Performing Language Detection Inference with MediaPipe Python API
DESCRIPTION: This comprehensive snippet demonstrates the full workflow for language detection using MediaPipe. It imports required modules, initializes the LanguageDetector with the downloaded model, performs detection on INPUT_TEXT, and then iterates through the results to print detected languages and their confidence scores. It outlines the core steps for using the MediaPipe Text API.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/language_detector/python/[MediaPipe_Python_Tasks]_Language_Detector.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
from mediapipe.tasks import python
from mediapipe.tasks.python import text

# STEP 2: Create a LanguageDetector object.
base_options = python.BaseOptions(model_asset_path="detector.tflite")
options = text.LanguageDetectorOptions(base_options=base_options)
detector = text.LanguageDetector.create_from_options(options)

# STEP 3: Get the language detcetion result for the input text.
detection_result = detector.detect(INPUT_TEXT)

# STEP 4: Process the detection result and print the languages detected and
# their scores.

for detection in detection_result.detections:
  print(f'{detection.language_code}: ({detection.probability:.2f})')
```

----------------------------------------

TITLE: Generating Text with LLM Pipeline - Python
DESCRIPTION: This method orchestrates the text generation process for an LLM. It first tokenizes the input prompt, then performs a prefill step to process the initial prompt and generate a KV cache. Finally, it calls the `_run_decode` method to generate the response tokens, returning the complete decoded text. It takes a `prompt` string and an optional `max_decode_steps` integer.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma2_tflite.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
def generate(self, prompt: str, max_decode_steps: int | None = None) -> str:
    messages=[{ 'role': 'user', 'content': prompt}]
    token_ids = self._tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
    # Initialize the prefill runner with the suitable input size.
    self._init_prefill_runner(len(token_ids))

    # Run prefill.
    # Prefill up to the seond to the last token of the prompt, because the last
    # token of the prompt will be used to bootstrap decode.
    prefill_token_length = len(token_ids) - 1

    print('Running prefill')
    kv_cache = self._run_prefill(token_ids[:prefill_token_length])
    # Run decode.
    print('Running decode')
    actual_max_decode_steps = self._max_kv_cache_seq_len - prefill_token_length - 1
    if max_decode_steps is not None:
      actual_max_decode_steps = min(actual_max_decode_steps, max_decode_steps)
    decode_text = self._run_decode(
        prefill_token_length,
        token_ids[prefill_token_length],
        kv_cache,
        actual_max_decode_steps,
    )
    return decode_text
```

----------------------------------------

TITLE: Creating a MediaPipe Task Bundle for a TFLite Model
DESCRIPTION: This Python snippet demonstrates how to create a task bundle for a converted TFLite model using the `mediapipe.tasks.python.genai.bundler`. It configures the `BundleConfig` with paths to the TFLite model and tokenizer, start/stop tokens, output filename, and an option for bytes-to-unicode mapping, then calls `create_bundle`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/llm_inference/bundling/llm_bundling.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
tflite_model="PATH/gemma.tflite" # @param {type:"string"}
tokenizer_model="PATH/tokenizer.model" # @param {type:"string"}
start_token="<bos>" # @param {type:"string"}
stop_token="<eos>" # @param {type:"string"}
output_filename="PATH/gemma.task" # @param {type:"string"}
enable_bytes_to_unicode_mapping=False # @param ["False", "True"] {type:"raw"}

config = bundler.BundleConfig(
    tflite_model=tflite_model,
    tokenizer_model=tokenizer_model,
    start_token=start_token,
    stop_tokens=[stop_token],
    output_filename=output_filename,
    enable_bytes_to_unicode_mapping=enable_bytes_to_unicode_mapping,
)
bundler.create_bundle(config)
```

----------------------------------------

TITLE: Performing Object Detection Inference with MediaPipe - Python
DESCRIPTION: This code demonstrates the complete process of running object detection using MediaPipe. It initializes an `ObjectDetector` with a pre-trained model, loads an image, performs the detection, and then visualizes the results by drawing bounding boxes and labels on a copy of the original image, finally displaying the annotated image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/object_detection/python/object_detector.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# STEP 1: Import the necessary modules.
import numpy as np
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an ObjectDetector object.
base_options = python.BaseOptions(model_asset_path='efficientdet.tflite')
options = vision.ObjectDetectorOptions(base_options=base_options,
                                       score_threshold=0.5)
detector = vision.ObjectDetector.create_from_options(options)

# STEP 3: Load the input image.
image = mp.Image.create_from_file(IMAGE_FILE)

# STEP 4: Detect objects in the input image.
detection_result = detector.detect(image)

# STEP 5: Process the detection result. In this case, visualize it.
image_copy = np.copy(image.numpy_view())
annotated_image = visualize(image_copy, detection_result)
rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
cv2_imshow(rgb_annotated_image)
```

----------------------------------------

TITLE: Performing Image Embedding and Cosine Similarity (MediaPipe Python)
DESCRIPTION: This snippet demonstrates how to initialize the MediaPipe `ImageEmbedder` with a downloaded model and custom options (L2 normalization, quantization). It then uses the embedder to generate embeddings for two images and calculates their cosine similarity, which indicates how alike the images are.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_embedder/python/image_embedder.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# Create options for Image Embedder
base_options = python.BaseOptions(model_asset_path='embedder.tflite')
l2_normalize = True #@param {type:"boolean"}
quantize = True #@param {type:"boolean"}
options = vision.ImageEmbedderOptions(
    base_options=base_options, l2_normalize=l2_normalize, quantize=quantize)


# Create Image Embedder
with vision.ImageEmbedder.create_from_options(options) as embedder:

  # Format images for MediaPipe
  first_image = mp.Image.create_from_file(IMAGE_FILENAMES[0])
  second_image = mp.Image.create_from_file(IMAGE_FILENAMES[1])
  first_embedding_result = embedder.embed(first_image)
  second_embedding_result = embedder.embed(second_image)

  # Calculate and print similarity
  similarity = vision.ImageEmbedder.cosine_similarity(
      first_embedding_result.embeddings[0],
      second_embedding_result.embeddings[0])
  print(similarity)
```

----------------------------------------

TITLE: Evaluating MediaPipe Gesture Recognizer Model Performance (Python)
DESCRIPTION: This snippet evaluates the performance of the trained gesture recognizer model using the `test_data` and a specified `batch_size`. It returns the loss and accuracy metrics, which are then printed to the console, providing an assessment of the model's generalization ability.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
loss, acc = model.evaluate(test_data, batch_size=1)
print(f"Test loss:{loss}, Test accuracy:{acc}")
```

----------------------------------------

TITLE: Running Face Stylizer Inference - Python
DESCRIPTION: This code initializes the MediaPipe `FaceStylizer` using the downloaded model and applies the pre-trained style to the input image. It demonstrates how to perform inference using the `stylize` method and then displays the resulting stylized image.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/face_stylizer/python/face_stylizer.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
import numpy as np
import mediapipe as mp

from mediapipe.tasks import python
from mediapipe.tasks.python import vision


# Create the options that will be used for FaceStylizer
base_options = python.BaseOptions(model_asset_path='face_stylizer.task')
options = vision.FaceStylizerOptions(base_options=base_options)

# Create the face stylizer
with vision.FaceStylizer.create_from_options(options) as stylizer:

  # Loop through demo image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe image file that will be stylized
    image = mp.Image.create_from_file(image_file_name)
    # Retrieve the stylized image
    stylized_image = stylizer.stylize(image)

    # Show the stylized image
    rgb_stylized_image = cv2.cvtColor(stylized_image.numpy_view(), cv2.COLOR_BGR2RGB)
    resize_and_show(rgb_stylized_image)
```

----------------------------------------

TITLE: Merging LoRA Weights into Base Model - PEFT - Python
DESCRIPTION: This snippet loads the fine-tuned PEFT model and merges its LoRA adapter weights back into the base model. This process creates a single, consolidated model that can be deployed without requiring the separate PEFT adapter, making it suitable for on-device inference or other deployment scenarios.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
from peft import AutoPeftModelForCausalLM
import torch

# Load PEFT model on CPU
model = AutoPeftModelForCausalLM.from_pretrained("gemma3-1b-sft")
# Merge LoRA and base model and save
merged_model = model.merge_and_unload()
```

----------------------------------------

TITLE: Building MediaPipe LLM Task Bundle for Gemma (Python)
DESCRIPTION: This function `build_gemma3_1b_it_q8` creates a MediaPipe LLM task bundle (`.task` file) by configuring the TFLite model, tokenizer, and specific LLM parameters like start/stop tokens and prompt prefixes/suffixes. It uses `llm_bundler.create_bundle` to generate the final deployable task file, which is then saved to `/content/gemma3_1b_it_q8_ekv1280.task`.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/Gemma3_1b_fine_tune.ipynb#_snippet_31

LANGUAGE: python
CODE:
```
from mediapipe.tasks.python.genai.bundler import llm_bundler

def build_gemma3_1b_it_q8():
  output_file = "/content/gemma3_1b_it_q8_ekv1280.task"
  tflite_model = "/content/gemma3_1b_finetune_q8_ekv1024.tflite"
  tokenizer_model = (
      "/content/tokenizer.model"
  )
  config = llm_bundler.BundleConfig(
      tflite_model=tflite_model,
      tokenizer_model=tokenizer_model,
      start_token="<bos>",
      stop_tokens=["<eos>"],
      output_filename=output_file,
      enable_bytes_to_unicode_mapping=False,
      prompt_prefix="<start_of_turn>user\n",
      prompt_suffix="<end_of_turn>\n<start_of_turn>model\n"
  )
  llm_bundler.create_bundle(config)

# Build the MediaPipe task bundle.
build_gemma3_1b_it_q8()
```

----------------------------------------

TITLE: Performing Image Segmentation and Mask Visualization with MediaPipe
DESCRIPTION: This code initializes the MediaPipe `ImageSegmenter` using the downloaded DeepLab v3 model. It then processes the test image to generate a category mask, which is used to create a visual representation where the foreground and background are highlighted with distinct colors (white and gray, respectively). It depends on `numpy`, `mediapipe`, and the `resize_and_show` function.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/image_segmentation/python/image_segmentation.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np
import mediapipe as mp

from mediapipe.tasks import python
from mediapipe.tasks.python import vision


BG_COLOR = (192, 192, 192) # gray
MASK_COLOR = (255, 255, 255) # white


# Create the options that will be used for ImageSegmenter
base_options = python.BaseOptions(model_asset_path='deeplabv3.tflite')
options = vision.ImageSegmenterOptions(base_options=base_options,
                                       output_category_mask=True)

# Create the image segmenter
with vision.ImageSegmenter.create_from_options(options) as segmenter:

  # Loop through demo image(s)
  for image_file_name in IMAGE_FILENAMES:

    # Create the MediaPipe image file that will be segmented
    image = mp.Image.create_from_file(image_file_name);

    # Retrieve the masks for the segmented image
    segmentation_result = segmenter.segment(image);
    category_mask = segmentation_result.category_mask;

    # Generate solid color images for showing the output segmentation mask.
    image_data = image.numpy_view();
    fg_image = np.zeros(image_data.shape, dtype=np.uint8);
    fg_image[:] = MASK_COLOR;
    bg_image = np.zeros(image_data.shape, dtype=np.uint8);
    bg_image[:] = BG_COLOR;

    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.2;
    output_image = np.where(condition, fg_image, bg_image);

    print(f'Segmentation mask of {name}:');
    resize_and_show(output_image)
```

----------------------------------------

TITLE: PASCAL VOC Dataset Directory and XML Structure
DESCRIPTION: This snippet describes the PASCAL VOC dataset format, featuring a `data` directory for images and an `Annotations` directory for per-image XML annotation files. It also provides the XML schema for these annotation files, detailing filename, object names, and bounding box coordinates.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/object_detector.ipynb#_snippet_5

LANGUAGE: Text
CODE:
```
<dataset_dir>/
  data/
    <file0>.<jpg/jpeg>
    ...
  Annotations/
    <file0>.xml
    ...
```

LANGUAGE: XML
CODE:
```
<annotation>
  <filename>file0.jpg</filename>
  <object>
    <name>kangaroo</name>
    <bndbox>
      <xmin>233</xmin>
      <ymin>89</ymin>
      <xmax>386</xmax>
      <ymax>262</ymax>
    </bndbox>
  </object>
  <object>
    ...
  </object>
  ...
</annotation>
```

----------------------------------------

TITLE: Training MediaPipe Gesture Recognizer with Custom Hyperparameters (Python)
DESCRIPTION: This snippet demonstrates how to train a new gesture recognizer model with customized hyperparameters. It sets a specific `learning_rate` and `export_dir` via `HParams`, and a `dropout_rate` via `ModelOptions`, then combines them into `GestureRecognizerOptions` before creating and training the model.
SOURCE: https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/customization/gesture_recognizer.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir="exported_model_2")
model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)
options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)
model_2 = gesture_recognizer.GestureRecognizer.create(
    train_data=train_data,
    validation_data=validation_data,
    options=options
)
```