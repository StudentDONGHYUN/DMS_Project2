TITLE: Creating and Activating Python Virtual Environment
DESCRIPTION: This command creates a new Python virtual environment named `mp_env` and then activates it. This practice isolates project dependencies, preventing conflicts with other Python projects or system-wide installations.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#_snippet_0

LANGUAGE: Bash
CODE:
```
$ python3 -m venv mp_env && source mp_env/bin/activate
```

----------------------------------------

TITLE: Cloning MediaPipe Repository (Bash)
DESCRIPTION: This command sequence clones the MediaPipe GitHub repository and navigates into its root directory, which is the first step for setting up the MediaPipe development environment.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/google/mediapipe.git
cd mediapipe
```

----------------------------------------

TITLE: Adding MediaPipe Android Solution APIs to Gradle Dependencies
DESCRIPTION: This snippet demonstrates how to add MediaPipe Android Solution APIs to an Android Studio project's `build.gradle` file. It includes the core solution library and optional libraries for Face Detection, Face Mesh, and Hands, allowing developers to easily incorporate these functionalities into their Android applications. These dependencies are fetched from Google's Maven Repository.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/android_solutions.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
dependencies {
    // MediaPipe solution-core is the foundation of any MediaPipe Solutions.
    implementation 'com.google.mediapipe:solution-core:latest.release'
    // Optional: MediaPipe Face Detection Solution.
    implementation 'com.google.mediapipe:facedetection:latest.release'
    // Optional: MediaPipe Face Mesh Solution.
    implementation 'com.google.mediapipe:facemesh:latest.release'
    // Optional: MediaPipe Hands Solution.
    implementation 'com.google.mediapipe:hands:latest.release'
}
```

----------------------------------------

TITLE: Cloning MediaPipe Repository and Navigating Directory
DESCRIPTION: These commands clone the MediaPipe GitHub repository with a depth of 1 (shallow clone) and then change the current directory into the newly cloned 'mediapipe' folder, preparing for further setup.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#_snippet_34

LANGUAGE: bash
CODE:
```
git clone --depth 1 https://github.com/google/mediapipe.git

cd mediapipe
```

----------------------------------------

TITLE: Detecting Faces with MediaPipe Face Detector (JavaScript)
DESCRIPTION: This snippet initializes the MediaPipe Face Detector task by loading the necessary WASM files and a pre-trained model. It then performs face detection on an HTML image element, returning the detected face locations and presence. Requires the @mediapipe/tasks-vision library.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#_snippet_0

LANGUAGE: JavaScript
CODE:
```
const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm"
);
const faceDetector = await FaceDetector.createFromModelPath(vision,
    "https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
);
const image = document.getElementById("image") as HTMLImageElement;
const detections = faceDetector.detect(image);
```

----------------------------------------

TITLE: Declaring Camera Permissions in AndroidManifest.xml
DESCRIPTION: This XML snippet declares the necessary permissions and features in `AndroidManifest.xml` to allow the application to access and use the device's camera. It requests `android.permission.CAMERA` and specifies the use of `android.hardware.camera`.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#_snippet_9

LANGUAGE: xml
CODE:
```
<!-- For using the camera -->
<uses-permission android:name="android.permission.CAMERA" />
<uses-feature android:name="android.hardware.camera" />
```

----------------------------------------

TITLE: Initializing and Using MediaPipe LLM Inference in JavaScript
DESCRIPTION: This snippet demonstrates how to initialize the MediaPipe LLM Inference task and generate a text response. It requires a FilesetResolver to load the WASM module from a CDN and a pre-trained LLM model, specified by MODEL_URL, to perform inference on the inputText.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/genai/README.md#_snippet_0

LANGUAGE: JavaScript
CODE:
```
const genai = await FilesetResolver.forGenAiTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai/wasm"
);
const llmInference = await LlmInference.createFromModelPath(genai, MODEL_URL);
const response = await llmInference.generateResponse(inputText);
```

----------------------------------------

TITLE: Cloning MediaPipe Repository (Bash)
DESCRIPTION: This command sequence clones the MediaPipe GitHub repository to the local machine and then changes the current directory into the newly cloned `mediapipe` directory. It is the initial step required to set up the development environment for MediaPipe projects.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/media_sequence.md#_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/google/mediapipe.git
cd mediapipe
```

----------------------------------------

TITLE: Initializing and Processing with MediaPipe Holistic in JavaScript
DESCRIPTION: This JavaScript code initializes the MediaPipe Holistic model, configures its options, and sets up a camera to feed video frames for processing. The `onResults` function handles the output, drawing segmentation masks, landmarks for pose, face, and hands onto a canvas.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md#_snippet_3

LANGUAGE: JavaScript
CODE:
```
<script type="module">
const videoElement = document.getElementsByClassName('input_video')[0];
const canvasElement = document.getElementsByClassName('output_canvas')[0];
const canvasCtx = canvasElement.getContext('2d');

function onResults(results) {
  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  canvasCtx.drawImage(results.segmentationMask, 0, 0,
                      canvasElement.width, canvasElement.height);

  // Only overwrite existing pixels.
  canvasCtx.globalCompositeOperation = 'source-in';
  canvasCtx.fillStyle = '#00FF00';
  canvasCtx.fillRect(0, 0, canvasElement.width, canvasElement.height);

  // Only overwrite missing pixels.
  canvasCtx.globalCompositeOperation = 'destination-atop';
  canvasCtx.drawImage(
      results.image, 0, 0, canvasElement.width, canvasElement.height);

  canvasCtx.globalCompositeOperation = 'source-over';
  drawConnectors(canvasCtx, results.poseLandmarks, POSE_CONNECTIONS,
                 {color: '#00FF00', lineWidth: 4});
  drawLandmarks(canvasCtx, results.poseLandmarks,
                {color: '#FF0000', lineWidth: 2});
  drawConnectors(canvasCtx, results.faceLandmarks, FACEMESH_TESSELATION,
                 {color: '#C0C0C070', lineWidth: 1});
  drawConnectors(canvasCtx, results.leftHandLandmarks, HAND_CONNECTIONS,
                 {color: '#CC0000', lineWidth: 5});
  drawLandmarks(canvasCtx, results.leftHandLandmarks,
                {color: '#00FF00', lineWidth: 2});
  drawConnectors(canvasCtx, results.rightHandLandmarks, HAND_CONNECTIONS,
                 {color: '#00CC00', lineWidth: 5});
  drawLandmarks(canvasCtx, results.rightHandLandmarks,
                {color: '#FF0000', lineWidth: 2});
  canvasCtx.restore();
}

const holistic = new Holistic({locateFile: (file) => {
  return `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`;
}});
holistic.setOptions({
  modelComplexity: 1,
  smoothLandmarks: true,
  enableSegmentation: true,
  smoothSegmentation: true,
  refineFaceLandmarks: true,
  minDetectionConfidence: 0.5,
  minTrackingConfidence: 0.5
});
holistic.onResults(onResults);

const camera = new Camera(videoElement, {
  onFrame: async () => {
    await holistic.send({image: videoElement});
  },
  width: 1280,
  height: 720
});
camera.start();
</script>
```

----------------------------------------

TITLE: Processing Webcam Input with MediaPipe Pose (Python)
DESCRIPTION: This snippet illustrates how to use MediaPipe Pose for real-time pose detection from a webcam feed. It initializes the `Pose` model for live video, continuously captures frames, processes them to detect pose landmarks, and displays the annotated video stream. It also includes performance optimization by marking images as not writeable.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/pose.md#_snippet_1

LANGUAGE: Python
CODE:
```
# For webcam input:
cap = cv2.VideoCapture(0)
with mp_pose.Pose(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5) as pose:
  while cap.isOpened():
    success, image = cap.read()
    if not success:
      print("Ignoring empty camera frame.")
      # If loading a video, use 'break' instead of 'continue'.
      continue

    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = pose.process(image)

    # Draw the pose annotation on the image.
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    mp_drawing.draw_landmarks(
        image,
        results.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
    # Flip the image horizontally for a selfie-view display.
    cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))
    if cv2.waitKey(5) & 0xFF == 27:
      break
cap.release()
```

----------------------------------------

TITLE: Cloning MediaPipe Repository (Bash)
DESCRIPTION: This command clones the official MediaPipe GitHub repository to your local machine. This repository contains all the source code, examples, and necessary files for building MediaPipe applications.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#_snippet_3

LANGUAGE: bash
CODE:
```
git clone https://github.com/google/mediapipe.git
```

----------------------------------------

TITLE: Installing MediaPipe Solution via NPM
DESCRIPTION: This snippet demonstrates how to install a MediaPipe solution package, specifically @mediapipe/holistic, using the npm package manager. This command adds the specified package to your project's node_modules directory, making it available for local development. It's a common method for integrating MediaPipe solutions into JavaScript projects.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/javascript.md#_snippet_0

LANGUAGE: Bash
CODE:
```
npm install @mediapipe/holistic.
```

----------------------------------------

TITLE: Detecting Holistic Body Landmarks with MediaPipe Holistic Landmarker (JavaScript)
DESCRIPTION: This snippet initializes the MediaPipe Holistic Landmarker task, loading the WASM files and a pre-trained model. It combines pose, face, and hand landmark detection to provide a complete set of human body landmarks from an HTML image element. Requires the @mediapipe/tasks-vision library.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#_snippet_5

LANGUAGE: JavaScript
CODE:
```
const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm"
);
const holisticLandmarker = await HolisticLandmarker.createFromModelPath(vision,
    "https://storage.googleapis.com/mediapipe-models/holistic_landmarker/holistic_landmarker/float16/1/hand_landmark.task"
);
const image = document.getElementById("image") as HTMLImageElement;
const landmarks = holisticLandmarker.detect(image);
```

----------------------------------------

TITLE: Recognizing Hand Gestures with MediaPipe Gesture Recognizer (JavaScript)
DESCRIPTION: This snippet initializes the MediaPipe Gesture Recognizer task, loading the WASM files and a pre-trained model. It then recognizes hand gestures in an HTML image element, providing both the recognized gesture results and hand landmarks. Requires the @mediapipe/tasks-vision library.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#_snippet_3

LANGUAGE: JavaScript
CODE:
```
const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm"
);
const gestureRecognizer = await GestureRecognizer.createFromModelPath(vision,
    "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
);
const image = document.getElementById("image") as HTMLImageElement;
const recognitions = gestureRecognizer.recognize(image);
```

----------------------------------------

TITLE: Installing Android APK via ADB
DESCRIPTION: This command installs the compiled Android Package Kit (APK) file onto a connected device or emulator using Android Debug Bridge (ADB). The APK is located in the `bazel-bin` directory under the application's path.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#_snippet_8

LANGUAGE: bash
CODE:
```
adb install bazel-bin/$APPLICATION_PATH/helloworld.apk
```

----------------------------------------

TITLE: Binding Custom C++ Class with pybind11
DESCRIPTION: This C++ snippet demonstrates how to use pybind11 to create Python bindings for a custom C++ class, 'MyType'. It shows the basic structure for defining the class within a pybind11 module, including its constructor and other potential member functions, making it accessible from Python.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#_snippet_26

LANGUAGE: C++
CODE:
```
#include "path/to/my_type/header/file.h"
#include "pybind11/pybind11.h"

namespace py = pybind11;

PYBIND11_MODULE(my_type_binding, m) {
  // Write binding code or a custom type caster for MyType.
  py::class_<MyType>(m, "MyType")
      .def(py::init<>())}

```

----------------------------------------

TITLE: Building MediaPipe AAR (Bazel)
DESCRIPTION: This Bazel command compiles and links the MediaPipe AAR. It includes various optimization flags (`-c opt`, `--strip=ALWAYS`, `-Oz`), specifies target CPU architectures (`arm64-v8a,armeabi-v7a`), and manages linking options to reduce binary size and improve performance. The command targets a generic AAR build file path and name.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/android_archive_library.md#_snippet_1

LANGUAGE: bash
CODE:
```
bazel build -c opt --strip=ALWAYS \
    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
    --fat_apk_cpu=arm64-v8a,armeabi-v7a \
    --legacy_whole_archive=0 \
    --features=-legacy_whole_archive \
    --copt=-fvisibility=hidden \
    --copt=-ffunction-sections \
    --copt=-fdata-sections \
    --copt=-fstack-protector \
    --copt=-Oz \
    --copt=-fomit-frame-pointer \
    --copt=-DABSL_MIN_LOG_LEVEL=2 \
    --linkopt=-Wl,--gc-sections,--strip-all \
    //path/to/the/aar/build/file:aar_name.aar
```

----------------------------------------

TITLE: Initializing MediaPipe CalculatorGraph with Text Config (Python)
DESCRIPTION: This snippet demonstrates how to initialize a MediaPipe CalculatorGraph using a text-based CalculatorGraphConfig protobuf. It sets up an input and output stream and defines a PassThroughCalculator. An output stream observer is registered to collect packets into a list, showcasing how to capture results.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#_snippet_31

LANGUAGE: Python
CODE:
```
import mediapipe as mp

config_text = """
  input_stream: 'in_stream'
  output_stream: 'out_stream'
  node {
    calculator: 'PassThroughCalculator'
    input_stream: 'in_stream'
    output_stream: 'out_stream'
  }
"""
graph = mp.CalculatorGraph(graph_config=config_text)
output_packets = []
graph.observe_output_stream(
    'out_stream',
    lambda stream_name, packet:
        output_packets.append(mp.packet_getter.get_str(packet)))
```

----------------------------------------

TITLE: Configuring and Running MediaPipe Face Mesh with Video Input - Android Java
DESCRIPTION: This snippet demonstrates how to initialize MediaPipe Face Mesh for video input on Android. It configures `FaceMeshOptions` for GPU usage and landmark refinement, sets up `VideoInput` to feed frames to the Face Mesh solution, and integrates `SolutionGlSurfaceView` for OpenGL rendering of results. It also shows how to listen for results, render them, and handle video selection from the device's media store using `ActivityResultLauncher`.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#_snippet_6

LANGUAGE: Java
CODE:
```
// For video input and result rendering with OpenGL.
FaceMeshOptions faceMeshOptions =
    FaceMeshOptions.builder()
        .setStaticImageMode(false)
        .setRefineLandmarks(true)
        .setMaxNumFaces(1)
        .setRunOnGpu(true).build();
FaceMesh faceMesh = new FaceMesh(this, faceMeshOptions);
faceMesh.setErrorListener(
    (message, e) -> Log.e(TAG, "MediaPipe Face Mesh error:" + message));

// Initializes a new VideoInput instance and connects it to MediaPipe Face Mesh Solution.
VideoInput videoInput = new VideoInput(this);
videoInput.setNewFrameListener(
    textureFrame -> faceMesh.send(textureFrame));

// Initializes a new GlSurfaceView with a ResultGlRenderer<FaceMeshResult> instance
// that provides the interfaces to run user-defined OpenGL rendering code.
// See mediapipe/examples/android/solutions/facemesh/src/main/java/com/google/mediapipe/examples/facemesh/FaceMeshResultGlRenderer.java
// as an example.
SolutionGlSurfaceView<FaceMeshResult> glSurfaceView =
    new SolutionGlSurfaceView<>(
        this, faceMesh.getGlContext(), faceMesh.getGlMajorVersion());
glSurfaceView.setSolutionResultRenderer(new FaceMeshResultGlRenderer());
glSurfaceView.setRenderInputImage(true);

faceMesh.setResultListener(
    faceMeshResult -> {
      NormalizedLandmark noseLandmark =
          result.multiFaceLandmarks().get(0).getLandmarkList().get(1);
      Log.i(
          TAG,
          String.format(
              "MediaPipe Face Mesh nose normalized coordinates (value range: [0, 1]): x=%f, y=%f",
              noseLandmark.getX(), noseLandmark.getY()));
      // Request GL rendering.
      glSurfaceView.setRenderData(faceMeshResult);
      glSurfaceView.requestRender();
    });

ActivityResultLauncher<Intent> videoGetter =
    registerForActivityResult(
        new ActivityResultContracts.StartActivityForResult(),
        result -> {
          Intent resultIntent = result.getData();
          if (resultIntent != null) {
            if (result.getResultCode() == RESULT_OK) {
              glSurfaceView.post(
                  () ->
                      videoInput.start(
                          this,
                          resultIntent.getData(),
                          faceMesh.getGlContext(),
                          glSurfaceView.getWidth(),
                          glSurfaceView.getHeight()));
            }
          }
        });
Intent pickVideoIntent = new Intent(Intent.ACTION_PICK);
pickVideoIntent.setDataAndType(MediaStore.Video.Media.INTERNAL_CONTENT_URI, "video/*");
videoGetter.launch(pickVideoIntent);
```

----------------------------------------

TITLE: Implementing MediaPipe Pose Tracking in JavaScript
DESCRIPTION: This JavaScript code initializes MediaPipe Pose, handles video input, processes pose detection results, and draws them onto a canvas. It configures the Pose model with options like modelComplexity, smoothLandmarks, and enableSegmentation, and uses the Camera utility to feed video frames to the model for real-time processing and visualization.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/pose.md#_snippet_3

LANGUAGE: JavaScript
CODE:
```
<script type="module">
const videoElement = document.getElementsByClassName('input_video')[0];
const canvasElement = document.getElementsByClassName('output_canvas')[0];
const canvasCtx = canvasElement.getContext('2d');
const landmarkContainer = document.getElementsByClassName('landmark-grid-container')[0];
const grid = new LandmarkGrid(landmarkContainer);

function onResults(results) {
  if (!results.poseLandmarks) {
    grid.updateLandmarks([]);
    return;
  }

  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  canvasCtx.drawImage(results.segmentationMask, 0, 0,
                      canvasElement.width, canvasElement.height);

  // Only overwrite existing pixels.
  canvasCtx.globalCompositeOperation = 'source-in';
  canvasCtx.fillStyle = '#00FF00';
  canvasCtx.fillRect(0, 0, canvasElement.width, canvasElement.height);

  // Only overwrite missing pixels.
  canvasCtx.globalCompositeOperation = 'destination-atop';
  canvasCtx.drawImage(
      results.image, 0, 0, canvasElement.width, canvasElement.height);

  canvasCtx.globalCompositeOperation = 'source-over';
  drawConnectors(canvasCtx, results.poseLandmarks, POSE_CONNECTIONS,
                 {color: '#00FF00', lineWidth: 4});
  drawLandmarks(canvasCtx, results.poseLandmarks,
                {color: '#FF0000', lineWidth: 2});
  canvasCtx.restore();

  grid.updateLandmarks(results.poseWorldLandmarks);
}

const pose = new Pose({locateFile: (file) => {
  return `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`;
}});
pose.setOptions({
  modelComplexity: 1,
  smoothLandmarks: true,
  enableSegmentation: true,
  smoothSegmentation: true,
  minDetectionConfidence: 0.5,
  minTrackingConfidence: 0.5
});
pose.onResults(onResults);

const camera = new Camera(videoElement, {
  onFrame: async () => {
    await pose.send({image: videoElement});
  },
  width: 1280,
  height: 720
});
camera.start();
</script>
```

----------------------------------------

TITLE: Implementing Camera Initialization with CameraXPreviewHelper (Java)
DESCRIPTION: This Java method, `startCamera()`, initializes `CameraXPreviewHelper` and sets a listener. When the camera starts, the listener receives the `SurfaceTexture` for frames, which is then saved, and the `previewDisplayView` is made visible to show the live preview.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#_snippet_19

LANGUAGE: Java
CODE:
```
public void startCamera() {
  cameraHelper = new CameraXPreviewHelper();
  cameraHelper.setOnCameraStartedListener(
    surfaceTexture -> {
      previewFrameTexture = surfaceTexture;
      // Make the display view visible to start showing the preview.
      previewDisplayView.setVisibility(View.VISIBLE);
    });
}
```

----------------------------------------

TITLE: MediaPipe Python Project Dependencies (requirements_lock.txt)
DESCRIPTION: This snippet provides the complete list of Python packages and their specific versions, including transitive dependencies, required for the MediaPipe project. It is an autogenerated lock file (`requirements_lock.txt`) created by `pip-compile` to ensure reproducible and consistent build environments across different systems.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/requirements_lock.txt#_snippet_0

LANGUAGE: Python requirements.txt
CODE:
```
#
# This file is autogenerated by pip-compile with Python 3.9
# by the following command:
#
#    pip-compile --output-file=mediapipe/opensource_only/requirements_lock.txt mediapipe/opensource_only/requirements.txt
#
absl-py==2.1.0
    # via -r mediapipe/opensource_only/requirements.txt
attrs==24.2.0
    # via -r mediapipe/opensource_only/requirements.txt
cffi==1.17.1
    # via sounddevice
contourpy==1.3.0
    # via matplotlib
cycler==0.12.1
    # via matplotlib
flatbuffers==24.3.25
    # via -r mediapipe/opensource_only/requirements.txt
fonttools==4.54.1
    # via matplotlib
importlib-metadata==8.5.0
    # via jax
importlib-resources==6.4.5
    # via matplotlib
jax==0.4.30
    # via -r mediapipe/opensource_only/requirements.txt
jaxlib==0.4.30
    # via
    #   -r mediapipe/opensource_only/requirements.txt
    #   jax
kiwisolver==1.4.7
    # via matplotlib
matplotlib==3.9.2
    # via -r mediapipe/opensource_only/requirements.txt
ml-dtypes==0.5.0
    # via
    #   jax
    #   jaxlib
numpy==1.26.4
    # via
    #   -r mediapipe/opensource_only/requirements.txt
    #   contourpy
    #   jax
    #   jaxlib
    #   matplotlib
    #   ml-dtypes
    #   opencv-contrib-python
    #   scipy
opencv-contrib-python==4.10.0.84
    # via -r mediapipe/opensource_only/requirements.txt
opt-einsum==3.4.0
    # via jax
packaging==24.1
    # via matplotlib
pillow==10.4.0
    # via matplotlib
protobuf==4.25.5
    # via -r mediapipe/opensource_only/requirements.txt
pycparser==2.22
    # via cffi
pyparsing==3.1.4
    # via matplotlib
python-dateutil==2.9.0.post0
    # via matplotlib
scipy==1.13.1
    # via
    #   jax
    #   jaxlib
sentencepiece==0.2.0
    # via -r mediapipe/opensource_only/requirements.txt
six==1.16.0
    # via python-dateutil
sounddevice==0.5.0
    # via -r mediapipe/opensource_only/requirements.txt
zipp==3.20.2
    # via
    #   importlib-metadata
    #   importlib-resources
```

----------------------------------------

TITLE: Detecting Hand Landmarks with MediaPipe Hand Landmarker (JavaScript)
DESCRIPTION: This snippet initializes the MediaPipe Hand Landmarker task, loading the WASM files and a pre-trained model. It then detects hand landmarks on an HTML image element, which can be used for localizing key points and rendering visual effects over hands. Requires the @mediapipe/tasks-vision library.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#_snippet_4

LANGUAGE: JavaScript
CODE:
```
const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm"
);
const handLandmarker = await HandLandmarker.createFromModelPath(vision,
    "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
);
const image = document.getElementById("image") as HTMLImageElement;
const landmarks = handLandmarker.detect(image);
```

----------------------------------------

TITLE: Setting Up HTML for MediaPipe Hands JavaScript
DESCRIPTION: This HTML snippet provides the basic structure for integrating MediaPipe Hands in a web application. It includes necessary script imports from MediaPipe's CDN for camera utilities, control utilities, drawing utilities, and the Hands solution itself. It also defines video and canvas elements for input and output display.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#_snippet_2

LANGUAGE: HTML
CODE:
```
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>
</head>

<body>
  <div class="container">
    <video class="input_video"></video>
    <canvas class="output_canvas" width="1280px" height="720px"></canvas>
  </div>
</body>
</html>
```

----------------------------------------

TITLE: Installing MediaPipe and Launching Python Interpreter
DESCRIPTION: Within the activated virtual environment, this sequence of commands first installs the MediaPipe Python package using pip, then launches the Python interpreter, making MediaPipe available for immediate use.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#_snippet_1

LANGUAGE: Bash
CODE:
```
(mp_env)$ pip install mediapipe
(mp_env)$ python3
```

----------------------------------------

TITLE: Real-time 3D Object Detection from Webcam with MediaPipe Objectron (Python)
DESCRIPTION: This snippet illustrates how to perform real-time 3D object detection using MediaPipe Objectron with live webcam input. It initializes the `Objectron` model for video processing, continuously captures frames, processes them, and then draws the detected 2D landmarks and 3D axes before displaying the annotated video feed.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#_snippet_1

LANGUAGE: Python
CODE:
```
# For webcam input:
cap = cv2.VideoCapture(0)
with mp_objectron.Objectron(static_image_mode=False,
                            max_num_objects=5,
                            min_detection_confidence=0.5,
                            min_tracking_confidence=0.99,
                            model_name='Shoe') as objectron:
  while cap.isOpened():
    success, image = cap.read()
    if not success:
      print("Ignoring empty camera frame.")
      # If loading a video, use 'break' instead of 'continue'.
      continue

    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = objectron.process(image)

    # Draw the box landmarks on the image.
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    if results.detected_objects:
        for detected_object in results.detected_objects:
            mp_drawing.draw_landmarks(
              image, detected_object.landmarks_2d, mp_objectron.BOX_CONNECTIONS)
            mp_drawing.draw_axis(image, detected_object.rotation,
                                 detected_object.translation)
    # Flip the image horizontally for a selfie-view display.
    cv2.imshow('MediaPipe Objectron', cv2.flip(image, 1))
    if cv2.waitKey(5) & 0xFF == 27:
      break
cap.release()
```

----------------------------------------

TITLE: Defining MediaPipe Calculator Nodes in Graph Configuration
DESCRIPTION: This snippet illustrates how to define calculator nodes within a MediaPipe graph configuration. It specifies the calculator type, its input streams, and its output streams, demonstrating both simple stream naming and tag-indexed naming for multiple outputs.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#_snippet_2

LANGUAGE: MediaPipe Graph Configuration
CODE:
```
node {
  calculator: "SomeAudioVideoCalculator"
  input_stream: "combined_input"
  output_stream: "VIDEO:video_stream"
  output_stream: "AUDIO:0:audio_left"
  output_stream: "AUDIO:1:audio_right"
}

node {
  calculator: "SomeAudioCalculator"
  input_stream: "audio_left"
  input_stream: "audio_right"
  output_stream: "audio_energy"
}
```

----------------------------------------

TITLE: Connecting Calculator Streams by Tag in MediaPipe Graph (Proto)
DESCRIPTION: This snippet demonstrates how to connect calculator input and output streams in a MediaPipe graph configuration using named streams and tags. It shows `SomeAudioVideoCalculator`'s `VIDEO` output connecting to `SomeVideoCalculator`'s `VIDEO_IN` input via `video_stream`, illustrating basic stream routing.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#_snippet_1

LANGUAGE: Protocol Buffer
CODE:
```
# Graph describing calculator SomeAudioVideoCalculator
node {
  calculator: "SomeAudioVideoCalculator"
  input_stream: "INPUT:combined_input"
  output_stream: "VIDEO:video_stream"
}
node {
  calculator: "SomeVideoCalculator"
  input_stream: "VIDEO_IN:video_stream"
  output_stream: "VIDEO_OUT:processed_video"
}
```

----------------------------------------

TITLE: Real-time Face Mesh Detection with MediaPipe JavaScript
DESCRIPTION: This JavaScript code initializes MediaPipe Face Mesh for real-time face landmark detection. It sets up a video stream as input, processes frames using the `FaceMesh` class, and draws the detected landmarks on a canvas. The `onResults` function handles the output, clearing the canvas and drawing the image along with multi-face landmarks using `drawConnectors` from the MediaPipe drawing utilities.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#_snippet_3

LANGUAGE: JavaScript
CODE:
```
const videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\n\nfunction onResults(results) {\n  canvasCtx.save();\n  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n  if (results.multiFaceLandmarks) {\n    for (const landmarks of results.multiFaceLandmarks) {\n      drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION,\n                     {color: '#C0C0C070', lineWidth: 1});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE, {color: '#FF3030'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYEBROW, {color: '#FF3030'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_IRIS, {color: '#FF3030'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYE, {color: '#30FF30'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYEBROW, {color: '#30FF30'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_IRIS, {color: '#30FF30'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_FACE_OVAL, {color: '#E0E0E0'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LIPS, {color: '#E0E0E0'});\n    }\n  }\n  canvasCtx.restore();\n}\n\nconst faceMesh = new FaceMesh({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;\n}});\nfaceMesh.setOptions({\n  maxNumFaces: 1,\n  refineLandmarks: true,\n  minDetectionConfidence: 0.5,\n  minTrackingConfidence: 0.5\n});\nfaceMesh.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await faceMesh.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();
```

----------------------------------------

TITLE: Real-time Face Mesh Detection with Webcam in Python
DESCRIPTION: This Python snippet demonstrates how to perform real-time face mesh detection using MediaPipe and OpenCV. It captures video from a webcam, processes each frame to detect facial landmarks, and then draws the tessellation, contours, and iris connections on the image before displaying it. It handles frame reading, color conversion, and display.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#_snippet_1

LANGUAGE: Python
CODE:
```
drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)
cap = cv2.VideoCapture(0)
with mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5) as face_mesh:
  while cap.isOpened():
    success, image = cap.read()
    if not success:
      print("Ignoring empty camera frame.")
      # If loading a video, use 'break' instead of 'continue'.
      continue

    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(image)

    # Draw the face mesh annotations on the image.
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    if results.multi_face_landmarks:
      for face_landmarks in results.multi_face_landmarks:
        mp_drawing.draw_landmarks(
            image=image,
            landmark_list=face_landmarks,
            connections=mp_face_mesh.FACEMESH_TESSELATION,
            landmark_drawing_spec=None,
            connection_drawing_spec=mp_drawing_styles
            .get_default_face_mesh_tesselation_style())
        mp_drawing.draw_landmarks(
            image=image,
            landmark_list=face_landmarks,
            connections=mp_face_mesh.FACEMESH_CONTOURS,
            landmark_drawing_spec=None,
            connection_drawing_spec=mp_drawing_styles
            .get_default_face_mesh_contours_style())
        mp_drawing.draw_landmarks(
            image=image,
            landmark_list=face_landmarks,
            connections=mp_face_mesh.FACEMESH_IRISES,
            landmark_drawing_spec=None,
            connection_drawing_spec=mp_drawing_styles
            .get_default_face_mesh_iris_connections_style())
    # Flip the image horizontally for a selfie-view display.
    cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))
    if cv2.waitKey(5) & 0xFF == 27:
      break
cap.release()
```

----------------------------------------

TITLE: Processing Webcam Input with MediaPipe Hands in Python
DESCRIPTION: This snippet demonstrates how to capture live video from a webcam, process each frame with MediaPipe Hands for hand landmark detection, and display the results. It initializes the MediaPipe Hands model with specified confidence levels and handles frame reading, color conversion, and drawing annotations.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#_snippet_1

LANGUAGE: Python
CODE:
```
# For webcam input:
cap = cv2.VideoCapture(0)
with mp_hands.Hands(
    model_complexity=0,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5) as hands:
  while cap.isOpened():
    success, image = cap.read()
    if not success:
      print("Ignoring empty camera frame.")
      # If loading a video, use 'break' instead of 'continue'.
      continue

    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(image)

    # Draw the hand annotations on the image.
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    if results.multi_hand_landmarks:
      for hand_landmarks in results.multi_hand_landmarks:
        mp_drawing.draw_landmarks(
            image,
            hand_landmarks,
            mp_hands.HAND_CONNECTIONS,
            mp_drawing_styles.get_default_hand_landmarks_style(),
            mp_drawing_styles.get_default_hand_connections_style())
    # Flip the image horizontally for a selfie-view display.
    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))
    if cv2.waitKey(5) & 0xFF == 27:
      break
cap.release()
```

----------------------------------------

TITLE: Processing Static Images with MediaPipe Holistic in Python
DESCRIPTION: This snippet demonstrates how to use MediaPipe Holistic to process a list of static images. It initializes the Holistic model with segmentation and face landmark refinement enabled, then processes each image to detect pose, face, and hand landmarks, and applies segmentation. It outputs annotated images and prints nose coordinates.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md#_snippet_0

LANGUAGE: Python
CODE:
```
import cv2
import mediapipe as mp
import numpy as np
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_holistic = mp.solutions.holistic

# For static images:
IMAGE_FILES = []
BG_COLOR = (192, 192, 192) # gray
with mp_holistic.Holistic(
    static_image_mode=True,
    model_complexity=2,
    enable_segmentation=True,
    refine_face_landmarks=True) as holistic:
  for idx, file in enumerate(IMAGE_FILES):
    image = cv2.imread(file)
    image_height, image_width, _ = image.shape
    # Convert the BGR image to RGB before processing.
    results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    if results.pose_landmarks:
      print(
          f'Nose coordinates: ('
          f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x * image_width}, '
          f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y * image_height})'
      )

    annotated_image = image.copy()
    # Draw segmentation on the image.
    # To improve segmentation around boundaries, consider applying a joint
    # bilateral filter to "results.segmentation_mask" with "image".
    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1
    bg_image = np.zeros(image.shape, dtype=np.uint8)
    bg_image[:] = BG_COLOR
    annotated_image = np.where(condition, annotated_image, bg_image)
    # Draw pose, left and right hands, and face landmarks on the image.
    mp_drawing.draw_landmarks(
        annotated_image,
        results.face_landmarks,
        mp_holistic.FACEMESH_TESSELATION,
        landmark_drawing_spec=None,
        connection_drawing_spec=mp_drawing_styles
        .get_default_face_mesh_tesselation_style())
    mp_drawing.draw_landmarks(
        annotated_image,
        results.pose_landmarks,
        mp_holistic.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.
        get_default_pose_landmarks_style())
    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)
    # Plot pose world landmarks.
    mp_drawing.plot_landmarks(
        results.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS)
```

----------------------------------------

TITLE: Building and Copying MediaPipe Assets to Android Project - Bash
DESCRIPTION: These commands build the MediaPipe binary graph using Bazel and then copy both the compiled binary graph (`.binarypb`) and the associated TensorFlow Lite model (`.tflite`) into the `app/src/main/assets` directory. These assets are crucial for MediaPipe's runtime operation within the Android application.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/android_archive_library.md#_snippet_5

LANGUAGE: bash
CODE:
```
bazel build -c opt mediapipe/graphs/face_detection:face_detection_mobile_gpu_binary_graph
cp bazel-bin/mediapipe/graphs/face_detection/face_detection_mobile_gpu.binarypb /path/to/your/app/src/main/assets/
cp mediapipe/modules/face_detection/face_detection_short_range.tflite /path/to/your/app/src/main/assets/
```

----------------------------------------

TITLE: Detecting Pose Landmarks with MediaPipe Pose Landmarker (JavaScript)
DESCRIPTION: This snippet demonstrates how to initialize the MediaPipe Pose Landmarker to detect body pose landmarks in an image. It involves loading the vision tasks WASM module and creating the landmarker from a model path. The `detect` method processes an HTML image element to return landmark results.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#_snippet_11

LANGUAGE: JavaScript
CODE:
```
const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm"
);
const poseLandmarker = await PoseLandmarker.createFromModelPath(vision,
    "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task"
);
const image = document.getElementById("image") as HTMLImageElement;
const landmarks = poseLandmarker.detect(image);
```

----------------------------------------

TITLE: Segmenting Images with MediaPipe Image Segmenter (JavaScript)
DESCRIPTION: This snippet shows how to initialize the MediaPipe Image Segmenter and perform image segmentation. It requires loading the vision tasks WASM module and creating the segmenter from a model path. The `segment` method processes an HTML image element and provides masks, width, and height via a callback function.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#_snippet_8

LANGUAGE: JavaScript
CODE:
```
const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm"
);
const imageSegmenter = await ImageSegmenter.createFromModelPath(vision,
    "https://storage.googleapis.com/mediapipe-models/image_segmenter/deeplab_v3/float32/1/deeplab_v3.tflite"
);
const image = document.getElementById("image") as HTMLImageElement;
imageSegmenter.segment(image, (masks, width, height) => {
  ...
});
```

----------------------------------------

TITLE: Including MediaPipe Libraries via CDN in HTML
DESCRIPTION: This HTML snippet shows how to include MediaPipe utility and solution libraries directly into a web page using a Content Delivery Network (CDN) like jsDelivr. By adding these <script> tags within the <head> section, the libraries become globally available in the browser, eliminating the need for local installation via npm. This method is useful for quick prototyping or when a full build system is not desired.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/javascript.md#_snippet_1

LANGUAGE: HTML
CODE:
```
<head>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@0.1/drawing_utils.js" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic@0.1/holistic.js" crossorigin="anonymous"></script>
</head>
```

----------------------------------------

TITLE: Installing Xcode Command Line Tools (Bash)
DESCRIPTION: This command installs the Xcode Command Line Tools, which are essential for various development tasks on macOS, including compiling code and using command-line utilities required by MediaPipe.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#_snippet_0

LANGUAGE: bash
CODE:
```
xcode-select --install
```

----------------------------------------

TITLE: Processing Image Input with MediaPipe Face Detection in Android
DESCRIPTION: This snippet demonstrates how to initialize MediaPipe Face Detection for static image mode, set up listeners for processing results and errors, and integrate with an ActivityResultLauncher to select and send images from the device gallery for detection. It shows how to draw detection results on a custom ImageView.
SOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_detection.md#_snippet_6

LANGUAGE: Java
CODE:
```
// For reading images from gallery and drawing the output in an ImageView.
FaceDetectionOptions faceDetectionOptions =
    FaceDetectionOptions.builder()
        .setStaticImageMode(true)
        .setModelSelection(0).build();
FaceDetection faceDetection = new FaceDetection(this, faceDetectionOptions);

// Connects MediaPipe Face Detection Solution to the user-defined ImageView
// instance that allows users to have the custom drawing of the output landmarks
// on it. See mediapipe/examples/android/solutions/facedetection/src/main/java/com/google/mediapipe/examples/facedetection/FaceDetectionResultImageView.java
// as an example.
FaceDetectionResultImageView imageView = new FaceDetectionResultImageView(this);
faceDetection.setResultListener(
    faceDetectionResult -> {
      if (faceDetectionResult.multiFaceDetections().isEmpty()) {
        return;
      }
      int width = faceDetectionResult.inputBitmap().getWidth();
      int height = faceDetectionResult.inputBitmap().getHeight();
      RelativeKeypoint noseTip =
          faceDetectionResult
              .multiFaceDetections()
              .get(0)
              .getLocationData()
              .getRelativeKeypoints(FaceKeypoint.NOSE_TIP);
      Log.i(
          TAG,
          String.format(
              "MediaPipe Face Detection nose tip coordinates (pixel values): x=%f, y=%f",
              noseTip.getX() * width, noseTip.getY() * height));
      // Request canvas drawing.
      imageView.setFaceDetectionResult(faceDetectionResult);
      runOnUiThread(() -> imageView.update());
    });
faceDetection.setErrorListener(
    (message, e) -> Log.e(TAG, "MediaPipe Face Detection error:" + message));

// ActivityResultLauncher to get an image from the gallery as Bitmap.
ActivityResultLauncher<Intent> imageGetter =
    registerForActivityResult(
        new ActivityResultContracts.StartActivityForResult(),
        result -> {
          Intent resultIntent = result.getData();
          if (resultIntent != null && result.getResultCode() == RESULT_OK) {
            Bitmap bitmap = null;
            try {
              bitmap =
                  MediaStore.Images.Media.getBitmap(
                      this.getContentResolver(), resultIntent.getData());
              // Please also rotate the Bitmap based on its orientation.
            } catch (IOException e) {
              Log.e(TAG, "Bitmap reading error:" + e);
            }
            if (bitmap != null) {
              faceDetection.send(bitmap);
            }
          }
        });
Intent pickImageIntent = new Intent(Intent.ACTION_PICK);
pickImageIntent.setDataAndType(MediaStore.Images.Media.INTERNAL_CONTENT_URI, "image/*");
imageGetter.launch(pickImageIntent);
```