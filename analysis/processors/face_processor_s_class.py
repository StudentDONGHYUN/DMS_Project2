"""
Face Processor (S-Class: ìµœì¢… í†µí•© ë²„ì „) - ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„ ì™„ë£Œ
'ë””ì§€í„¸ ì‹¬ë¦¬í•™ì'ë¡œì„œ ìš´ì „ìì˜ ì–¼êµ´ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ëª¨ë“  ìƒì²´ ì‹ í˜¸ë¥¼
ì¸ì§€ ì‹¬ë¦¬í•™, ì•ˆêµ¬ ìš´ë™ ë¶„ì„, ì›ê²© ê´‘í˜ˆë¥˜ì¸¡ì •(rPPG) ë“±ì˜
ì—°êµ¬ ê¸°ë°˜ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import math
import time
from collections import deque
from typing import Any, Dict, List, Tuple

import numpy as np
from cachetools import TTLCache, cached
from mediapipe.framework.formats import landmark_pb2
from scipy.signal import butter, detrend, filtfilt, find_peaks
from scipy.fft import rfft, rfftfreq
import cv2

from config.settings import get_config
from core.constants import AnalysisConstants, MathConstants, MediaPipeConstants
from core.definitions import GazeZone
from core.interfaces import (IDriverIdentifier, IDrowsinessDetector,
                             IEmotionRecognizer, IFaceDataProcessor,
                             IGazeClassifier, IMetricsUpdater)

logger = logging.getLogger(__name__)


class FaceDataProcessor(IFaceDataProcessor):
    """
    ì–¼êµ´ ë°ì´í„° ì „ë¬¸ ì²˜ë¦¬ê¸° (S-Class)
    
    ğŸ¯ ìˆ˜ì • ì‚¬í•­: 
    - ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìš”êµ¬í•˜ëŠ” ì„¸ ê°œì˜ ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„ ì™„ë£Œ
    - ê¸°ì¡´ì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    """

    def __init__(
        self,
        metrics_updater: IMetricsUpdater,
        drowsiness_detector: IDrowsinessDetector,
        emotion_recognizer: IEmotionRecognizer,
        gaze_classifier: IGazeClassifier,
        driver_identifier: IDriverIdentifier,
    ):
        self.metrics_updater = metrics_updater
        self.drowsiness_detector = drowsiness_detector
        self.emotion_recognizer = emotion_recognizer
        self.gaze_classifier = gaze_classifier
        self.driver_identifier = driver_identifier

        self.config = get_config()

        # --- ìƒíƒœ ì¶”ì  ë³€ìˆ˜ ---
        self.current_gaze_zone = GazeZone.FRONT
        self.gaze_zone_start_time = time.time()

        # EMA í•„í„° ì„¤ì •
        self.head_pose_history = deque(maxlen=self.config.face.ema_filter_size)
        self.ema_alpha = self.config.face.ema_alpha

        # [S-Class] ì•ˆêµ¬ ìš´ë™ ë° ë™ê³µ ë¶„ì„ì„ ìœ„í•œ ì´ë ¥ ë²„í¼
        history_size = self.config.face.saccade_history_size
        self.left_eye_history = deque(maxlen=history_size)
        self.right_eye_history = deque(maxlen=history_size)
        self.pupil_size_history = deque(maxlen=history_size * 2)  # ë™ê³µì€ ì¢€ ë” ê¸´ ì´ë ¥ ì¶”ì 

        # [S-Class] rPPG ì‹ í˜¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë²„í¼
        rppg_buffer_size = int(self.config.rppg.fps * self.config.rppg.window_size_s)
        self.rppg_signal_buffer = deque(maxlen=rppg_buffer_size)

        logger.info("FaceDataProcessor (S-Class) ì´ˆê¸°í™” ì™„ë£Œ - ë””ì§€í„¸ ì‹¬ë¦¬í•™ì ì¤€ë¹„ë¨")

    def get_processor_name(self) -> str:
        return "FaceDataProcessor"

    def get_required_data_types(self) -> List[str]:
        return ["face_landmarks", "face_blendshapes", "facial_transformation_matrixes"]

    # =============================================================================
    # ğŸ¯ **ì¶”ê°€ëœ ë¶€ë¶„**: ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìš”êµ¬í•˜ëŠ” ì„¸ ê°œì˜ ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„
    # =============================================================================
    
    async def process_face_landmarks(self, landmarks: Any, timestamp: float) -> Dict[str, Any]:
        """
        ì–¼êµ´ ëœë“œë§ˆí¬ ì „ìš© ì²˜ë¦¬ (ì¸í„°í˜ì´ìŠ¤ ìš”êµ¬ì‚¬í•­)
        
        ì´ ë©”ì„œë“œëŠ” ëœë“œë§ˆí¬ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ì¡¸ìŒ ë¶„ì„ê³¼ ì‚¬ì¼€ì´ë“œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        S-Class ë²„ì „ì—ì„œëŠ” ê³ ê¸‰ ì•ˆêµ¬ ìš´ë™ ë¶„ì„ë„ í¬í•¨ë©ë‹ˆë‹¤.
        """
        try:
            # ê¸°ë³¸ ì¡¸ìŒ ë¶„ì„
            drowsiness_data = await self.process_drowsiness_analysis(landmarks, timestamp)
            
            # [S-Class] ê³ ê¸‰ ì•ˆêµ¬ ìš´ë™ ë¶„ì„
            saccade_data = self._analyze_saccadic_movement(landmarks, timestamp)
            
            # ìš´ì „ì ì‹ ì› í™•ì¸
            driver_data = await self.process_driver_identification(landmarks)
            
            # ê²°ê³¼ í†µí•©
            result = {}
            result.update(drowsiness_data)
            result.update({'saccade': saccade_data})
            result.update(driver_data)
            
            return result
            
        except Exception as e:
            logger.error(f"ì–¼êµ´ ëœë“œë§ˆí¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                'drowsiness': {'status': 'error', 'confidence': 0.0},
                'saccade': {'saccade_velocity_norm': 0.0, 'saccade_count_per_s': 0.0},
                'driver': {'identity': 'unknown', 'confidence': 0.0}
            }

    async def process_face_blendshapes(self, blendshapes: Any, timestamp: float) -> Dict[str, Any]:
        """
        ì–¼êµ´ ë¸”ë Œë“œì…°ì´í”„ ì „ìš© ì²˜ë¦¬ (ì¸í„°í˜ì´ìŠ¤ ìš”êµ¬ì‚¬í•­)
        
        ì´ ë©”ì„œë“œëŠ” ë¸”ë Œë“œì…°ì´í”„ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ê°ì • ë¶„ì„ê³¼ ë™ê³µ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        S-Class ë²„ì „ì—ì„œëŠ” ì¸ì§€ ë¶€í•˜ ë¶„ì„ë„ í¬í•¨ë©ë‹ˆë‹¤.
        """
        try:
            # ê¸°ë³¸ ê°ì • ë¶„ì„
            emotion_data = await self.process_emotion_analysis(blendshapes, timestamp)
            
            # [S-Class] ê³ ê¸‰ ë™ê³µ ì—­í•™ ë¶„ì„
            pupil_data = self._analyze_pupil_dynamics(blendshapes)
            
            # ê²°ê³¼ í†µí•©
            result = {}
            result.update(emotion_data)
            result.update({'pupil': pupil_data})
            
            return result
            
        except Exception as e:
            logger.error(f"ì–¼êµ´ ë¸”ë Œë“œì…°ì´í”„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                'emotion': {'state': None, 'confidence': 0.0},
                'pupil': {'estimated_pupil_diameter': 0.0, 'pupil_variability': 0.0}
            }

    async def process_facial_transformation(self, transformation: Any, timestamp: float) -> Dict[str, Any]:
        """
        ì–¼êµ´ ë³€í™˜ í–‰ë ¬ ì „ìš© ì²˜ë¦¬ (ì¸í„°í˜ì´ìŠ¤ ìš”êµ¬ì‚¬í•­)
        
        ì´ ë©”ì„œë“œëŠ” 3D ë³€í™˜ í–‰ë ¬ì„ ë°›ì•„ì„œ ë¨¸ë¦¬ ìì„¸ì™€ ì‹œì„  ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        S-Class ë²„ì „ì—ì„œëŠ” ì•ˆì •í™”ëœ ë¨¸ë¦¬ ìì„¸ ì¶”ì ì´ í¬í•¨ë©ë‹ˆë‹¤.
        """
        try:
            # [S-Class] ê³ ê¸‰ ë¨¸ë¦¬ ìì„¸ ë° ì‹œì„  ë¶„ì„ (ì•ˆì •í™” í¬í•¨)
            gaze_data = await self.process_head_pose_and_gaze(transformation, timestamp)
            
            return gaze_data
            
        except Exception as e:
            logger.error(f"ì–¼êµ´ ë³€í™˜ í–‰ë ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._get_default_pose_gaze_data()

    # =============================================================================
    # ê¸°ì¡´ S-Class ë©”ì„œë“œë“¤ (ë³€ê²½ ì—†ìŒ)
    # =============================================================================

    async def process_data(self, data: Any, image: np.ndarray, timestamp: float) -> Dict[str, Any]:
        logger.debug(f"[face_processor_s_class] process_data input: {data}")
        if hasattr(data, 'face_landmarks'):
            logger.debug(f"[face_processor_s_class] face_landmarks: {getattr(data, 'face_landmarks', None)}")
        if not data or not data.face_landmarks:
            return await self._handle_no_face_detected()

        landmarks = data.face_landmarks[0]
        results = {'face_detected': True}

        # 0.5. rPPG ì‹ í˜¸ ì¶”ì¶œ (ë§¤ í”„ë ˆì„ ìˆ˜í–‰)
        self._extract_rppg_signal(image, landmarks, timestamp)

        # 1. ê¸°ë³¸ ë¶„ì„ (ì¡¸ìŒ, ê°ì •, ìì„¸, ì‹ ì›)
        tasks = [
            self.process_drowsiness_analysis(landmarks, timestamp),
            self.process_driver_identification(landmarks)
        ]
        if data.face_blendshapes:
            tasks.append(self.process_emotion_analysis(data.face_blendshapes[0], timestamp))
        if data.facial_transformation_matrixes:
            tasks.append(self.process_head_pose_and_gaze(data.facial_transformation_matrixes[0], timestamp))

        analysis_results = await asyncio.gather(*tasks)
        for res in analysis_results:
            results.update(res)

        # 2. [S-Class] ê³ ê¸‰ ì¸ì§€/ìƒë¦¬ ìƒíƒœ ë¶„ì„
        results['saccade'] = self._analyze_saccadic_movement(landmarks, timestamp)

        if data.face_blendshapes:
            results['pupil'] = self._analyze_pupil_dynamics(data.face_blendshapes[0])

        # rPPGëŠ” ë¦¬ì†ŒìŠ¤ ì†Œëª¨ê°€ ìˆìœ¼ë¯€ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        if int(timestamp * 10) % self.config.rppg.run_interval == 0:
            results['rppg'] = self._estimate_heart_rate_from_rppg()

        # 3. ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        #    rPPG ë²„í¼ê°€ ì°¼ì„ ë•Œë§Œ ì‹¬ë°•ìˆ˜ ê³„ì‚°
        if len(self.rppg_signal_buffer) == self.rppg_signal_buffer.maxlen:
            results['rppg'] = self._estimate_heart_rate_from_rppg()
        
        self._update_all_metrics(results)
        return results

    async def process_drowsiness_analysis(self, landmarks: Any, timestamp: float) -> Dict[str, Any]:
        """ ì¡¸ìŒ ê°ì§€ ì „ë¬¸ ë¶„ì„ """
        try:
            drowsiness_result = self.drowsiness_detector.detect_drowsiness(landmarks, timestamp)
            return {'drowsiness': drowsiness_result}
        except Exception as e:
            logger.error(f"ì¡¸ìŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {'drowsiness': {'status': 'error', 'confidence': 0.0}}

    async def process_emotion_analysis(self, blendshapes: Any, timestamp: float) -> Dict[str, Any]:
        """ ê°ì • ì¸ì‹ ì „ë¬¸ ë¶„ì„ """
        try:
            emotion_result = self.emotion_recognizer.analyze_emotion(blendshapes, timestamp)
            return {'emotion': emotion_result}
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {'emotion': {'state': None, 'confidence': 0.0}}

    async def process_head_pose_and_gaze(self, transformation_matrix: Any, timestamp: float) -> Dict[str, Any]:
        """ [ê³ ë„í™”] ë¨¸ë¦¬ ìì„¸ ë° ì‹œì„  ë¶„ì„ """
        try:
            raw_head_pose = self._extract_euler_angles_from_matrix(transformation_matrix)
            stable_head_pose = self._stabilize_head_pose(raw_head_pose)
            new_gaze_zone = self.gaze_classifier.classify(stable_head_pose['yaw'], stable_head_pose['pitch'], timestamp)
            gaze_zone_duration = self._update_gaze_zone_tracking(new_gaze_zone)
            gaze_stability = self.gaze_classifier.get_gaze_stability()
            attention_focus = self.gaze_classifier.get_attention_focus_score()
            deviation_score = self._calculate_gaze_deviation_score(
                stable_head_pose, new_gaze_zone, gaze_zone_duration, gaze_stability
            )
            return {
                'gaze': {
                    'head_yaw': stable_head_pose['yaw'],
                    'head_pitch': stable_head_pose['pitch'],
                    'head_roll': stable_head_pose['roll'],
                    'current_zone': new_gaze_zone,
                    'zone_duration': gaze_zone_duration,
                    'stability': gaze_stability,
                    'attention_focus': attention_focus,
                    'deviation_score': deviation_score
                }
            }
        except Exception as e:
            logger.error(f"ë¨¸ë¦¬ ìì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._get_default_pose_gaze_data()

    @cached(cache=TTLCache(maxsize=5, ttl=300))
    def _cached_identify_driver(self, landmarks_tuple: Tuple) -> Dict[str, Any]:
        """ [ê³ ë„í™”] ìºì‹œëœ ìš´ì „ì ì‹ë³„ """
        landmarks = [landmark_pb2.NormalizedLandmark(x=lm[0], y=lm[1], z=lm[2]) for lm in landmarks_tuple]
        return self.driver_identifier.identify_driver(landmarks)

    async def process_driver_identification(self, landmarks: Any) -> Dict[str, Any]:
        """ ìš´ì „ì ì‹ ì› í™•ì¸ """
        try:
            landmarks_tuple = tuple((lm.x, lm.y, lm.z) for lm in landmarks)
            driver_info = self._cached_identify_driver(landmarks_tuple)
            return {'driver': driver_info}
        except Exception as e:
            logger.error(f"ìš´ì „ì ì‹ë³„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {'driver': {'identity': 'unknown', 'confidence': 0.0}}

    def _analyze_saccadic_movement(self, landmarks: Any, timestamp: float) -> Dict[str, Any]:
        """ [S-Class] ì•ˆêµ¬ ë„ì•½ ìš´ë™(Saccade) ë° ì‹œì„  ê³ ì •(Fixation) ì‹¤ì œ êµ¬í˜„ """
        try:
            left_iris_lm = landmarks[MediaPipeConstants.EyeLandmarks.LEFT_IRIS_CENTER]
            right_iris_lm = landmarks[MediaPipeConstants.EyeLandmarks.RIGHT_IRIS_CENTER]
            current_left_pos = np.array([left_iris_lm.x, left_iris_lm.y])
            current_right_pos = np.array([right_iris_lm.x, right_iris_lm.y])

            self.left_eye_history.append({'time': timestamp, 'pos': current_left_pos})
            self.right_eye_history.append({'time': timestamp, 'pos': current_right_pos})

            if len(self.left_eye_history) < self.config.face.saccade_min_samples:
                return {'saccade_velocity_norm': 0.0, 'saccade_count_per_s': 0.0, 'gaze_fixation_stability': 1.0}

            left_analysis = self._calculate_eye_movement_metrics(self.left_eye_history)
            right_analysis = self._calculate_eye_movement_metrics(self.right_eye_history)

            return {
                'saccade_velocity_norm': (left_analysis['velocity'] + right_analysis['velocity']) / 2,
                'saccade_count_per_s': (left_analysis['count'] + right_analysis['count']) / 2,
                'gaze_fixation_stability': (left_analysis['stability'] + right_analysis['stability']) / 2
            }
        except Exception as e:
            logger.error(f"Saccade ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'saccade_velocity_norm': 0.0, 'saccade_count_per_s': 0.0, 'gaze_fixation_stability': 0.0}

    def _calculate_eye_movement_metrics(self, history: deque) -> Dict[str, float]:
        """ ë‹¨ì¼ ëˆˆì˜ ì´ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ Saccade ë° Fixation ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ """
        timestamps = np.array([item['time'] for item in history])
        positions = np.array([item['pos'] for item in history])
        dt = np.diff(timestamps)
        dt[dt == 0] = 1e-6
        ds = np.linalg.norm(np.diff(positions, axis=0), axis=1)
        velocities = ds / dt

        saccade_velocity_threshold = self.config.face.saccade_velocity_threshold
        peaks, properties = find_peaks(velocities, height=saccade_velocity_threshold, distance=3)
        total_duration = timestamps[-1] - timestamps[0]
        if total_duration == 0:
            total_duration = 1.0

        saccade_count_per_s = len(peaks) / total_duration
        avg_saccade_velocity = np.mean(properties['peak_heights']) if len(peaks) > 0 else 0.0

        is_fixation = velocities < saccade_velocity_threshold
        fixation_points = positions[1:][is_fixation]

        if len(fixation_points) > 2:
            dispersion = np.mean(np.std(fixation_points, axis=0))
            stability_score = max(0.0, 1.0 - dispersion / self.config.face.fixation_dispersion_max)
        else:
            stability_score = 1.0

        return {'velocity': avg_saccade_velocity, 'count': saccade_count_per_s, 'stability': stability_score}

    def _analyze_pupil_dynamics(self, blendshapes: Any) -> Dict[str, Any]:
        """ [S-Class] ë™ê³µ ë°˜ì‘ ë° ë³€í™”ìœ¨ ë¶„ì„ """
        try:
            blendshapes_dict = {cat.category_name: cat.score for cat in blendshapes}
            pupil_size_y = (blendshapes_dict.get('eyeLookUpLeft', 0) - blendshapes_dict.get('eyeLookDownLeft', 0))
            pupil_size_x = (blendshapes_dict.get('eyeLookOutLeft', 0) - blendshapes_dict.get('eyeLookInLeft', 0))
            pupil_diameter_est = np.linalg.norm([pupil_size_x, pupil_size_y])
            self.pupil_size_history.append(pupil_diameter_est)

            pupil_variability = np.std(self.pupil_size_history) if len(self.pupil_size_history) > 1 else 0.0
            cognitive_load = (blendshapes_dict.get('browDownLeft', 0) + blendshapes_dict.get('browDownRight', 0)) / 2

            return {
                'estimated_pupil_diameter': pupil_diameter_est,
                'pupil_variability': pupil_variability,
                'cognitive_load_indicator': cognitive_load
            }
        except Exception as e:
            logger.error(f"ë™ê³µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'estimated_pupil_diameter': 0.0, 'pupil_variability': 0.0, 'cognitive_load_indicator': 0.0}

    def _extract_rppg_signal(self, image: np.ndarray, landmarks: Any, timestamp: float):
        """ [S-Class] ë§¤ í”„ë ˆì„ì—ì„œ ì´ë§ˆ ì˜ì—­ì˜ í‰ê·  ë…¹ìƒ‰ ì±„ë„ ê°’ì„ ì¶”ì¶œí•˜ì—¬ ë²„í¼ì— ì €ì¥ """
        try:
            img_h, img_w, _ = image.shape
            
            # 1. ì´ë§ˆ ROI ëœë“œë§ˆí¬ ì •ì˜
            forehead_indices = MediaPipeConstants.FaceROIs.FOREHEAD_ROI
            forehead_points = np.array(
                [[landmarks[i].x * img_w, landmarks[i].y * img_h] for i in forehead_indices],
                dtype=np.int32
            )
            
            # 2. ROI ë§ˆìŠ¤í¬ ìƒì„±
            mask = np.zeros((img_h, img_w), dtype=np.uint8)
            cv2.fillPoly(mask, [forehead_points], 255)
            
            # 3. ROI ì˜ì—­ì˜ í‰ê·  ë…¹ìƒ‰ ì±„ë„ ê°’ ê³„ì‚°
            green_channel_mean = cv2.mean(image, mask=mask)[1] # G ì±„ë„ì€ ì¸ë±ìŠ¤ 1
            
            # 4. ë²„í¼ì— (íƒ€ì„ìŠ¤íƒ¬í”„, ì‹ í˜¸ê°’) ì €ì¥
            if green_channel_mean > 0:
                self.rppg_signal_buffer.append((timestamp, green_channel_mean))
        except Exception as e:
            logger.error(f"rPPG ì‹ í˜¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _estimate_heart_rate_from_rppg(self) -> Dict[str, Any]:
        """
        [S-Class] ë²„í¼ë§ëœ rPPG ì‹ í˜¸ë¡œ ì‹¬ë°•ìˆ˜ ë° HRVë¥¼ ê³„ì‚°í•˜ëŠ” ì‹¤ì œ êµ¬í˜„
        - [ê³ ë„í™”] 3ë‹¨ê³„ ì‹ í˜¸ í’ˆì§ˆ ê²€ì¦ì„ í†µí•´ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” HRV ê°’ì„ íê¸°
        """
        try:
            signal_data = list(self.rppg_signal_buffer)
            timestamps = np.array([item[0] for item in signal_data])
            raw_signal = np.array([item[1] for item in signal_data])

            # 1. ì‹ í˜¸ ì „ì²˜ë¦¬: Detrending ë° Band-pass í•„í„°ë§
            detrended_signal = detrend(raw_signal)
            fs = self.config.rppg.fps
            lowcut, highcut = self.config.rppg.low_cut_hz, self.config.rppg.high_cut_hz
            nyquist = 0.5 * fs
            low, high = lowcut / nyquist, highcut / nyquist
            b, a = butter(1, [low, high], btype='band')
            filtered_signal = filtfilt(b, a, detrended_signal)

            # 2. ì£¼íŒŒìˆ˜ ë¶„ì„ (FFT) ë° HR ê³„ì‚°
            N = len(filtered_signal)
            yf = rfft(filtered_signal)
            xf = rfftfreq(N, 1 / fs)
            freq_mask = (xf >= lowcut) & (xf <= highcut)
            
            if not np.any(freq_mask):
                raise ValueError("ìœ íš¨ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            fft_power = np.abs(yf[freq_mask])**2
            peak_freq_index = np.argmax(fft_power)
            peak_freq = xf[freq_mask][peak_freq_index]
            hr_bpm = peak_freq * 60

            # --- HRV ê³„ì‚°ì„ ìœ„í•œ 3ë‹¨ê³„ ê²€ì¦ ì‹œì‘ ---
            
            # 3-1ë‹¨ê³„: FFT ê¸°ë°˜ ì‹ í˜¸ í’ˆì§ˆ(SNR) ê²€ì¦
            peak_power = fft_power[peak_freq_index]
            noise_power = np.mean(np.delete(fft_power, peak_freq_index)) if len(fft_power) > 1 else 1
            snr = peak_power / noise_power if noise_power > 0 else 0
            
            signal_quality = 0.6  # ê¸°ë³¸ì ìœ¼ë¡œ HRì€ ê³„ì‚°ë˜ì—ˆë‹¤ê³  ê°€ì •
            hrv_ms = 0.0

            if snr < self.config.rppg.snr_threshold:
                # SNRì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì‹ í˜¸ í’ˆì§ˆì´ ë‚˜ë¹  HRV ê³„ì‚°ì„ ì‹œë„í•˜ì§€ ì•ŠìŒ
                signal_quality = 0.3 # ì‹ í˜¸ í’ˆì§ˆ ë‚®ìŒ
                logger.warning(f"rPPG SNRì´ ì„ê³„ê°’ ë¯¸ë§Œ({snr:.2f} < {self.config.rppg.snr_threshold}). HRV ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            else:
                # 3-2ë‹¨ê³„: ì‹ ë¢°ë„ ë†’ì€ í”¼í¬ íƒì§€ (prominence ì‚¬ìš©)
                prominence_threshold = np.std(filtered_signal) * 0.4
                ibi_peaks, _ = find_peaks(filtered_signal, height=0, distance=fs*0.5, prominence=prominence_threshold)
                
                if len(ibi_peaks) > 3: # ìµœì†Œ 3ê°œ ì´ìƒì˜ IBIê°€ ìˆì–´ì•¼ HRVê°€ ì˜ë¯¸ ìˆìŒ
                    # 3-3ë‹¨ê³„: IBI í•©ë¦¬ì„± ê²€ì¦
                    ibi_s = np.diff(timestamps[ibi_peaks])  # ì´ˆ ë‹¨ìœ„
                    
                    # ë¹„ì •ìƒì ì¸ IBI ì œê±° (ì˜ˆ: 2ì´ˆ ì´ìƒ ë˜ëŠ” 0.3ì´ˆ ë¯¸ë§Œ)
                    valid_ibi_mask = (ibi_s > 0.3) & (ibi_s < 2.0)
                    ibi_ms_valid = ibi_s[valid_ibi_mask] * 1000

                    if len(ibi_ms_valid) > 2:
                        hrv_std = np.std(ibi_ms_valid)
                        # IBIì˜ í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ í¬ë©´ ë…¸ì´ì¦ˆë¡œ ê°„ì£¼í•˜ê³  íê¸°
                        if hrv_std < self.config.rppg.hrv_std_threshold:
                            hrv_ms = hrv_std
                            signal_quality = 0.9 # HRVê¹Œì§€ ì„±ê³µì ìœ¼ë¡œ ê³„ì‚°ë¨
                        else:
                            logger.warning(f"HRV í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤({hrv_std:.2f}ms). ë…¸ì´ì¦ˆë¡œ ê°„ì£¼í•˜ì—¬ íê¸°í•©ë‹ˆë‹¤.")
                            signal_quality = 0.5 # HRì€ ì‹ ë¢°í•˜ì§€ë§Œ HRVëŠ” íê¸°
                    else:
                        signal_quality = 0.5 # ìœ íš¨í•œ IBIê°€ ë¶€ì¡±
                else:
                    signal_quality = 0.4 # í”¼í¬ ê²€ì¶œ ì‹¤íŒ¨

            return {
                'estimated_hr_bpm': hr_bpm,
                'estimated_hrv_ms': hrv_ms,
                'signal_quality': signal_quality
            }

        except Exception as e:
            logger.error(f"rPPG ì‹¬ë°•ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {'estimated_hr_bpm': 0, 'estimated_hrv_ms': 0, 'signal_quality': 0.1}

    def _extract_euler_angles_from_matrix(self, transform_matrix: Any) -> Dict[str, float]:
        """ 3D ë³€í™˜ í–‰ë ¬ì—ì„œ ì˜¤ì¼ëŸ¬ ê°ë„ ì¶”ì¶œ """
        try:
            R = np.array(transform_matrix).reshape(4, 4)[:3, :3]
            sy = math.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)
            
            if sy > MathConstants.EPSILON:
                x = math.atan2(R[2, 1], R[2, 2])
                y = math.atan2(-R[2, 0], sy)
                z = math.atan2(R[1, 0], R[0, 0])
            else:
                x = math.atan2(-R[1, 2], R[1, 1])
                y = math.atan2(-R[2, 0], sy)
                z = 0
            
            return {'yaw': -math.degrees(y), 'pitch': -math.degrees(x), 'roll': math.degrees(z)}
        except Exception as e:
            logger.error(f"ì˜¤ì¼ëŸ¬ ê°ë„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {'yaw': 0.0, 'pitch': 0.0, 'roll': 0.0}

    def _stabilize_head_pose(self, raw_pose: Dict[str, float]) -> Dict[str, float]:
        """ [ê³ ë„í™”] EMA í•„í„°ë¥¼ ì´ìš©í•œ ë¨¸ë¦¬ ìì„¸ ì•ˆì •í™” """
        if not self.head_pose_history:
            stable_pose = raw_pose
        else:
            last_pose = self.head_pose_history[-1]
            stable_pose = {
                'yaw': self.ema_alpha * raw_pose['yaw'] + (1 - self.ema_alpha) * last_pose['yaw'],
                'pitch': self.ema_alpha * raw_pose['pitch'] + (1 - self.ema_alpha) * last_pose['pitch'],
                'roll': self.ema_alpha * raw_pose['roll'] + (1 - self.ema_alpha) * last_pose['roll'],
            }
        self.head_pose_history.append(stable_pose)
        return stable_pose

    def _update_gaze_zone_tracking(self, new_gaze_zone: GazeZone) -> float:
        """ ì‹œì„  êµ¬ì—­ ë³€ê²½ ì¶”ì  ë° ì§€ì† ì‹œê°„ ê³„ì‚° """
        current_time = time.time()
        if new_gaze_zone != self.current_gaze_zone:
            self.current_gaze_zone = new_gaze_zone
            self.gaze_zone_start_time = current_time
            return 0.0
        else:
            return current_time - self.gaze_zone_start_time

    def _calculate_gaze_deviation_score(
        self, head_pose: Dict[str, float], gaze_zone: GazeZone,
        zone_duration: float, gaze_stability: float
    ) -> float:
        """ [ê³ ë„í™”] ì‹œì„  í¸ì°¨ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° """
        yaw_score = min(1.0, abs(head_pose['yaw']) / AnalysisConstants.Thresholds.HEAD_YAW_EXTREME)
        pitch_score = min(1.0, abs(head_pose['pitch']) / AnalysisConstants.Thresholds.HEAD_PITCH_LIMIT)
        base_angle_score = max(yaw_score, pitch_score)
        
        instability_penalty = (1.0 - gaze_stability) * 0.3
        
        zone_risk = AnalysisConstants.GazeZoneRisk.get(gaze_zone, 0.5)
        duration_factor = min(1.0, zone_duration / 3.0)
        
        final_score = (base_angle_score * 0.4) + (zone_risk * duration_factor * 0.4) + (instability_penalty * 0.2)
        return min(1.0, final_score)

    def _update_all_metrics(self, results: Dict[str, Any]):
        """ ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¤‘ì•™ ë©”íŠ¸ë¦­ ê´€ë¦¬ìì— ì—…ë°ì´íŠ¸ """
        if 'drowsiness' in results: self.metrics_updater.update_drowsiness_metrics(results['drowsiness'])
        if 'emotion' in results: self.metrics_updater.update_emotion_metrics(results['emotion'])
        if 'gaze' in results: self.metrics_updater.update_gaze_metrics(results['gaze'])
        # S-Class ì „ìš© ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if hasattr(self.metrics_updater, 'update_saccade_metrics') and 'saccade' in results:
            self.metrics_updater.update_saccade_metrics(results['saccade'])
        if hasattr(self.metrics_updater, 'update_pupil_metrics') and 'pupil' in results:
            self.metrics_updater.update_pupil_metrics(results['pupil'])
        if hasattr(self.metrics_updater, 'update_rppg_metrics') and 'rppg' in results:
            self.metrics_updater.update_rppg_metrics(results['rppg'])

    async def _handle_no_face_detected(self) -> Dict[str, Any]:
        """ ì–¼êµ´ ë¯¸ê°ì§€ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ """
        logger.warning("ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ - ëª¨ë“  ì–¼êµ´ ê´€ë ¨ ì§€í‘œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
        default_gaze = self._get_default_pose_gaze_data()['gaze']
        return {
            'face_detected': False,
            'drowsiness': {'status': 'no_face', 'confidence': 0.0},
            'emotion': {'state': None, 'confidence': 0.0},
            'gaze': default_gaze,
            'driver': {'identity': 'unknown', 'confidence': 0.0},
            'saccade': {'saccade_velocity_norm': 0.0, 'saccade_count_per_s': 0.0, 'gaze_fixation_stability': 0.0},
            'pupil': {'estimated_pupil_diameter': 0.0, 'pupil_variability': 0.0, 'cognitive_load_indicator': 0.0},
            'rppg': {'estimated_hr_bpm': 0, 'estimated_hrv_ms': 0, 'signal_quality': 0.0}
        }

    def _get_default_pose_gaze_data(self) -> Dict[str, Any]:
        """ ê¸°ë³¸ ìì„¸ ë° ì‹œì„  ë°ì´í„° """
        return {
            'gaze': {
                'head_yaw': 0.0, 'head_pitch': 0.0, 'head_roll': 0.0,
                'current_zone': GazeZone.FRONT, 'zone_duration': 0.0,
                'stability': 1.0, 'attention_focus': 1.0, 'deviation_score': 0.0
            }
        }