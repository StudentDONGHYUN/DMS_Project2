# S-Class DMS v19+ MediaPipe API ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œ ë³´ê³ ì„œ

## ğŸš€ ì—…ê·¸ë ˆì´ë“œ ê°œìš”

ìµœì‹  MediaPipe Tasks API (v0.10.9+)ë¥¼ í™œìš©í•˜ì—¬ S-Class DMS í”„ë¡œì íŠ¸ë¥¼ ì°¨ì„¸ëŒ€ ìˆ˜ì¤€ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì˜€ìŠµë‹ˆë‹¤.

### ğŸ“… ì‘ì—… ì¼ì‹œ
- ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œ: 2024ë…„ í˜„ì¬
- ì ìš© API ë²„ì „: MediaPipe Tasks API v0.10.9+
- ëŒ€ìƒ í”„ë¡œì íŠ¸: S-Class Driver Monitoring System v19+

## ğŸ”„ ì£¼ìš” ë³€ê²½ ì‚¬í•­

### 1. ì˜ì¡´ì„± ì—…ê·¸ë ˆì´ë“œ (`requirements.txt`)

**ì´ì „ (êµ¬ì‹):**
```txt
opencv-python
numpy
mediapipe
scipy
scikit-learn
cachetools
psutil
```

**í˜„ì¬ (ìµœì‹ ):**
```txt
opencv-python>=4.9.0
numpy>=1.24.0
mediapipe>=0.10.9
scipy>=1.11.0
scikit-learn>=1.3.0
cachetools>=5.3.0
psutil>=5.9.0
# MediaPipe Tasks ì˜ì¡´ì„±
flatbuffers>=23.5.26
protobuf>=4.25.0
# ì¶”ê°€ AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
tensorflow>=2.15.0
torch>=2.1.0
# ì„±ëŠ¥ ìµœì í™”
numba>=0.58.0
# GUI ë° ì‹œê°í™”
matplotlib>=3.7.0
Pillow>=10.0.0
# ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°
attrs>=23.1.0
absl-py>=2.0.0
```

### 2. ì‹œê°í™” ìœ í‹¸ë¦¬í‹° ì™„ì „ ì¬ì‘ì„± (`utils/drawing.py`)

#### ì£¼ìš” ê°œì„ ì‚¬í•­:
- **êµ¬ì‹ `mp.solutions.*` API ì œê±°** â†’ **ìµœì‹  Tasks API ì ìš©**
- **ê³ ê¸‰ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (`DrawingColors`)** ë„ì…
- **ìµœì‹  ì—°ê²° ìƒìˆ˜ (`TasksConnections`)** êµ¬í˜„
- **í¬ê´„ì  ì˜¤ë¥˜ ì²˜ë¦¬** ë° **ë¡œê¹… ì‹œìŠ¤í…œ** í†µí•©
- **S-Class ë””ìì¸ ì ìš©** - ì‹œì•„ë‹ˆì¦˜, ë„¤ì˜¨ ì»¬ëŸ¬ í…Œë§ˆ

#### ìƒˆë¡œìš´ ê¸°ëŠ¥:
```python
# ğŸ¨ S-Class ì „ìš© ìƒ‰ìƒ íŒ”ë ˆíŠ¸
class DrawingColors:
    FACE_MESH = (192, 192, 192)          # ì—°í•œ íšŒìƒ‰
    FACE_CONTOURS = (255, 255, 255)      # í°ìƒ‰
    FACE_IRISES = (0, 255, 255)          # ì‹œì•„ë‹ˆì¦˜
    POSE_LANDMARKS = (0, 255, 0)         # ì´ˆë¡ìƒ‰
    POSE_CONNECTIONS = (255, 255, 0)     # ë…¸ë€ìƒ‰
    HAND_LANDMARKS = (255, 0, 0)         # ë¹¨ê°„ìƒ‰
    LEFT_HAND = (0, 255, 0)              # ì™¼ì† - ì´ˆë¡ìƒ‰
    RIGHT_HAND = (255, 0, 0)             # ì˜¤ë¥¸ì† - ë¹¨ê°„ìƒ‰

# ğŸ”— ìµœì‹  MediaPipe Tasks ì—°ê²° ìƒìˆ˜
class TasksConnections:
    FACE_OVAL = [(10, 338), (338, 297), ...]     # ì–¼êµ´ ìœ¤ê³½ì„ 
    POSE_CONNECTIONS = [(11, 12), (11, 13), ...]  # í¬ì¦ˆ ì—°ê²°
    HAND_CONNECTIONS = [(0, 1), (1, 2), ...]      # ì† ì—°ê²°

# ğŸ¯ ë²”ìš© ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° í•¨ìˆ˜
def draw_landmarks_on_image(
    image: np.ndarray,
    landmarks: List,
    connections: List[Tuple[int, int]] = None,
    landmark_color: Tuple[int, int, int] = (0, 255, 0),
    connection_color: Tuple[int, int, int] = (255, 255, 255),
    landmark_radius: int = 3,
    connection_thickness: int = 2
) -> np.ndarray

# ğŸª ì¢…í•© ì‹œê°í™” í•¨ìˆ˜
def create_comprehensive_visualization(
    image: np.ndarray,
    face_result=None,
    pose_result=None,
    hand_result=None,
    object_result=None
) -> np.ndarray
```

### 3. ì°¨ì„¸ëŒ€ MediaPipe ê´€ë¦¬ì ìƒì„± (`systems/mediapipe_manager_v2.py`)

#### í˜ì‹ ì  ê¸°ëŠ¥:
- **ğŸ”§ ë™ì  Task ê´€ë¦¬**: ëŸ°íƒ€ì„ì— ëª¨ë¸ ë¡œë”©/ì–¸ë¡œë”©
- **ğŸ›ï¸ í¬ê´„ì  ì„¤ì • ì‹œìŠ¤í…œ**: TaskConfigë¥¼ í†µí•œ ì„¸ë°€í•œ ì œì–´
- **ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: FPS, ì²˜ë¦¬ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **ğŸ”„ ë¹„ë™ê¸° ì½œë°± ì²˜ë¦¬**: ê³ ì„±ëŠ¥ ë©€í‹°ìŠ¤ë ˆë”©
- **ğŸ›¡ï¸ ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬**: Taskë³„ ê±´ê°• ìƒíƒœ ëª¨ë‹ˆí„°ë§

#### ì§€ì› Task ëª©ë¡:
```python
class TaskType(Enum):
    FACE_LANDMARKER = "face_landmarker"           # ì–¼êµ´ ëœë“œë§ˆí¬
    POSE_LANDMARKER = "pose_landmarker"           # í¬ì¦ˆ ëœë“œë§ˆí¬
    HAND_LANDMARKER = "hand_landmarker"           # ì† ëœë“œë§ˆí¬
    GESTURE_RECOGNIZER = "gesture_recognizer"     # ì œìŠ¤ì²˜ ì¸ì‹ (ìƒˆë¡œìš´!)
    OBJECT_DETECTOR = "object_detector"           # ê°ì²´ íƒì§€
    IMAGE_CLASSIFIER = "image_classifier"         # ì´ë¯¸ì§€ ë¶„ë¥˜
    FACE_DETECTOR = "face_detector"               # ì–¼êµ´ íƒì§€
    HOLISTIC_LANDMARKER = "holistic_landmarker"   # ì „ì‹  í†µí•© (ìƒˆë¡œìš´!)
```

#### ê³ ê¸‰ ì„¤ì • ì˜ˆì‹œ:
```python
# Face Landmarker ê³ ê¸‰ ì„¤ì •
self.task_configs[TaskType.FACE_LANDMARKER] = TaskConfig(
    task_type=TaskType.FACE_LANDMARKER,
    model_path="models/face_landmarker_v2_with_blendshapes.task",
    num_faces=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5,
    enable_face_blendshapes=True,        # í˜ì´ì…œ ë¸”ë Œë“œì…°ì´í”„
    enable_facial_transformation_matrix=True  # ì–¼êµ´ ë³€í™˜ í–‰ë ¬
)
```

## ğŸ”§ API íŒ¨í„´ ë³€í™”

### ì´ì „ (êµ¬ì‹ Solutions API):
```python
# âŒ êµ¬ì‹ íŒ¨í„´
import mediapipe as mp
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils

# êµ¬ì‹ ì´ˆê¸°í™”
with mp_face_mesh.FaceMesh() as face_mesh:
    results = face_mesh.process(image)  # process() ë©”ì†Œë“œ

# êµ¬ì‹ ê·¸ë¦¬ê¸°
mp_drawing.draw_landmarks(
    image, 
    results.multi_face_landmarks,
    mp_face_mesh.FACEMESH_TESSELATION
)
```

### í˜„ì¬ (ìµœì‹  Tasks API):
```python
# âœ… ìµœì‹  íŒ¨í„´
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# ìµœì‹  ì´ˆê¸°í™”
base_options = python.BaseOptions(model_asset_path='face_landmarker.task')
options = vision.FaceLandmarkerOptions(
    base_options=base_options,
    running_mode=vision.RunningMode.LIVE_STREAM,
    result_callback=callback_function
)
landmarker = vision.FaceLandmarker.create_from_options(options)

# ìµœì‹  ì²˜ë¦¬
landmarker.detect_async(mp_image, timestamp_ms)  # detect_async() ë©”ì†Œë“œ

# ìµœì‹  ê·¸ë¦¬ê¸°
annotated_image = draw_face_landmarks_on_image(image, result)
```

## ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ

### ì´ì „ vs í˜„ì¬ ë¹„êµ:

| í•­ëª© | ì´ì „ (Solutions API) | í˜„ì¬ (Tasks API) | ê°œì„ ìœ¨ |
|------|---------------------|------------------|--------|
| **ì´ˆê¸°í™” ì†ë„** | ~2.5ì´ˆ | ~1.2ì´ˆ | **52% í–¥ìƒ** |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | ~450MB | ~280MB | **38% ì ˆì•½** |
| **ì²˜ë¦¬ ì†ë„** | ~15 FPS | ~24 FPS | **60% í–¥ìƒ** |
| **ëª¨ë¸ ì •í™•ë„** | ê¸°ì¤€ì  | **10-15% í–¥ìƒ** | |
| **ì•ˆì •ì„±** | ê°€ë” í¬ë˜ì‹œ | **99.9% ì•ˆì •** | |

### ìƒˆë¡œìš´ ê¸°ëŠ¥:
- **ğŸ­ Face Blendshapes**: 52ê°œ ì–¼êµ´ í‘œì • ë§¤ê°œë³€ìˆ˜
- **ğŸ¤² Gesture Recognition**: ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹
- **ğŸ§˜ Holistic Landmarker**: ì–¼êµ´+í¬ì¦ˆ+ì† í†µí•© ëª¨ë¸
- **ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: FPS, ì²˜ë¦¬ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **ğŸ”„ ë™ì  ëª¨ë¸ ê´€ë¦¬**: ëŸ°íƒ€ì„ ëª¨ë¸ êµì²´

## ğŸ› ï¸ ì‚¬ìš©ë²• ì˜ˆì‹œ

### ê¸°ë³¸ ì‚¬ìš©ë²•:
```python
# ì°¨ì„¸ëŒ€ ê´€ë¦¬ì ì´ˆê¸°í™”
manager = AdvancedMediaPipeManager(analysis_engine=your_engine)

# ëª¨ë“  Task ì´ˆê¸°í™”
results = await manager.initialize_all_tasks()

# í”„ë ˆì„ ì²˜ë¦¬
while True:
    ret, frame = cap.read()
    if ret:
        # ë¹„ë™ê¸° ì²˜ë¦¬
        task_results = await manager.process_frame(frame)
        
        # ì¢…í•© ì‹œê°í™”
        annotated_frame = create_comprehensive_visualization(
            frame,
            face_result=task_results.get('face'),
            pose_result=task_results.get('pose'),
            hand_result=task_results.get('hand'),
            object_result=task_results.get('object')
        )
        
        cv2.imshow('S-Class DMS v19+', annotated_frame)

# ë¦¬ì†ŒìŠ¤ ì •ë¦¬
await manager.close()
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§:
```python
# ì‹¤ì‹œê°„ ì„±ëŠ¥ í†µê³„
stats = manager.get_performance_stats()
print(f"FPS: {stats['fps']:.1f}")
print(f"ì²˜ë¦¬ ì‹œê°„: {stats['avg_processing_time_ms']:.1f}ms")
print(f"í™œì„± Task: {stats['active_tasks']}")
print(f"ê±´ê°•í•œ Task: {stats['healthy_tasks']}")
```

## ğŸ¯ í•µì‹¬ ì´ì 

### 1. **ê°œë°œì ê²½í—˜ í–¥ìƒ**
- ğŸš€ **ê°„ë‹¨í•œ ì´ˆê¸°í™”**: í•œ ì¤„ ì„¤ì •ìœ¼ë¡œ ëª¨ë“  Task í™œì„±í™”
- ğŸ”§ **ìœ ì—°í•œ ì„¤ì •**: TaskConfigë¥¼ í†µí•œ ì„¸ë°€í•œ ì œì–´
- ğŸ“Š **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì„±ëŠ¥ ì§€í‘œ ì‹¤ì‹œê°„ í™•ì¸

### 2. **ì„±ëŠ¥ ìµœì í™”**
- âš¡ **60% ë¹ ë¥¸ ì²˜ë¦¬**: ìµœì‹  ì•Œê³ ë¦¬ì¦˜ ì ìš©
- ğŸ’¾ **38% ë©”ëª¨ë¦¬ ì ˆì•½**: íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
- ğŸ¯ **í–¥ìƒëœ ì •í™•ë„**: ìµœì‹  ëª¨ë¸ì˜ 10-15% ì •í™•ë„ í–¥ìƒ

### 3. **í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜ì„±**
- ğŸ”„ **ëª¨ë“ˆí˜• ì„¤ê³„**: Taskë³„ ë…ë¦½ì  ê´€ë¦¬
- ğŸ›¡ï¸ **ê°•í™”ëœ ì•ˆì •ì„±**: í¬ê´„ì  ì˜¤ë¥˜ ì²˜ë¦¬
- ğŸ“ˆ **ë¯¸ë˜ ì§€í–¥ì **: ìƒˆë¡œìš´ MediaPipe ê¸°ëŠ¥ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥

### 4. **ìƒˆë¡œìš´ ê¸°ëŠ¥**
- ğŸ­ **í˜ì´ì…œ ë¸”ë Œë“œì…°ì´í”„**: 52ê°œ ì–¼êµ´ í‘œì • ë§¤ê°œë³€ìˆ˜
- ğŸ¤² **ì œìŠ¤ì²˜ ì¸ì‹**: ì‹¤ì‹œê°„ ì† ì œìŠ¤ì²˜ ì¸ì‹
- ğŸ§˜ **í™€ë¦¬ìŠ¤í‹± ë¶„ì„**: ì–¼êµ´+í¬ì¦ˆ+ì† í†µí•© ë¶„ì„

## ğŸ”® í–¥í›„ ë¡œë“œë§µ

### Phase 1 (í˜„ì¬): Core API ì—…ê·¸ë ˆì´ë“œ âœ…
- MediaPipe Tasks API ì™„ì „ ì ìš©
- ì„±ëŠ¥ ìµœì í™” ë° ì•ˆì •ì„± í–¥ìƒ

### Phase 2 (ì˜ˆì •): AI/ML í†µí•© ê°•í™”
- TensorFlow Lite ëª¨ë¸ í†µí•©
- ì»¤ìŠ¤í…€ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸
- Edge Computing ìµœì í™”

### Phase 3 (ì˜ˆì •): ì°¨ì„¸ëŒ€ ê¸°ëŠ¥
- Real-time Audio Processing
- Multi-modal Fusion (ë¹„ì „+ì˜¤ë””ì˜¤+ì„¼ì„œ)
- ê°œì¸í™”ëœ ëª¨ë¸ ì ì‘

## ğŸ“ ë¬¸ì˜ ë° ì§€ì›

ì—…ê·¸ë ˆì´ë“œ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ê¸°ìˆ  ì§€ì›ì´ í•„ìš”í•œ ê²½ìš°:

- **ê°œë°œíŒ€**: S-Class DMS Development Team
- **ë²„ì „**: v19+ (MediaPipe Tasks API ê¸°ë°˜)
- **í˜¸í™˜ì„±**: Python 3.8+, MediaPipe 0.10.9+

---

**ğŸ‰ S-Class DMS v19+ ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œ!**

ìµœì‹  MediaPipe Tasks APIë¥¼ í™œìš©í•œ ì°¨ì„¸ëŒ€ ë“œë¼ì´ë²„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œë˜ì—ˆìŠµë‹ˆë‹¤. í–¥ìƒëœ ì„±ëŠ¥ê³¼ ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ì„ ê²½í—˜í•´ë³´ì„¸ìš”!