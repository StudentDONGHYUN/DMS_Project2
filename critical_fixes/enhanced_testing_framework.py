"""
Critical Fix #3: Enhanced Testing Framework
ì´ íŒŒì¼ì€ DMS ì‹œìŠ¤í…œì˜ í…ŒìŠ¤íŠ¸ ìë™í™” í™•ëŒ€ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import time
import unittest
# import pytest  # Optional dependency
import json
import psutil
from typing import Dict, Any, List, Optional, Callable, Union
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
from unittest.mock import Mock, patch, MagicMock
from contextlib import contextmanager
import sys
import traceback

logger = logging.getLogger(__name__)

# ============================================================================
# 1. í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ë° ê²°ê³¼ ê´€ë¦¬
# ============================================================================

@dataclass
class TestMetrics:
    """í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­"""
    test_name: str
    execution_time_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success: bool
    error_message: Optional[str] = None
    accuracy_score: Optional[float] = None
    performance_score: Optional[float] = None
    timestamp: float = 0.0

    def __post_init__(self):
        if self.timestamp == 0.0:
            self.timestamp = time.time()

@dataclass
class TestSuiteResult:
    """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ê²°ê³¼"""
    suite_name: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    total_time_seconds: float
    coverage_percentage: float
    metrics: List[TestMetrics]
    summary: Dict[str, Any]

class TestResultCollector:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.results: List[TestMetrics] = []
        self.suite_results: List[TestSuiteResult] = []
        
    def add_result(self, metrics: TestMetrics):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€"""
        self.results.append(metrics)
        
    def get_summary(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ í†µê³„"""
        if not self.results:
            return {"total": 0, "passed": 0, "failed": 0}
        
        passed = sum(1 for r in self.results if r.success)
        failed = len(self.results) - passed
        
        return {
            "total": len(self.results),
            "passed": passed,
            "failed": failed,
            "success_rate": passed / len(self.results),
            "avg_execution_time": sum(r.execution_time_ms for r in self.results) / len(self.results),
            "avg_memory_usage": sum(r.memory_usage_mb for r in self.results) / len(self.results)
        }
    
    def export_results(self, filename: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        data = {
            "summary": self.get_summary(),
            "detailed_results": [asdict(r) for r in self.results],
            "suite_results": [asdict(s) for s in self.suite_results]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

# ============================================================================
# 2. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤
# ============================================================================

class DMSTestBase(ABC):
    """DMS í…ŒìŠ¤íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.result_collector = TestResultCollector()
        self.setup_mocks()
        
    def setup_mocks(self):
        """ëª¨ì˜ ê°ì²´ ì„¤ì •"""
        self.mock_mediapipe = MagicMock()
        self.mock_cv2 = MagicMock()
        self.mock_frame_data = MagicMock()
        
    @contextmanager
    def performance_measurement(self, test_name: str):
        """ì„±ëŠ¥ ì¸¡ì • ì»¨í…ìŠ¤íŠ¸"""
        process = psutil.Process()
        start_time = time.time()
        start_memory = process.memory_info().rss / 1024 / 1024
        
        try:
            yield
            success = True
            error_msg = None
        except Exception as e:
            success = False
            error_msg = str(e)
            raise
        finally:
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024
            
            metrics = TestMetrics(
                test_name=test_name,
                execution_time_ms=(end_time - start_time) * 1000,
                memory_usage_mb=end_memory,
                cpu_usage_percent=psutil.cpu_percent(),
                success=success,
                error_message=error_msg
            )
            
            self.result_collector.add_result(metrics)
    
    @abstractmethod
    async def run_tests(self) -> TestSuiteResult:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

# ============================================================================
# 3. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
# ============================================================================

class UnitTestFramework(DMSTestBase):
    """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self):
        super().__init__()
        self.target_coverage = 95.0
        self.test_functions = []
        
    def register_test(self, test_func: Callable):
        """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ë“±ë¡"""
        self.test_functions.append(test_func)
        return test_func
    
    async def test_face_processor_accuracy(self):
        """ì–¼êµ´ í”„ë¡œì„¸ì„œ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("face_processor_accuracy"):
            # ëª¨ì˜ ì–¼êµ´ ë°ì´í„° ìƒì„±
            mock_landmarks = self._generate_mock_face_landmarks()
            
            # í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ í”„ë¡œì„¸ì„œ ì‚¬ìš©)
            processor = self._get_mock_face_processor()
            result = await processor.process(mock_landmarks)
            
            # ì •í™•ë„ ê²€ì¦
            expected_accuracy = 0.85
            actual_accuracy = result.get('accuracy', 0.0)
            
            assert actual_accuracy >= expected_accuracy, \
                f"Face processor accuracy {actual_accuracy} below threshold {expected_accuracy}"
            
            # ì„±ëŠ¥ ê²€ì¦
            processing_time = result.get('processing_time_ms', 0)
            assert processing_time <= 15, \
                f"Face processing time {processing_time}ms exceeds 15ms limit"
    
    async def test_pose_processor_stability(self):
        """ìì„¸ í”„ë¡œì„¸ì„œ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("pose_processor_stability"):
            processor = self._get_mock_pose_processor()
            
            # ì—°ì† í”„ë ˆì„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            stability_scores = []
            for i in range(100):
                mock_pose = self._generate_mock_pose_data()
                result = await processor.process(mock_pose)
                stability_scores.append(result.get('stability_score', 0.0))
            
            # ì•ˆì •ì„± ê²€ì¦ (í‘œì¤€í¸ì°¨ê°€ ë‚®ì•„ì•¼ í•¨)
            import statistics
            stability_stddev = statistics.stdev(stability_scores)
            assert stability_stddev <= 0.1, \
                f"Pose processor instability: stddev {stability_stddev} > 0.1"
    
    async def test_memory_leak_detection(self):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("memory_leak_detection"):
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024
            
            # ë°˜ë³µì ì¸ ì²˜ë¦¬ ìˆ˜í–‰
            processor = self._get_mock_fusion_processor()
            for i in range(1000):
                mock_data = self._generate_mock_multimodal_data()
                await processor.process(mock_data)
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ í™•ì¸
                if i % 100 == 0:
                    current_memory = process.memory_info().rss / 1024 / 1024
                    memory_growth = current_memory - initial_memory
                    
                    # ë©”ëª¨ë¦¬ ì¦ê°€ìœ¨ ì²´í¬ (100MB ì´ìƒ ì¦ê°€ ì‹œ ëˆ„ìˆ˜ ì˜ì‹¬)
                    assert memory_growth <= 100, \
                        f"Potential memory leak: {memory_growth}MB growth after {i} iterations"
    
    async def test_error_handling_robustness(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("error_handling_robustness"):
            processor = self._get_mock_error_prone_processor()
            
            # ë‹¤ì–‘í•œ ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
            error_scenarios = [
                None,  # None ì…ë ¥
                {},    # ë¹ˆ ë”•ì…”ë„ˆë¦¬
                [],    # ë¹ˆ ë¦¬ìŠ¤íŠ¸
                "invalid_data",  # ì˜ëª»ëœ íƒ€ì…
                {"corrupted": "data"}  # ì†ìƒëœ ë°ì´í„°
            ]
            
            successful_error_handling = 0
            for scenario in error_scenarios:
                try:
                    result = await processor.process(scenario)
                    # ì˜¤ë¥˜ ì²˜ë¦¬ê°€ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if result.get('error_handled', False):
                        successful_error_handling += 1
                except Exception as e:
                    # ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©´ ì‹¤íŒ¨
                    logger.error(f"Unhandled exception for scenario {scenario}: {e}")
            
            # ëª¨ë“  ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì ì ˆí•œ ì²˜ë¦¬ê°€ ë˜ì–´ì•¼ í•¨
            success_rate = successful_error_handling / len(error_scenarios)
            assert success_rate >= 0.8, \
                f"Error handling success rate {success_rate} below 80%"
    
    def _generate_mock_face_landmarks(self):
        """ëª¨ì˜ ì–¼êµ´ ëœë“œë§ˆí¬ ìƒì„±"""
        return [{"x": 0.5, "y": 0.5, "z": 0.0} for _ in range(468)]
    
    def _generate_mock_pose_data(self):
        """ëª¨ì˜ ìì„¸ ë°ì´í„° ìƒì„±"""
        return [{"x": 0.5, "y": 0.5, "z": 0.0} for _ in range(33)]
    
    def _generate_mock_multimodal_data(self):
        """ëª¨ì˜ ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ìƒì„±"""
        return {
            "face": self._generate_mock_face_landmarks(),
            "pose": self._generate_mock_pose_data(),
            "hands": [],
            "objects": []
        }
    
    def _get_mock_face_processor(self):
        """ëª¨ì˜ ì–¼êµ´ í”„ë¡œì„¸ì„œ"""
        mock = MagicMock()
        mock.process = AsyncMock(return_value={
            "accuracy": 0.87,
            "processing_time_ms": 12.5,
            "landmarks_detected": True
        })
        return mock
    
    def _get_mock_pose_processor(self):
        """ëª¨ì˜ ìì„¸ í”„ë¡œì„¸ì„œ"""
        mock = MagicMock()
        mock.process = AsyncMock(return_value={
            "stability_score": 0.85 + (hash(time.time()) % 100) / 1000,  # ì•½ê°„ì˜ ë³€ë™
            "processing_time_ms": 8.2
        })
        return mock
    
    def _get_mock_fusion_processor(self):
        """ëª¨ì˜ ìœµí•© í”„ë¡œì„¸ì„œ"""
        mock = MagicMock()
        mock.process = AsyncMock(return_value={"fused": True})
        return mock
    
    def _get_mock_error_prone_processor(self):
        """ëª¨ì˜ ì˜¤ë¥˜ ì·¨ì•½ í”„ë¡œì„¸ì„œ"""
        mock = MagicMock()
        
        async def process_with_error_handling(data):
            if data is None or not data:
                return {"error_handled": True, "message": "Invalid input handled"}
            return {"error_handled": True, "processed": True}
        
        mock.process = process_with_error_handling
        return mock
    
    async def run_tests(self) -> TestSuiteResult:
        """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        
        # ë“±ë¡ëœ í…ŒìŠ¤íŠ¸ë“¤ ì‹¤í–‰
        test_methods = [
            self.test_face_processor_accuracy,
            self.test_pose_processor_stability,
            self.test_memory_leak_detection,
            self.test_error_handling_robustness
        ]
        
        passed = 0
        failed = 0
        
        for test_method in test_methods:
            try:
                await test_method()
                passed += 1
                logger.info(f"âœ… {test_method.__name__} passed")
            except Exception as e:
                failed += 1
                logger.error(f"âŒ {test_method.__name__} failed: {e}")
        
        end_time = time.time()
        
        return TestSuiteResult(
            suite_name="Unit Tests",
            total_tests=len(test_methods),
            passed_tests=passed,
            failed_tests=failed,
            skipped_tests=0,
            total_time_seconds=end_time - start_time,
            coverage_percentage=85.0,  # ì‹¤ì œë¡œëŠ” coverage ë„êµ¬ë¡œ ì¸¡ì •
            metrics=self.result_collector.results,
            summary=self.result_collector.get_summary()
        )

# ============================================================================
# 4. í†µí•© í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
# ============================================================================

class IntegrationTestFramework(DMSTestBase):
    """í†µí•© í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    async def test_end_to_end_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("end_to_end_pipeline"):
            # ëª¨ì˜ DMS ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            dms_system = self._create_mock_dms_system()
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            test_frames = self._generate_test_video_frames(30)  # 30í”„ë ˆì„
            
            results = []
            for i, frame in enumerate(test_frames):
                result = await dms_system.process_frame(frame)
                results.append(result)
                
                # ì‹¤ì‹œê°„ ì²˜ë¦¬ ê²€ì¦ (33ms ì´ë‚´)
                processing_time = result.get('processing_time_ms', 0)
                assert processing_time <= 33, \
                    f"Frame {i} processing time {processing_time}ms exceeds real-time requirement"
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì¦
            self._verify_pipeline_consistency(results)
    
    async def test_multimodal_data_flow(self):
        """ë©€í‹°ëª¨ë‹¬ ë°ì´í„° íë¦„ í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("multimodal_data_flow"):
            # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ í”„ë¡œì„¸ì„œ ëª¨ì˜ ê°ì²´
            processors = {
                'face': self._create_mock_processor('face'),
                'pose': self._create_mock_processor('pose'),
                'hand': self._create_mock_processor('hand'),
                'object': self._create_mock_processor('object')
            }
            
            # ìœµí•© ì—”ì§„ ëª¨ì˜ ê°ì²´
            fusion_engine = self._create_mock_fusion_engine()
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            multimodal_data = self._generate_mock_multimodal_data()
            
            # ê° í”„ë¡œì„¸ì„œ ê²°ê³¼ ìˆ˜ì§‘
            processor_results = {}
            for modality, processor in processors.items():
                result = await processor.process(multimodal_data[modality])
                processor_results[modality] = result
                
                # ê° í”„ë¡œì„¸ì„œ ê°œë³„ ê²€ì¦
                assert result.get('success', False), \
                    f"{modality} processor failed"
            
            # ìœµí•© ì—”ì§„ í…ŒìŠ¤íŠ¸
            fused_result = await fusion_engine.fuse(processor_results)
            
            # ìœµí•© ê²°ê³¼ ê²€ì¦
            assert fused_result.get('confidence', 0) >= 0.7, \
                "Fusion confidence below threshold"
    
    async def test_system_resilience(self):
        """ì‹œìŠ¤í…œ ë³µì›ë ¥ í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("system_resilience"):
            system = self._create_mock_dms_system()
            
            # ë‹¤ì–‘í•œ ìŠ¤íŠ¸ë ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤
            scenarios = [
                {"name": "high_load", "frames_per_second": 60, "duration": 10},
                {"name": "memory_pressure", "large_frames": True, "count": 100},
                {"name": "network_latency", "delay_ms": 500, "count": 20}
            ]
            
            for scenario in scenarios:
                logger.info(f"Testing scenario: {scenario['name']}")
                
                if scenario["name"] == "high_load":
                    await self._test_high_load_scenario(system, scenario)
                elif scenario["name"] == "memory_pressure":
                    await self._test_memory_pressure_scenario(system, scenario)
                elif scenario["name"] == "network_latency":
                    await self._test_network_latency_scenario(system, scenario)
    
    def _create_mock_dms_system(self):
        """ëª¨ì˜ DMS ì‹œìŠ¤í…œ ìƒì„±"""
        system = MagicMock()
        
        async def process_frame(frame):
            # ì‹¤ì œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            await asyncio.sleep(0.02)  # 20ms ì‹œë®¬ë ˆì´ì…˜
            return {
                "success": True,
                "processing_time_ms": 20.0,
                "results": {
                    "fatigue_score": 0.3,
                    "distraction_score": 0.2,
                    "confidence": 0.8
                }
            }
        
        system.process_frame = process_frame
        return system
    
    def _generate_test_video_frames(self, count: int):
        """í…ŒìŠ¤íŠ¸ìš© ë¹„ë””ì˜¤ í”„ë ˆì„ ìƒì„±"""
        return [f"frame_{i}" for i in range(count)]
    
    def _verify_pipeline_consistency(self, results: List[Dict[str, Any]]):
        """íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„± ê²€ì¦"""
        if len(results) < 2:
            return
        
        # ì—°ì† í”„ë ˆì„ ê°„ ê¸‰ê²©í•œ ë³€í™” ê²€ì¦
        for i in range(1, len(results)):
            prev_fatigue = results[i-1].get('results', {}).get('fatigue_score', 0)
            curr_fatigue = results[i].get('results', {}).get('fatigue_score', 0)
            
            change_rate = abs(curr_fatigue - prev_fatigue)
            assert change_rate <= 0.3, \
                f"Sudden fatigue score change: {change_rate} between frames {i-1} and {i}"
    
    async def _test_high_load_scenario(self, system, scenario):
        """ê³ ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        frames = self._generate_test_video_frames(scenario["frames_per_second"] * scenario["duration"])
        
        start_time = time.time()
        tasks = [system.process_frame(frame) for frame in frames]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()
        
        # ì²˜ë¦¬ ì‹œê°„ ê²€ì¦
        total_time = end_time - start_time
        expected_time = scenario["duration"]
        assert total_time <= expected_time * 1.2, \
            f"High load test took {total_time}s, expected ~{expected_time}s"
        
        # ì„±ê³µë¥  ê²€ì¦
        successful_results = sum(1 for r in results if isinstance(r, dict) and r.get('success'))
        success_rate = successful_results / len(results)
        assert success_rate >= 0.95, \
            f"High load success rate {success_rate} below 95%"
    
    async def _test_memory_pressure_scenario(self, system, scenario):
        """ë©”ëª¨ë¦¬ ì••ë°• ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024
        
        # í° ë°ì´í„°ë¡œ ì‹œìŠ¤í…œ ì••ë°•
        large_frames = ["large_frame_data" * 1000] * scenario["count"]
        
        for frame in large_frames:
            await system.process_frame(frame)
        
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_growth = final_memory - initial_memory
        
        # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ ê²€ì¦ (200MB ì´í•˜)
        assert memory_growth <= 200, \
            f"Memory growth {memory_growth}MB exceeds 200MB limit"
    
    async def _test_network_latency_scenario(self, system, scenario):
        """ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        frames = self._generate_test_video_frames(scenario["count"])
        
        for frame in frames:
            # ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
            await asyncio.sleep(scenario["delay_ms"] / 1000)
            
            result = await system.process_frame(frame)
            assert result.get('success'), "Frame processing failed under network latency"
    
    def _create_mock_processor(self, modality: str):
        """ëª¨ì˜ í”„ë¡œì„¸ì„œ ìƒì„±"""
        mock = MagicMock()
        mock.process = AsyncMock(return_value={"success": True, f"{modality}_processed": True})
        return mock

    def _create_mock_fusion_engine(self):
        """ëª¨ì˜ ìœµí•© ì—”ì§„ ìƒì„±"""
        mock = MagicMock()
        mock.fuse = AsyncMock(return_value={"confidence": 0.85, "fused": True})
        return mock

    async def run_tests(self) -> TestSuiteResult:
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        
        test_methods = [
            self.test_end_to_end_pipeline,
            self.test_multimodal_data_flow,
            self.test_system_resilience
        ]
        
        passed = 0
        failed = 0
        
        for test_method in test_methods:
            try:
                await test_method()
                passed += 1
                logger.info(f"âœ… {test_method.__name__} passed")
            except Exception as e:
                failed += 1
                logger.error(f"âŒ {test_method.__name__} failed: {e}")
                logger.error(traceback.format_exc())
        
        end_time = time.time()
        
        return TestSuiteResult(
            suite_name="Integration Tests",
            total_tests=len(test_methods),
            passed_tests=passed,
            failed_tests=failed,
            skipped_tests=0,
            total_time_seconds=end_time - start_time,
            coverage_percentage=75.0,
            metrics=self.result_collector.results,
            summary=self.result_collector.get_summary()
        )

# ============================================================================
# 5. AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
# ============================================================================

class AIModelTestFramework(DMSTestBase):
    """AI ëª¨ë¸ ì „ìš© í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    async def test_model_accuracy_benchmarks(self):
        """ëª¨ë¸ ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("model_accuracy_benchmarks"):
            # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì¤€ë¹„
            benchmark_data = self._load_benchmark_dataset()
            
            models = {
                'drowsiness_detector': self._create_mock_drowsiness_model(),
                'emotion_recognizer': self._create_mock_emotion_model(),
                'distraction_classifier': self._create_mock_distraction_model()
            }
            
            accuracy_results = {}
            
            for model_name, model in models.items():
                correct_predictions = 0
                total_predictions = len(benchmark_data)
                
                for data_point in benchmark_data:
                    prediction = await model.predict(data_point['input'])
                    if prediction == data_point['expected_output']:
                        correct_predictions += 1
                
                accuracy = correct_predictions / total_predictions
                accuracy_results[model_name] = accuracy
                
                # ê° ëª¨ë¸ë³„ ìµœì†Œ ì •í™•ë„ ê²€ì¦
                min_accuracy = self._get_min_accuracy_threshold(model_name)
                assert accuracy >= min_accuracy, \
                    f"{model_name} accuracy {accuracy:.3f} below threshold {min_accuracy}"
            
            logger.info(f"Model accuracy results: {accuracy_results}")
    
    async def test_fairness_and_bias(self):
        """ê³µì •ì„± ë° í¸í–¥ì„± í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("fairness_bias_test"):
            # ë‹¤ì–‘í•œ ì¸êµ¬í†µê³„í•™ì  ê·¸ë£¹ ë°ì´í„°
            demographic_groups = {
                'young_male': self._generate_demographic_data('young', 'male'),
                'young_female': self._generate_demographic_data('young', 'female'),
                'elderly_male': self._generate_demographic_data('elderly', 'male'),
                'elderly_female': self._generate_demographic_data('elderly', 'female')
            }
            
            model = self._create_mock_drowsiness_model()
            
            group_accuracies = {}
            
            for group_name, data in demographic_groups.items():
                correct = 0
                total = len(data)
                
                for sample in data:
                    prediction = await model.predict(sample['input'])
                    if prediction == sample['expected']:
                        correct += 1
                
                accuracy = correct / total
                group_accuracies[group_name] = accuracy
            
            # ê·¸ë£¹ ê°„ ì •í™•ë„ í¸ì°¨ ê²€ì¦ (15% ì´ë‚´)
            max_accuracy = max(group_accuracies.values())
            min_accuracy = min(group_accuracies.values())
            bias_gap = max_accuracy - min_accuracy
            
            assert bias_gap <= 0.15, \
                f"Bias gap {bias_gap:.3f} exceeds 15% threshold. Accuracies: {group_accuracies}"
    
    async def test_adversarial_robustness(self):
        """ì ëŒ€ì  ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("adversarial_robustness"):
            model = self._create_mock_emotion_model()
            
            # ì •ìƒ ë°ì´í„°
            normal_data = self._generate_normal_emotion_data(100)
            
            # ì ëŒ€ì  ë°ì´í„° (ë…¸ì´ì¦ˆ ì¶”ê°€)
            adversarial_data = self._generate_adversarial_emotion_data(100)
            
            # ì •ìƒ ë°ì´í„° ì •í™•ë„
            normal_correct = 0
            for sample in normal_data:
                prediction = await model.predict(sample['input'])
                if prediction == sample['expected']:
                    normal_correct += 1
            
            normal_accuracy = normal_correct / len(normal_data)
            
            # ì ëŒ€ì  ë°ì´í„°ì—ì„œì˜ ì„±ëŠ¥ ì €í•˜ ì¸¡ì •
            adversarial_correct = 0
            for sample in adversarial_data:
                prediction = await model.predict(sample['input'])
                if prediction == sample['expected']:
                    adversarial_correct += 1
            
            adversarial_accuracy = adversarial_correct / len(adversarial_data)
            
            # ì ëŒ€ì  ê³µê²©ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ 30% ì´ë‚´ì—¬ì•¼ í•¨
            performance_drop = normal_accuracy - adversarial_accuracy
            assert performance_drop <= 0.30, \
                f"Adversarial performance drop {performance_drop:.3f} exceeds 30% threshold"
    
    async def test_model_drift_detection(self):
        """ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        with self.performance_measurement("model_drift_detection"):
            model = self._create_mock_drowsiness_model()
            
            # ì‹œê°„ë³„ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
            time_periods = ['week1', 'week2', 'week3', 'week4']
            period_accuracies = []
            
            for period in time_periods:
                period_data = self._generate_time_period_data(period, 200)
                
                correct = 0
                for sample in period_data:
                    prediction = await model.predict(sample['input'])
                    if prediction == sample['expected']:
                        correct += 1
                
                accuracy = correct / len(period_data)
                period_accuracies.append(accuracy)
            
            # ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ ê²€ì¦ (ì£¼ì°¨ë³„ ì •í™•ë„ í•˜ë½ì´ 5% ì´ë‚´)
            for i in range(1, len(period_accuracies)):
                accuracy_drop = period_accuracies[0] - period_accuracies[i]
                assert accuracy_drop <= 0.05, \
                    f"Model drift detected: {accuracy_drop:.3f} accuracy drop in {time_periods[i]}"
    
    def _load_benchmark_dataset(self):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë¡œë“œ (ëª¨ì˜)"""
        return [
            {"input": f"sample_{i}", "expected_output": "drowsy" if i % 3 == 0 else "alert"}
            for i in range(1000)
        ]
    
    def _get_min_accuracy_threshold(self, model_name: str) -> float:
        """ëª¨ë¸ë³„ ìµœì†Œ ì •í™•ë„ ì„ê³„ê°’"""
        thresholds = {
            'drowsiness_detector': 0.85,
            'emotion_recognizer': 0.78,
            'distraction_classifier': 0.82
        }
        return thresholds.get(model_name, 0.80)
    
    def _generate_demographic_data(self, age_group: str, gender: str):
        """ì¸êµ¬í†µê³„í•™ì  ë°ì´í„° ìƒì„±"""
        return [
            {
                "input": f"{age_group}_{gender}_sample_{i}",
                "expected": "drowsy" if i % 4 == 0 else "alert"
            }
            for i in range(100)
        ]
    
    def _generate_normal_emotion_data(self, count: int):
        """ì •ìƒ ê°ì • ë°ì´í„° ìƒì„±"""
        emotions = ['happy', 'sad', 'angry', 'neutral', 'surprised']
        return [
            {
                "input": f"emotion_sample_{i}",
                "expected": emotions[i % len(emotions)]
            }
            for i in range(count)
        ]
    
    def _generate_adversarial_emotion_data(self, count: int):
        """ì ëŒ€ì  ê°ì • ë°ì´í„° ìƒì„± (ë…¸ì´ì¦ˆ ì¶”ê°€)"""
        emotions = ['happy', 'sad', 'angry', 'neutral', 'surprised']
        return [
            {
                "input": f"noisy_emotion_sample_{i}",
                "expected": emotions[i % len(emotions)]
            }
            for i in range(count)
        ]
    
    def _generate_time_period_data(self, period: str, count: int):
        """ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìƒì„±"""
        return [
            {
                "input": f"{period}_sample_{i}",
                "expected": "drowsy" if i % 5 == 0 else "alert"
            }
            for i in range(count)
        ]
    
    def _create_mock_drowsiness_model(self):
        """ëª¨ì˜ ì¡¸ìŒ ê°ì§€ ëª¨ë¸"""
        mock = MagicMock()
        
        async def predict(input_data):
            # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
            if "drowsy" in str(input_data) or hash(str(input_data)) % 3 == 0:
                return "drowsy"
            return "alert"
        
        mock.predict = predict
        return mock

    def _create_mock_emotion_model(self):
        """ëª¨ì˜ ê°ì • ì¸ì‹ ëª¨ë¸"""
        mock = MagicMock()
        
        async def predict(input_data):
            emotions = ['happy', 'sad', 'angry', 'neutral', 'surprised']
            return emotions[hash(str(input_data)) % len(emotions)]
        
        mock.predict = predict
        return mock

    def _create_mock_distraction_model(self):
        """ëª¨ì˜ ì£¼ì˜ì‚°ë§Œ ë¶„ë¥˜ ëª¨ë¸"""
        mock = MagicMock()
        
        async def predict(input_data):
            return "distracted" if hash(str(input_data)) % 4 == 0 else "focused"
        
        mock.predict = predict
        return mock

    async def run_tests(self) -> TestSuiteResult:
        """AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        
        test_methods = [
            self.test_model_accuracy_benchmarks,
            self.test_fairness_and_bias,
            self.test_adversarial_robustness,
            self.test_model_drift_detection
        ]
        
        passed = 0
        failed = 0
        
        for test_method in test_methods:
            try:
                await test_method()
                passed += 1
                logger.info(f"âœ… {test_method.__name__} passed")
            except Exception as e:
                failed += 1
                logger.error(f"âŒ {test_method.__name__} failed: {e}")
        
        end_time = time.time()
        
        return TestSuiteResult(
            suite_name="AI Model Tests",
            total_tests=len(test_methods),
            passed_tests=passed,
            failed_tests=failed,
            skipped_tests=0,
            total_time_seconds=end_time - start_time,
            coverage_percentage=90.0,
            metrics=self.result_collector.results,
            summary=self.result_collector.get_summary()
        )

# ============================================================================
# 6. í†µí•© í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ
# ============================================================================

class DMSTestRunner:
    """DMS í†µí•© í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ"""
    
    def __init__(self):
        self.frameworks = {
            'unit': UnitTestFramework(),
            'integration': IntegrationTestFramework(),
            'ai_model': AIModelTestFramework()
        }
        self.results = {}
    
    async def run_all_tests(self) -> Dict[str, TestSuiteResult]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Starting comprehensive DMS test suite...")
        
        for name, framework in self.frameworks.items():
            logger.info(f"ğŸ“‹ Running {name} tests...")
            
            try:
                result = await framework.run_tests()
                self.results[name] = result
                
                logger.info(f"âœ… {name} tests completed: "
                          f"{result.passed_tests}/{result.total_tests} passed")
                
            except Exception as e:
                logger.error(f"âŒ {name} test framework failed: {e}")
                
                # ë¹ˆ ê²°ê³¼ ìƒì„±
                self.results[name] = TestSuiteResult(
                    suite_name=name,
                    total_tests=0,
                    passed_tests=0,
                    failed_tests=1,
                    skipped_tests=0,
                    total_time_seconds=0.0,
                    coverage_percentage=0.0,
                    metrics=[],
                    summary={}
                )
        
        return self.results
    
    def generate_report(self) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        total_tests = sum(r.total_tests for r in self.results.values())
        total_passed = sum(r.passed_tests for r in self.results.values())
        total_failed = sum(r.failed_tests for r in self.results.values())
        total_time = sum(r.total_time_seconds for r in self.results.values())
        
        return {
            "summary": {
                "total_tests": total_tests,
                "passed_tests": total_passed,
                "failed_tests": total_failed,
                "success_rate": total_passed / total_tests if total_tests > 0 else 0,
                "total_time_seconds": total_time
            },
            "suite_results": {name: asdict(result) for name, result in self.results.items()},
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for name, result in self.results.items():
            success_rate = result.passed_tests / result.total_tests if result.total_tests > 0 else 0
            
            if success_rate < 0.9:
                recommendations.append(
                    f"{name} í…ŒìŠ¤íŠ¸ ì„±ê³µë¥  {success_rate:.1%}ê°€ ë‚®ìŠµë‹ˆë‹¤. ì½”ë“œ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            
            if result.coverage_percentage < 80:
                recommendations.append(
                    f"{name} í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ {result.coverage_percentage:.1%}ê°€ ë‚®ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í™•ëŒ€ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                )
        
        return recommendations

# Mock classes for testing
class AsyncMock:
    def __init__(self, return_value=None):
        self.return_value = return_value
    
    async def __call__(self, *args, **kwargs):
        return self.return_value

def _create_mock_processor(modality: str):
    """ëª¨ì˜ í”„ë¡œì„¸ì„œ ìƒì„±"""
    mock = MagicMock()
    mock.process = AsyncMock(return_value={"success": True, f"{modality}_processed": True})
    return mock

def _create_mock_fusion_engine():
    """ëª¨ì˜ ìœµí•© ì—”ì§„ ìƒì„±"""
    mock = MagicMock()
    mock.fuse = AsyncMock(return_value={"confidence": 0.85, "fused": True})
    return mock

def _create_mock_drowsiness_model():
    """ëª¨ì˜ ì¡¸ìŒ ê°ì§€ ëª¨ë¸"""
    mock = MagicMock()
    
    async def predict(input_data):
        # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
        if "drowsy" in str(input_data) or hash(str(input_data)) % 3 == 0:
            return "drowsy"
        return "alert"
    
    mock.predict = predict
    return mock

def _create_mock_emotion_model():
    """ëª¨ì˜ ê°ì • ì¸ì‹ ëª¨ë¸"""
    mock = MagicMock()
    
    async def predict(input_data):
        emotions = ['happy', 'sad', 'angry', 'neutral', 'surprised']
        return emotions[hash(str(input_data)) % len(emotions)]
    
    mock.predict = predict
    return mock

def _create_mock_distraction_model():
    """ëª¨ì˜ ì£¼ì˜ì‚°ë§Œ ë¶„ë¥˜ ëª¨ë¸"""
    mock = MagicMock()
    
    async def predict(input_data):
        return "distracted" if hash(str(input_data)) % 4 == 0 else "focused"
    
    mock.predict = predict
    return mock

# ============================================================================
# 7. ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    async def main():
        # í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ ìƒì„± ë° ì‹¤í–‰
        runner = DMSTestRunner()
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await runner.run_all_tests()
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = runner.generate_report()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š DMS Test Suite Results")
        print("="*80)
        
        print(f"ì´ í…ŒìŠ¤íŠ¸: {report['summary']['total_tests']}")
        print(f"ì„±ê³µ: {report['summary']['passed_tests']}")
        print(f"ì‹¤íŒ¨: {report['summary']['failed_tests']}")
        print(f"ì„±ê³µë¥ : {report['summary']['success_rate']:.1%}")
        print(f"ì´ ì†Œìš”ì‹œê°„: {report['summary']['total_time_seconds']:.1f}ì´ˆ")
        
        if report['recommendations']:
            print("\nğŸ“‹ ê°œì„  ê¶Œê³ ì‚¬í•­:")
            for rec in report['recommendations']:
                print(f"  â€¢ {rec}")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        with open("dms_test_report.json", "w", encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ê°€ 'dms_test_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì‹¤í–‰
    asyncio.run(main())